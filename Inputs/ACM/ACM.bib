@inproceedings{10.5555/519309.856467,
author = {Haniff, David J. and Baber, Chris},
title = {Wearable Computers for the Fire Service and Police Force: Technological and Human Factors},
year = {1999},
isbn = {0769504280},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Wearable computer applications can be defined broadly as situationally-aware and situationally-unaware. A situationally-aware fire-fighter application which is under development is described and a situationally-unaware police force application is presented. However, underlying these developments is the issue of the appropriateness of the user interface for these applications.},
booktitle = {Proceedings of the 3rd IEEE International Symposium on Wearable Computers},
pages = {185},
keywords = {wearable computers, police force, interface design, fire service},
series = {ISWC '99}
}

@inproceedings{10.1145/3576914.3588016,
author = {Ledgerwood, Scott and Lewis, Jack and Karhoff, Jeffrey and Zhu, Qi and Whitlock, Matthew and Chelen, Julia},
title = {The Technical Development of an Extended Reality Research Testbed for Public Safety},
year = {2023},
isbn = {9798400700491},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576914.3588016},
doi = {10.1145/3576914.3588016},
abstract = {The study of public safety technology, interventions, and training involves variable and hazardous conditions, which complicate observation and measurement. Informative evaluation approaches require reasonable representation of these conditions. Nonetheless, representing these conditions is resource intensive and difficult to replicate. For these reasons, public safety research may be limited by low-fidelity approaches that differ from the intended real-world application and by the inaccessibility of more realistic training. Extended Reality (XR) environments offer highly immersive and repeatable training for first responders, as well as controlled methods for technical research. In this paper, we discuss the development of the National Institute of Standards and Technology (NIST) Public Safety Immersive Test Center (PSITC): a testbed for multi-sensory extended reality-based research for public safety scenarios. We describe how the PSITC supports realistic training for public safety with an overview of the center’s design, development, and technical implementation. We discuss methods to address the challenges of building such a testbed for XR-based research, including integration of nascent technologies from various vendors, extensive use of sensor and imaging technologies, and intergovernmental cooperation between the First Responder Network Authority and the NIST Public Safety Communications Research Division. This paper introduces a model for the development of immersive centers built to evaluate prototypes for public safety operations, improve training for emergency response, and support public safety technology research.},
booktitle = {Proceedings of Cyber-Physical Systems and Internet of Things Week 2023},
pages = {292–296},
numpages = {5},
keywords = {Haptics, Training, Public Safety, Mixed / Extended / Augmented Reality, Optics, Human-Centered Computing, Virtual Reality, Emergency Response, User Experience},
location = {San Antonio, TX, USA},
series = {CPS-IoT Week '23}
}

@inproceedings{10.1007/978-3-030-49695-1_40,
author = {Phillips, Nate and Kruse, Brady and Khan, Farzana Alam and Swan II, J. Edward and Bethel, Cindy L.},
title = {A Robotic Augmented Reality Virtual Window for Law Enforcement Operations},
year = {2020},
isbn = {978-3-030-49694-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-49695-1_40},
doi = {10.1007/978-3-030-49695-1_40},
abstract = {In room-clearing tasks, SWAT team members suffer from a lack of initial environmental information: knowledge about what is in a room and what relevance or threat level it represents for mission parameters. Normally this gap in situation awareness is rectified only upon room entry, forcing SWAT team members to rely on quick responses and near-instinctual reactions. This can lead to dangerously escalating situations or important missed information which, in turn, can increase the likelihood of injury and even mortality. Thus, we present an x-ray vision system for the dynamic scanning and display of room content, using a robotic platform to mitigate operator risk. This system maps a room using a robot-equipped stereo depth camera and, using an augmented reality (AR) system, presents the resulting geographic information according to the perspective of each officer. This intervention has the potential to notably lower risk and increase officer situation awareness, all while team members are in the relative safety of cover. With these potential stakes, it is important to test the viability of this system natively and in an operational SWAT team context.},
booktitle = {Virtual, Augmented and Mixed Reality. Design and Interaction: 12th International Conference, VAMR 2020, Held as Part of the 22nd HCI International Conference, HCII 2020, Copenhagen, Denmark, July 19–24, 2020, Proceedings, Part I},
pages = {591–610},
numpages = {20},
keywords = {Augmented reality, X-Ray Vision, Situation awareness, Robotics},
location = {Copenhagen, Denmark}
}

@inproceedings{10.1007/978-3-030-49695-1_44,
author = {Phillips, Nate and Kruse, Brady and Khan, Farzana Alam and Swan II, J. Edward and Bethel, Cindy L.},
title = {Correction to: A Robotic Augmented Reality Virtual Window for Law Enforcement Operations},
year = {2020},
isbn = {978-3-030-49694-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-49695-1_44},
doi = {10.1007/978-3-030-49695-1_44},
abstract = {The original version of this chapter was revised. The acknowledgement was inadvertently forgotten. It has been added.},
booktitle = {Virtual, Augmented and Mixed Reality. Design and Interaction: 12th International Conference, VAMR 2020, Held as Part of the 22nd HCI International Conference, HCII 2020, Copenhagen, Denmark, July 19–24, 2020, Proceedings, Part I},
pages = {C1},
location = {Copenhagen, Denmark}
}

@inproceedings{10.1145/3170427.3188652,
author = {Hahn, J\"{u}rgen and Ludwig, Bernd and Wolff, Christian},
title = {Mixed Reality-Based Process Control of Automatic Printed Circuit Board Assembly Lines},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3188652},
doi = {10.1145/3170427.3188652},
abstract = {A mixed reality (MR)-based concept for supporting and optimizing the way operators work with automatic printed circuit board (PCB) assembly lines, is proposed. In order to enhance the work process' interface, users are outfitted with a head-mounted display (HMD), so they can both actively access process relevant machine data and passively receive system notifications in a heads-up display (HUD), instead of having to manually query the terminal of the machine of interest at its very location. This approach was implemented and tested in a field study with one of the assembly lines of an electronics manufacturing services (EMS)-company. 30 staff members were recruited as test subjects and 90% of them appreciated the system deployment, due to its noticeable additional benefits compared to the status quo.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {assistance systems, mixed reality, industrial assembly, human-centered computing},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.1007/978-3-030-21565-1_3,
author = {Carruth, Daniel W. and Hudson, Christopher R. and Bethel, Cindy L. and Pleva, Matus and Ondas, Stanislav and Juhar, Jozef},
title = {Using HMD for Immersive Training of Voice-Based Operation of Small Unmanned Ground Vehicles},
year = {2019},
isbn = {978-3-030-21564-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-21565-1_3},
doi = {10.1007/978-3-030-21565-1_3},
abstract = {Voice recognition systems provide a method of hands-free control of robotic systems that may be helpful in law enforcement or military domains. However, the constraints of the operational environment limit the capabilities of the on-board voice recognition system to a keyword-based command system. To effectively use the system, the users must learn the available commands and practice pronunciation to ensure accurate recognition. Virtual reality simulation provides users the opportunity to train with the voice recognition system and the robot platform in realistic interactive scenarios. Training using virtual reality with a head-mounted display may increase immersion and sense of presence compared to using a keyboard and monitor. A small pilot study compared user experience in the desktop mode and the virtual reality mode of our voice recognition system training tool. Participants controlled a simulated unmanned ground vehicle in two different modes across four different environments. The results revealed no significant differences in simulator sickness, sense of presence, or perceived usability. However, when asked to choose between the desktop mode and the head-mounted display mode, results indicate users’ overall preference for the head-mounted display. However, the users also perceive the head-mounted display to be more complex, less consistent, and more difficult to learn to use. The desktop mode was perceived as easier to use and users reported being more confident when using it.},
booktitle = {Virtual, Augmented and Mixed Reality. Applications and Case Studies : 11th International Conference, VAMR 2019, Held as Part of the 21st HCI International Conference, HCII 2019, Orlando, FL, USA, July 26–31, 2019, Proceedings, Part II},
pages = {34–46},
numpages = {13},
keywords = {Speech recognition, Usability, Virtual reality, Human-robot interaction},
location = {Orlando, FL, USA}
}

@inproceedings{10.1145/2030112.2030185,
author = {Rahnama, Hossein and Jamshidi, Sina and Johns, Stephen and Shepard, Alan},
title = {CAMPUS: Context Aware Mobile Platform for Uniformed Security},
year = {2011},
isbn = {9781450306300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030112.2030185},
doi = {10.1145/2030112.2030185},
abstract = {The Context Aware Mobile Platform for Uniformed Security (CAMPUS) system was developed with the aim of using ambient intelligence and mobile communications to information sources as an aid to field security and law enforcement personnel. The system is centered on a context aware information distribution framework and a client operating on a head mounted display. The system allows field service personnel to receive information relevant to their current operations, review information from sensors, and communicate with colleagues while remaining vigilant during security and law enforcement operations. Sensor integration and mapping technologies are the key elements in provisioning of appropriate information services.},
booktitle = {Proceedings of the 13th International Conference on Ubiquitous Computing},
pages = {489–490},
numpages = {2},
keywords = {law enforcement, ubiquitous computing, field security, mobile systems, context awareness},
location = {Beijing, China},
series = {UbiComp '11}
}

@inproceedings{10.1145/2992154.2996880,
author = {Chan, Edwin and Wang, Yuxi and Seyed, Teddy and Maurer, Frank},
title = {ERWear: Wearable System Design through the Lens of First Responders},
year = {2016},
isbn = {9781450342483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2992154.2996880},
doi = {10.1145/2992154.2996880},
abstract = {We explore the design of a wearable computing solution for first responders. Wearable devices have many uses, but commercial devices are not suitable for emergency response. First responders face high risk and volatile situations, and wearables possess significant potential to keep responders safe. A lack of understanding exists when designing wearables for first responders. Existing research focuses on the physical implementation of various sensors, rather than usability. Combining literature and extensive interviews, we devise design guidelines for responder-oriented wearable systems. We propose a prototype system, and discuss early feedback from responders.},
booktitle = {Proceedings of the 2016 ACM International Conference on Interactive Surfaces and Spaces},
pages = {489–492},
numpages = {4},
keywords = {emergency management, first responder, head-mounted display, wearable devices},
location = {Niagara Falls, Ontario, Canada},
series = {ISS '16}
}

@inproceedings{10.1145/3359996.3364250,
author = {Khamis, Mohamed and Schuster, Nora and George, Ceenu and Pfeiffer, Max},
title = {ElectroCutscenes: Realistic Haptic Feedback in Cutscenes of Virtual Reality Games Using Electric Muscle Stimulation},
year = {2019},
isbn = {9781450370011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359996.3364250},
doi = {10.1145/3359996.3364250},
abstract = {Cutscenes in Virtual Reality (VR) games enhance story telling by delivering output in the form of visual, auditory, or haptic feedback (e.g., using vibrating handheld controllers). Since they lack interaction in the form of user input, cutscenes would significantly benefit from improved feedback. We introduce the concept and implementation of ElectroCutscenes, where Electric Muscle Stimulation (EMS) is leveraged to elicit physical user movements to different body parts to correspond to those of personal avatars in cutscenes of VR games while the user stays passive. Through a user study (N=22) in which users passively received kinesthetic feedback resulting in involuntarily movements, we show that ElectroCutscenes significantly increases perceived presence and realism compared to controller-based vibrotactile and no haptic feedback. Furthermore, we found preliminary evidence that combining visual and EMS feedback can evoke movements that are not actuated by either of them alone. We discuss how to enhance realism and presence of cutscenes in VR games even when EMS can partially rather than completely actuate the desired body movements.},
booktitle = {Proceedings of the 25th ACM Symposium on Virtual Reality Software and Technology},
articleno = {13},
numpages = {10},
keywords = {Haptic Feedback, Haptics, EMS, Head-mounted Displays},
location = {Parramatta, NSW, Australia},
series = {VRST '19}
}

@inproceedings{10.1145/2970930.2970957,
author = {Mentler, Tilo and Berndt, Henrik and Herczeg, Michael},
title = {Optical Head-Mounted Displays for Medical Professionals: Cognition-Supporting Human-Computer Interaction Design},
year = {2016},
isbn = {9781450342445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970930.2970957},
doi = {10.1145/2970930.2970957},
abstract = {Optical head-mounted displays are an emerging digital technology in domains like healthcare where computer usage in mobile contexts is required but hand-held devices are not particularly suited for practical reasons (e.g. hygienic regulations). As the work of paramedics, nurses and physicians is not only physically but also mentally challenging, the promise of hands-free interaction alone will not ensure efficient and safe usage. Rather, various aspects of cognitive ergonomics have to be carefully considered. Best practices for designing wearable interactive systems have to be evaluated. Based on the results of different user-centered system design projects and studies with members of Emergency Medical Services, nurses and clinical physicians, we will discuss optical head-mounted displays with respect to human-computer interaction. Interaction design as well as lessons learned concerning tasks and workflows will be summarized. Transmodal consistency is introduced as a general design principle for digital technologies supporting multiple input and output modalities like touch, gestures and speech.},
booktitle = {Proceedings of the European Conference on Cognitive Ergonomics},
articleno = {26},
numpages = {8},
keywords = {Optical Head-Mounted Displays, Interface Design, Google Glass, Usability, Interaction Design},
location = {Nottingham, United Kingdom},
series = {ECCE '16}
}

@inproceedings{10.1007/978-3-642-35377-2_65,
author = {Talavera, Guillermo and Martin, Renat and Rodr\'{\i}guez-Alsina, Aitor and Garcia, Joan and Fern\'{a}ndez, Francesc and Carrabina, Jordi},
title = {Protecting Firefighters with Wearable Devices},
year = {2012},
isbn = {9783642353765},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-35377-2_65},
doi = {10.1007/978-3-642-35377-2_65},
abstract = {Emergency units typically operate under extremely harsh conditions and could benefit from new technologies to perform at their highest potential and provide and ideal test case to push wereable computing to its limits. In this paper we present our work on smart textiles and wearable devices for firefighters. The objective of our work is to create an smart t-shirt capable of measuring the rate and thermal stress state which the user is subject to. For that, several sensors monitor different parameters and send the information, via bluetooth low energy wireless protocol, to a mobile phone and a wrist-watch both with bluetooth low energy communication capabilities. In this paper we explain the main features of our work and show some test accomplished with different levels of temperature to test the robustness of the system.},
booktitle = {Proceedings of the 6th International Conference on Ubiquitous Computing and Ambient Intelligence},
pages = {470–477},
numpages = {8},
keywords = {wearable devices, body area network, firefighters, bluetooth low-energy, sensors, monitor, smart textiles, smart-phone},
location = {Vitoria-Gasteiz, Spain},
series = {UCAmI'12}
}

@inproceedings{10.1145/2345396.2345582,
author = {Kunnath, Abishek Thekkeyil and Pradeep, Preeja and Ramesh, Maneesha Vinodini},
title = {Locating and Monitoring Emergency Responder Using a Wearable Device},
year = {2012},
isbn = {9781450311960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2345396.2345582},
doi = {10.1145/2345396.2345582},
abstract = {When a disaster occurs, activities like search, rescue, recovery, and cleanup are carried out by emergency responders. This paper proposes a new framework for supporting the safety and health of emergency responders by locating their position and monitoring their vital signs using a Wireless Wearable Device. As an initial step towards the development of a wireless wearable tracking and monitoring system for emergency responders, this system uses an iterative localization based scheme, which provides the exact position of each emergency responder, and monitors their vital signs like skin temperature and pulse rate. Any change in the vital signs can be easily sensed and tracked, and could be used to provide warnings when critical events are detected. The system could be used to send early warning alerts and for communication between emergency responders.},
booktitle = {Proceedings of the International Conference on Advances in Computing, Communications and Informatics},
pages = {1163–1168},
numpages = {6},
keywords = {monitoring centre, unlocalized node, position estimation, localized node, incident commander},
location = {Chennai, India},
series = {ICACCI '12}
}

@inproceedings{10.1145/3316782.3321542,
author = {Koutitas, George and Smith, Kenneth Scott and Lawrence, Grayson and Metsis, Vangelis and Stamper, Clayton and Trahan, Mark and Lehr, Ted},
title = {A Virtual and Augmented Reality Platform for the Training of First Responders of the Ambulance Bus},
year = {2019},
isbn = {9781450362320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316782.3321542},
doi = {10.1145/3316782.3321542},
abstract = {AmBus is a bus-sized ambulance that EMS personnel utilize during large-scale emergencies. Although EMS personnel receive annual training, evidence shows current training efforts leave some personnel unfamiliar with the AmBus system and unprepared to respond to an emergency. This work presents a novel interactive training application, utilizing emerging technologies in virtual and augmented reality, that can be delivered remotely to the distributed EMS personnel before they assemble, or as they are assembling. Our initial findings show that such an application can better prepare first responders to be as effective as possible in using the life-saving features of the AmBus. The methodology described in this work can be expanded to include other first responders, and, ultimately, lives may be saved because personnel are better prepared.},
booktitle = {Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {299–302},
numpages = {4},
keywords = {training, augmented reality, virtual reality, first responders},
location = {Rhodes, Greece},
series = {PETRA '19}
}

@inproceedings{10.1145/3123024.3129269,
author = {Abdelrahman, Yomna and Knierim, Pascal and Wozniak, Pawel W. and Henze, Niels and Schmidt, Albrecht},
title = {See through the Fire: Evaluating the Augmentation of Visual Perception of Firefighters Using Depth and Thermal Cameras},
year = {2017},
isbn = {9781450351904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123024.3129269},
doi = {10.1145/3123024.3129269},
abstract = {Our visual perception is limited to the abilities of our eyes, where we only perceive visible light. This limitation might influence how we perceive and react to our surroundings, however, this limitation might endanger us in certain scenarios e.g. firefighting. In this paper, we explore the potential of augmenting the visual sensing of the firefighters using depth and thermal imaging to increase their awareness about the environment. Additionally, we built and evaluated two form factors, hand held and head mounted display. To evaluate our built prototypes, we conducted two user studies in a simulated fire environment with real firefighters. In this workshop paper, we present our findings from the evaluation of the concept and prototypes with real firefighters.},
booktitle = {Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers},
pages = {693–696},
numpages = {4},
keywords = {firefighters, depth cameras, thermal imaging},
location = {Maui, Hawaii},
series = {UbiComp '17}
}

@inproceedings{10.1145/3384419.3430421,
author = {Dai, Zhuangzhuang and Saputra, Muhamad Risqi U. and Lu, Chris Xiaoxuan and Trigoni, Niki and Markham, Andrew},
title = {Indoor Positioning System in Visually-Degraded Environments with Millimetre-Wave Radar and Inertial Sensors: Demo Abstract},
year = {2020},
isbn = {9781450375900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384419.3430421},
doi = {10.1145/3384419.3430421},
abstract = {Positional estimation is of great importance in the public safety sector. Emergency responders such as fire fighters, medical rescue teams, and the police will all benefit from a resilient positioning system to deliver safe and effective emergency services. Unfortunately, satellite navigation (e.g., GPS) offers limited coverage in indoor environments. It is also not possible to rely on infrastructure based solutions. To this end, wearable sensor-aided navigation techniques, such as those based on camera and Inertial Measurement Units (IMU), have recently emerged recently as an accurate, infrastructure-free solution. Together with an increase in the computational capabilities of mobile devices, motion estimation can be performed in real-time. In this demonstration, we present a real-time indoor positioning system which fuses millimetre-wave (mmWave) radar and IMU data via deep sensor fusion. We employ mmWave radar rather than an RGB camera as it provides better robustness to visual degradation (e.g., smoke, darkness, etc.) while at the same time requiring lower computational resources to enable runtime computation. We implemented the sensor system on a handheld device and a mobile computer running at 10 FPS to track a user inside an apartment. Good accuracy and resilience were exhibited even in poorly illuminated scenes.},
booktitle = {Proceedings of the 18th Conference on Embedded Networked Sensor Systems},
pages = {623–624},
numpages = {2},
keywords = {IMU, deep learning, indoor positioning, millimeter-wave sensor},
location = {Virtual Event, Japan},
series = {SenSys '20}
}

@inproceedings{10.1145/3281505.3281506,
author = {Kono, Michinari and Miyaki, Takashi and Rekimoto, Jun},
title = {In-Pulse: Inducing Fear and Pain in Virtual Experiences},
year = {2018},
isbn = {9781450360869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281505.3281506},
doi = {10.1145/3281505.3281506},
abstract = {Researchers have attempted to increase the realism of virtual reality (VR) applications in many ways. Combinations of the visual, auditory and haptic feedback have successfully simulated experiences in VR, however, multimedia contents may also stimulate emotions. In this paper, we especially paid attention to negative emotions that may be perceived in such experiences (e.g., fear). We hypothesized that volunteering, visual, mechanical, and electrical feedback may induce negative emotional feedback to users. In-Pulse is a novel system and approach to explore the potential of bringing this emotional feedback to users. We designed a head-mounted display (HMD) combined with mechanical and electrical muscle stimulation (EMS) actuators. A user study was performed to explore the effect of our approaches with combinations with VR contents. The results suggest that mechanical actuators and EMS can improve the experience of virtual experiences.},
booktitle = {Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology},
articleno = {40},
numpages = {5},
keywords = {fear, electrical muscle stimulation, pain, head-mounted display, emotion, wearables, virtual reality},
location = {Tokyo, Japan},
series = {VRST '18}
}

@inproceedings{10.1145/3025453.3025600,
author = {Lopes, Pedro and You, Sijing and Cheng, Lung-Pan and Marwecki, Sebastian and Baudisch, Patrick},
title = {Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025600},
doi = {10.1145/3025453.3025600},
abstract = {We explore how to add haptics to walls and other heavy objects in virtual reality. When a user tries to push such an object, our system actuates the user's shoulder, arm, and wrist muscles by means of electrical muscle stimulation, creating a counter force that pulls the user's arm backwards. Our device accomplishes this in a wearable form factor.In our first user study, participants wearing a head-mounted display interacted with objects provided with different types of EMS effects. The repulsion design (visualized as an electrical field) and the soft design (visualized as a magnetic field) received high scores on "prevented me from passing through" as well as "realistic".In a second study, we demonstrate the effectiveness of our approach by letting participants explore a virtual world in which all objects provide haptic EMS effects, including walls, gates, sliders, boxes, and projectiles.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {1471–1482},
numpages = {12},
keywords = {proprioception, force feedback, virtual reality, muscle interfaces, ems, haptics, real-walking},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{10.1145/2468356.2468778,
author = {Munteanu, Cosmin and Fournier, H\'{e}l\`{a}ne and Lapointe, Jean-Fran\c{c}ois and Emond, Bruno and Kondratova, Irina},
title = {We'll Take It from Here: Letting the Users Take Charge of the Evaluation and Why That Turned out Well},
year = {2013},
isbn = {9781450319522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2468356.2468778},
doi = {10.1145/2468356.2468778},
abstract = {The operational challenges faced by law enforcement and public safety personnel are constantly evolving, while the training and certification process has stayed the same. New technologies such as virtual reality, mixed reality, or game-based simulators are being researched as promising enhancements to traditional training methods. However, their widespread adoption, particularly by smaller units, faces barriers such as cost - due in no small part to the difficulties of developing and especially evaluating such large-scale interactive systems. In this case study, we present MINT - a low-cost mixed-reality Multimodal INteractive Training system, aimed at supporting the training of small- and medium-sized law enforcement and infantry units. We discuss the challenges and approaches taken in the participatory design of the training system, its agile-based development and implementation, and its qualitative evaluation with users and subject-matter experts.},
booktitle = {CHI '13 Extended Abstracts on Human Factors in Computing Systems},
pages = {2383–2384},
numpages = {2},
keywords = {immersive gaming, user studies, mixed-reality interaction, evaluation methodology},
location = {Paris, France},
series = {CHI EA '13}
}

@inproceedings{10.1145/3266037.3271631,
author = {Wolf, Dennis and Hnatek, Leo and Rukzio, Enrico},
title = {Face/On: Actuating the Facial Contact Area of a Head-Mounted Display for Increased Immersion},
year = {2018},
isbn = {9781450359498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266037.3271631},
doi = {10.1145/3266037.3271631},
abstract = {In this demonstration, we introduce Face/On, an embedded feedback device that leverages the contact area between the user's face and a virtual reality (VR) head-mounted display (HMD) to provide rich haptic feedback in virtual environments (VEs). Head-worn haptic feedback devices have been explored in previous work to provide directional cues via grids of actuators and localized feedback on the users' skin. Most of these solutions were immersion breaking due to their encumbering and uncomfortable design and build around a single actuator type, thus limiting the overall fidelity and flexibility of the haptic feedback. We present Face/On, a VR HMD face cushion with three types of discreetly embedded actuators that provide rich haptic feedback without encumbering users with invasive instrumentation on the body. By combining vibro-tactile and thermal feedback with electrical muscle stimulation (EMS), Face/On can simulate a wide range of scenarios and benefit from synergy effects between these feedback types.},
booktitle = {Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
pages = {146–148},
numpages = {3},
keywords = {vr, feedback, heat, haptic, vibration, multi-modal},
location = {Berlin, Germany},
series = {UIST '18 Adjunct}
}

@inproceedings{10.1145/3526114.3558653,
author = {Zempo, Keiichi and Kashiwabara, Ryo and Wakatsuki, Naoto and Mizutani, Koichi},
title = {Silent Subwoofer System Using Myoelectric Stimulation to Presents the Acoustic Deep Bass Experiences},
year = {2022},
isbn = {9781450393218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526114.3558653},
doi = {10.1145/3526114.3558653},
abstract = {This study demonstrates a portable, low-noise system that utilizes electrical muscle stimulation (EMS) to present a body-sensory acoustic experience similar to that experienced during live concerts. Twenty-four participants wore head-mounted displays (HMDs), headphones, and the proposed system and experienced a live concert in a virtual reality (VR) space to evaluate the system. We found that the system was not inferior to a system with loudspeakers and subwoofers, where ambient noise concerns precision in rhythm and harmony. These results could be explained by the user perceiving the EMS experience as a single signal when the EMS stimulation is presented in conjunction with visual and acoustic stimuli (e.g., the kicking of a bass drum, the bass sound generated from the kicking, and the acoustic sensation caused by the bass sound). The proposed method offers a novel EMS-based body-sensory acoustic experience, and the results of this study may lead to an improved experience not only for live concerts in VR space but also for everyday music listening.},
booktitle = {Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {75},
numpages = {3},
keywords = {VR live concert, acoustic interface, EMS application, multi-modal media},
location = {Bend, OR, USA},
series = {UIST '22 Adjunct}
}

@inproceedings{10.1145/3399715.3399961,
author = {Battistoni, Pietro and Di Gregorio, Marianna and Giordano, Domenico and Sebillo, Monica and Tortora, Genoveffa and Vitiello, Giuliana},
title = {Wearable Interfaces and Advanced Sensors to Enhance Firefighters Safety in Forest Fires},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399961},
doi = {10.1145/3399715.3399961},
abstract = {The forest fires represent a social emergency that requires significant economic and organizational commitment. Safety and the lack of reliable and timely localization of firefighters is a big problem. In this paper, we present Karya Advanced Sensor, an automatic, accurate, and reliable IT solution able to locate firefighters in harsh environments and support decision making activities at control rooms. The system consists of sensors perfectly integrated into firefighters' uniforms, which are used to monitor in real-time individual operators' activities as well as the entire fire area. In particular, in case a firefighter gets injured, the system will activate the rescue teams quickly, as there will be a constant link between the firefighters and the medical assistance. The firefighter can also specify the reason for the accident, which is critical information for a more timely and appropriate health intervention. Moreover, the system is able to perform an automatic real-time mapping of forest fires and possibly estimate its propagation rate, providing precious support to control rooms, which are the center of the team coordination.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {102},
numpages = {3},
keywords = {Wearable Interfaces, Forest Fire, Sensor, Firefighters Safety, Safety},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.5555/1777753.1777769,
author = {Bonfiglio, Annalisa and Carbonaro, Nicola and Chuzel, Cyril and Curone, Davide and Dudnik, Gabriela and Germagnoli, Fabio and Hatherall, David and Koller, Jean Mark and Lanier, Thierry and Loriga, Giannicola and Luprano, Jean and Magenes, Giovanni and Paradiso, Rita and Tognetti, Alessandro and Voirin, Guy and Waite, Rhys},
title = {Managing Catastrophic Events by Wearable Mobile Systems},
year = {2007},
isbn = {3540756671},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Proetex is a European Integrated Project dedicated to micro- and nano-technology-based wearable equipment for emergency operators. During the first year of work, a careful analysis of several emergency scenarios has been carried out and has resulted in the design of a complete "smart" uniform for fire-fighters and emergency rescuers. These garments aim at monitoring both physiological parameters, position and posture of the operators and the presence of external potential sources of danger and to send these data to a remote coordinating unit. In the following, the main issues of the design flow will be described and discussed.},
booktitle = {Proceedings of the 1st International Conference on Mobile Information Technology for Emergency Response},
pages = {95–105},
numpages = {11},
keywords = {smart textiles, wearable, sensors, transmission},
location = {Sankt Augustin, Germany},
series = {MobileResponse'07}
}

@inproceedings{10.1145/2459236.2459271,
author = {Sch\"{o}nauer, Christian and Vonach, Emanuel and Gerstweiler, Georg and Kaufmann, Hannes},
title = {3D Building Reconstruction and Thermal Mapping in Fire Brigade Operations},
year = {2013},
isbn = {9781450319041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2459236.2459271},
doi = {10.1145/2459236.2459271},
abstract = {Fire fighting remains a dangerous profession despite many recent technological and organizational measures. Sensors and technical systems can augment the performance of fire fighters to increase safety and efficiency during operation. An important aspect in that context is the awareness of location, structure and thermal properties of the environment.This paper focuses on the design and development of a mobile system, which can reconstruct a 3d model of a building's interior structure in real-time and fuses the visualization with the image of a thermal camera. In addition the position and viewing direction of the fire fighter within the model is determined and a thermal map can be generated from the gathered data, which could help an operational commander to guide his men during a mission.First tests with our system in different situations showed good results, being able to reconstruct different larger scenes and create thermal maps thereof.},
booktitle = {Proceedings of the 4th Augmented Human International Conference},
pages = {202–205},
numpages = {4},
keywords = {augmented reality, thermal camera, real-time dense reconstruction, fire fighter safety},
location = {Stuttgart, Germany},
series = {AH '13}
}

@inproceedings{10.1007/978-3-319-39952-2_14,
author = {Bailie, Tess and Martin, Jim and Aman, Zachary and Brill, Ryan and Herman, Alan},
title = {Implementing User-Centered Methods and Virtual Reality to Rapidly Prototype Augmented Reality Tools for Firefighters},
year = {2016},
isbn = {9783319399515},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-39952-2_14},
doi = {10.1007/978-3-319-39952-2_14},
abstract = {Designing and testing products for high-risk emergencies is a challenging task, especially due to the inhibitive cost of building testing environments that recreate the psychological pressures of the field. The chaotic nature of emergency environments makes gathering accurate data amidst the chaos of such environments difficult, while ethical and practical considerations limit prototype deployment in potentially life-threatening situations. These environments pose serious risk to physical and mental well-being. This paper provides a case study to examine the benefits and drawbacks of a Virtual Reality VR environment to test prototypes of a tool for firefighters. The VR simulated environment out performs a physical simulation because it is cheaper and safer, generates more reliable data, and provides greater control and flexibility of prototypes, allowing designers to test prototypes more rapidly than in a physical environment. This paper summarizes a 9-month Draper-sponsored capstone project with 5 HCII students.},
booktitle = {Proceedings, Part II, of the 10th International Conference on Foundations of Augmented Cognition: Neuroergonomics and Operational Neuroscience - Volume 9744},
pages = {135–144},
numpages = {10},
keywords = {Human centered design, Contextual design, Rapid prototyping, Augmented cognition, Virtual reality}
}

@inproceedings{10.1145/3301019.3323898,
author = {Starks, Denny L. and Dillahunt, Tawanna and Haimson, Oliver L.},
title = {Designing Technology to Support Safety for Transgender Women &amp; Non-Binary People of Color},
year = {2019},
isbn = {9781450362702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301019.3323898},
doi = {10.1145/3301019.3323898},
abstract = {This work provides a preliminary understanding of how transgender women and non-binary people of color experience violence and manage safety, and what opportunities exist for HCI to support the safety needs of this community. We conducted nine interviews to understand how participants practice safety and what role technology played, if any, in these experiences. Interviewees expressed physical and psychological safety concerns, and managed safety by informing friends of their location using digital technologies, making compromises, and avoiding law enforcement. We designed U-Signal, a wearable technology and accompanying smartphone application prototype to increase physical safety and decrease safety concerns, reduce violence, and help build community.},
booktitle = {Companion Publication of the 2019 on Designing Interactive Systems Conference 2019 Companion},
pages = {289–294},
numpages = {6},
keywords = {non-binary, location-based applications, safety, people of color, violence, transgender, transphobia, trans people of color (tpoc), community, wearables},
location = {San Diego, CA, USA},
series = {DIS '19 Companion}
}

@inproceedings{10.5555/1700307.1700516,
author = {Mporas, losif and Kocsis, Otilia and Ganchev, Todor and Fakotakis, Nikos},
title = {A Collaborative Speech Enhancement Approach for Speech Recognition in Motorcycle Environment},
year = {2009},
isbn = {9781424432974},
publisher = {IEEE Press},
abstract = {Aiming at the optimization of the speech recognition performance, we investigate various configurations for a speech front-end, which is part of a multimodal dialogue interaction interface of a wearable solution for information support of the motorcycle police force on the move. Initially, the practical value of various speech enhancement techniques is assessed, and subsequently a collaborative scheme employing independent speech enhancement channels, which operate in parallel on a common input, is proposed. It was experimentally found that the Adaboost. M1 algorithm is the most advantageous among a number of fusion methods. The improvement of speech recognition accuracy due to the collaborative speech enhancement scheme is expressed as gain of 8% in terms of word recognition rate, when compared to the performance of the best speech enhancement channel, alone.},
booktitle = {Proceedings of the 16th International Conference on Digital Signal Processing},
pages = {1254–1259},
numpages = {6},
keywords = {motorcycle environment, data fusion, speech recognition, speech enhancement},
location = {Santorini, Greece},
series = {DSP'09}
}

@inproceedings{10.1145/3152896.3152905,
author = {Polese, Michele and Mezzavilla, Marco and Rangan, Sundeep and Kessler, Coitt and Zorzi, Michele},
title = {MmWave for Future Public Safety Communications},
year = {2017},
isbn = {9781450354240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152896.3152905},
doi = {10.1145/3152896.3152905},
abstract = {The technologies developed for the next generation of cellular networks (i.e., 5G) are potential enablers for future Public Safety Communication (PSC) systems. These will indeed need advanced communication techniques, capable of providing real-time, low-latency and reliable interactions in different scenarios (vehicular, aerial, unmanned) and different network architectures. There is great interest in the millimeter wave (mmWave) band and in general in the spectrum above 6 GHz, since the bandwidth that can be allocated at these frequencies is much higher compared to the traditional (and congested) sub-6 GHz bands. This would enable orders of magnitude greater throughput and low latency, which could be used for example to stream high definition video or virtual/augmented reality data to first responders or for the remote control of autonomous robots. In this paper we illustrate both the potential of mmWave communications for PSC (also with a typical use case) and the issues that must be solved before this technology can be reliably adopted and mmWave PSC networks become a reality.},
booktitle = {Proceedings of the First CoNEXT Workshop on ICT Tools for Emergency Networks and DisastEr Relief},
pages = {44–49},
numpages = {6},
keywords = {public safety communications, mmWave, emergency networks},
location = {Incheon, Republic of Korea},
series = {I-TENDER '17}
}

@inproceedings{10.1007/978-3-031-25312-6_55,
author = {Hashem, Ahmed and Schlechter, Thomas},
title = {Drone Detection Using Deep Learning: A Benchmark Study},
year = {2023},
isbn = {978-3-031-25311-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-25312-6_55},
doi = {10.1007/978-3-031-25312-6_55},
abstract = {Since Unmanned Aerial Vehicles (UAVs) became available to the civilian public, it has witnessed dramatic spread and exponential popularity. This escalation gave rise to privacy and security concerns, both on the recreational and institutional levels. Although it is mainly used for leisure and productivity activities, it is evident that UAVs can also be used for malicious purposes. Today, as legislation and law enforcement federations can hardly control every incident, many institutions resort to surveillance systems to prevent hostile drone intrusion.Although drone detection can be carried out using different technologies, such as radar or ultra-sonic, visual detection is arguably the most efficient method. Other than being cheap and readily available, cameras are typically a part of any surveillance system. Moreover, the rise of deep learning and neural network models rendered visual recognition very reliable [9, 21].In this work, three state-of-the-art object detectors, namely YOLOv4, SSD-MobileNetv1 and SSD-VGG16, are tested and compared to find the best performing detector on our drone data-set of 23,863 collected and annotated images. The main work covers detailed reportage of the results of each model, as well as a comprehensive comparison between them. In terms of accuracy and real-time capability, the best performance was achieved by the SSD-VGG16 model, which scored average precision (AP50) of 90.4%, average recall (AR) of 72.7% and inference speed of 58 frames per second on the NVIDIA Jetson Xavier kit.},
booktitle = {Computer Aided Systems Theory – EUROCAST 2022: 18th International Conference, Las Palmas de Gran Canaria, Spain, February 20–25, 2022, Revised Selected Papers},
pages = {468–475},
numpages = {8},
keywords = {Artificial intelligence, Drone detection, Security, Neural network},
location = {Las Palmas de Gran Canaria, Spain}
}

@inproceedings{10.1145/1900179.1900242,
author = {Lee, Jaikyung and Cha, Moohyun and Choi, Byungil and Kim, Taesung},
title = {A Team-Based Firefighter Training Platform Using the Virtual Environment},
year = {2010},
isbn = {9781450304597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1900179.1900242},
doi = {10.1145/1900179.1900242},
abstract = {The increasing complexity of modern buildings, such as high-rise buildings and underground subway stations, has brought about an increase in the scale of potential hazards, which in turn requires changes in firefighter training. Due to cost, time, and safety requirements for the trainee, it is impossible for firefighters in training to experience a real fire. In additional, repeated training is impossible if real fire were used. We proposed a team-based firefighter training platform using the virtual environment for complex buildings. The platform provides training and evaluation of firefighting and mission-based team training. To achieve an immersive virtual environment, VR, AR and haptics technique is adopted. A numerical analysis method is used to model real fire phenomena.},
booktitle = {Proceedings of the 9th ACM SIGGRAPH Conference on Virtual-Reality Continuum and Its Applications in Industry},
pages = {299–302},
numpages = {4},
keywords = {VR, virtual environment, AR, firefighter team training},
location = {Seoul, South Korea},
series = {VRCAI '10}
}

@inproceedings{10.1007/978-3-031-37117-2_10,
author = {Guarda, Teresa and Lopes, Isabel and Bustos, Samuel and Ribeiro, Isabel and Fernandes, Ant\'{o}nio},
title = {Augmented Computing and Smart Cities Sustainability},
year = {2023},
isbn = {978-3-031-37116-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-37117-2_10},
doi = {10.1007/978-3-031-37117-2_10},
abstract = {Smart Cities promote a great improvement in urban environments in terms of sustainability, leveraged by the use of technologies that allow the optimization and monitoring of different systems, from waste management systems to public safety. There are several technologies that can play a key role in this process, highlighting augmented computing, augmented reality, virtual reality, artificial intelligence, and machine learning. In this context, smart cities collects and analyses, optimizing the various systems processes. There are many challenges that arise in order to have sustainable smart cities, challenges in terms of privacy and data security, but also in promoting inclusion and equity. It is important to adopt an inclusive and holistic approach that involves all stakeholders and takes into account the specific needs and objectives of a city. With this in mind, the adoption of augmented computing technologies facilitates the creation of more livable and also more sustainable urban environments. The main objective of this work is to explore the area of augmented computing in the context of smart cities sustainability.},
booktitle = {Computational Science and Its Applications – ICCSA 2023 Workshops: Athens, Greece, July 3–6, 2023, Proceedings, Part V},
pages = {123–132},
numpages = {10},
keywords = {Machine Learning, Virtual and Augmented Reality, Smart City, Augmented Computing, Artificial Intelligence},
location = {Athens, Greece}
}

@inproceedings{10.1145/3460112.3471954,
author = {Santy, Sebastin and Bali, Kalika and Choudhury, Monojit and Dandapat, Sandipan and Ganu, Tanuja and Shukla, Anurag and Shah, Jahanvi and Seshadri, Vivek},
title = {Language Translation as a Socio-Technical System:Case-Studies of Mixed-Initiative Interactions},
year = {2021},
isbn = {9781450384537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460112.3471954},
doi = {10.1145/3460112.3471954},
abstract = {Seamless access to information in a rapidly globalizing world demands for availability of information across, ideally all but at the least a large number of, languages. Machine translation has been proposed as a technological solution to this complex problem. However, despite seven decades of research, and recently seen rapid progress in the field - thanks to deep learning and availability of large data-sets, perfect machine translation across a large number of the world’s languages still remains elusive. In fact, it is a distant and perhaps even an impossible goal. Erroneous translations, on the other hand, can be detrimental in critical situations such as talking to a law enforcement officer; or, they could potentially perpetuate social biases or stereotypes, for instance, by producing mis-gendered translations. In this work, we argue that language translation is inherently a socio-technical system, which has to be viewed, studied, and optimized for, as such. The need and context of translation, the socio-demographic factors behind the human translators as well as the consumers of the translated content affect the complexity of the translation system, as much as the accuracy of the technology and its interface. Through a series of case studies on mixed-initiative interaction based approach to translation, we bring out the various socio-technical factors and their complex interactions that one has to bear in mind while designing for the ideal human-machine translation systems. Through these observations, we make multiple recommendations which, at the core, suggest that ”solving” translation in the real sense would require more coordinated efforts between the technical (NLP) and social communities (HCI + CSCW + DEV).},
booktitle = {Proceedings of the 4th ACM SIGCAS Conference on Computing and Sustainable Societies},
pages = {156–172},
numpages = {17},
keywords = {mixed-initiative interaction, interactive translation, socio-technical systems, human-agent interaction, human-centered AI},
location = {Virtual Event, Australia},
series = {COMPASS '21}
}

@inproceedings{10.1145/3266037.3266100,
author = {Nishida, Jun and Suzuki, Kenji},
title = {Wearable Kinesthetic I/O Device for Sharing Muscle Compliance},
year = {2018},
isbn = {9781450359498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266037.3266100},
doi = {10.1145/3266037.3266100},
abstract = {In this paper, we present a wearable kinesthetic I/O device, which is able to measure and intervene in multiple muscle activities simultaneously through the same electrodes. The developed system includes an I/O module, capable of measuring the electromyogram (EMG) of four muscle tissues, while applying electrical muscle stimulation (EMS) at the same time. The developed wearable system is configured in a scalable manner for achieving 1) high stimulus frequency (up to 70 Hz), 2) wearable dimensions in which the device can be placed along the limbs, and 3) flexibility of the number of I/O electrodes (up to 32 channels). In a pilot user study, which shared the wrist compliance between two persons, participants were able to recognize the level of their confederate's wrist joint compliance using a 4-point Likert scale. The developed system would benefit a physical therapist and a patient, during hand rehabilitation, using a peg board for sharing their wrist compliance and grip force, which are usually difficult to be observed in a visual contact.},
booktitle = {Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
pages = {57–59},
numpages = {3},
keywords = {emg, joint compliance, ems, wearable device},
location = {Berlin, Germany},
series = {UIST '18 Adjunct}
}

@inproceedings{10.1007/978-3-030-59990-4_1,
author = {Alber, Florian and Hackett, Sean and Cai, Yang},
title = {Haptic Helmet for Emergency Responses in Virtual and Live Environments},
year = {2020},
isbn = {978-3-030-59989-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59990-4_1},
doi = {10.1007/978-3-030-59990-4_1},
abstract = {Communication between team members in emergency situations is critical for first responders to ensure their safety and efficiency. In many cases, the thick smoke and noises in a burning building impair algorithms for navigational guidance. Here we present a helmet-based haptic interface with eccentric motors and communication channels. As part of the NIST PSCR Haptic Interfaces for Public Safety Challenge, our helmet with an embedded haptic interface in the headband enables communication with first responders through haptic signals about direction, measurements, and alerts. The haptic interface can be connected over LoRa for live communication or via USB to VR simulation system. With our affordable, robust, and intuitive system we took victory in the Haptic Challenge after the VR and live trials at a firefighter training facility.},
booktitle = {HCI International 2020 – Late Breaking Papers: Virtual and Augmented Reality: 22nd HCI International Conference, HCII 2020, Copenhagen, Denmark, July 19–24, 2020, Proceedings},
pages = {3–11},
numpages = {9},
keywords = {Sensor fusion, Haptics, Firefighter, Augmented reality, Haptic interface, Helmet, First response, Wearable sensors, Hyper-reality},
location = {Copenhagen, Denmark}
}

@inproceedings{10.1145/3491102.3517538,
author = {Schlosser, Paul and Matthews, Ben},
title = {Designing for Inaccessible Emergency Medical Service Contexts: Development and Evaluation of the Contextual Secondary Video Toolkit},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517538},
doi = {10.1145/3491102.3517538},
abstract = {Designing technology for emergency medical services (EMS) can be difficult, for example, due to limited access to domain experts. To support designers who aim to engage in a participatory design process in EMS environments, we created and evaluated the Contextual Secondary Video Toolkit (CSVT). This method uses secondary video material and design cards that allow domain experts to identify and prioritise challenges in their work environment and generate design ideas that address them. We illustrate the effects of the CSVT on design processes by analysing four workshops during which aeromedical EMS staff explored the potential of augmented reality to support their work. Our results indicate that the CSVT can support reflection about work practices, aid the generation of design ideas, and facilitate genuine participation. Furthermore, our data indicates that the use of secondary video in design projects is appropriate and even has certain advantages compared to primary field video.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {533},
numpages = {17},
keywords = {Design method, Emergency medical services, Participatory design, Head-worn display, Secondary video, Augmented reality},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3132211.3134459,
author = {Yi, Shanhe and Hao, Zijiang and Zhang, Qingyang and Zhang, Quan and Shi, Weisong and Li, Qun},
title = {LAVEA: Latency-Aware Video Analytics on Edge Computing Platform},
year = {2017},
isbn = {9781450350877},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132211.3134459},
doi = {10.1145/3132211.3134459},
abstract = {Along the trend pushing computation from the network core to the edge where the most of data are generated, edge computing has shown its potential in reducing response time, lowering bandwidth usage, improving energy efficiency and so on. At the same time, low-latency video analytics is becoming more and more important for applications in public safety, counter-terrorism, self-driving cars, VR/AR, etc. As those tasks are either computation intensive or bandwidth hungry, edge computing fits in well here with its ability to flexibly utilize computation and bandwidth from and between each layer. In this paper, we present LAVEA, a system built on top of an edge computing platform, which offloads computation between clients and edge nodes, collaborates nearby edge nodes, to provide low-latency video analytics at places closer to the users. We have utilized an edge-first design and formulated an optimization problem for offloading task selection and prioritized offloading requests received at the edge node to minimize the response time. In case of a saturating workload on the front edge node, we have designed and compared various task placement schemes that are tailed for inter-edge collaboration. We have implemented and evaluated our system. Our results reveal that the client-edge configuration has a speedup ranging from 1.3x to 4x (1.2x to 1.7x) against running in local (client-cloud configuration). The proposed shortest scheduling latency first scheme outputs the best overall task placement performance for inter-edge collaboration.},
booktitle = {Proceedings of the Second ACM/IEEE Symposium on Edge Computing},
articleno = {15},
numpages = {13},
keywords = {edge computing, computation offloading},
location = {San Jose, California},
series = {SEC '17}
}

@inproceedings{10.1145/3544548.3581292,
author = {Uhl, Jakob Carl and Schrom-Feiertag, Helmut and Regal, Georg and Gallhuber, Katja and Tscheligi, Manfred},
title = {Tangible Immersive Trauma Simulation: Is Mixed Reality the next Level of Medical Skills Training?},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581292},
doi = {10.1145/3544548.3581292},
abstract = {In medical simulation training two approaches are currently rather disjunct: realistic manikins are used to teach physical skills and procedures and VR systems are used to train situation assessment and decision making. We propose a mixed reality approach, which allows trainees to use real tools and their hands when interacting with a physical manikin overlaid with a responsive virtual avatar. In close exchange with first responder organizations, we developed and evaluated an MR training scenario. In the scenario, users can talk to injured people in a car accident, assess the threat of the environment, and utilize real medical equipment. Participants experienced high levels of physical- and self-presence, increased stress levels, and reported a high technology acceptance. The proposed main requirements of first responders regarding haptic multi-sensory skill training in MR and the lessons learned from the workshop aim to guide the design of training solutions for medical training in MR.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {513},
numpages = {17},
keywords = {mixed reality, haptic feedback, training, presence, first responder},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/2345316.2345327,
author = {Percivall, George},
title = {Realizing the Geospatial Potential of Mobile, IoT and Big Data},
year = {2012},
isbn = {9781450311137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2345316.2345327},
doi = {10.1145/2345316.2345327},
abstract = {What happens when you have connected sensors in everyone's pockets, homes, vehicles, workspaces, street corners, shopping areas, and more? With the convergence of Mobile Computing, the Internet of Things (IoT), and the ability to gather and analyze this Big Data, the availability of massive amounts of information will continue to be gathered and you can expect the unexpected to happen.The themes of this panel are driving development in information technology, but what is the intersection with geospatial? Location determination and use of location for context are core capabilities of Mobile and IoT. Knowing your location along with nearby Points of Interest (PoIs) and Indoor maps provide a new level of spatial awareness and decision making. This information will be used and viewed in new ways including Augmented Reality (AR). Social computing with geospatial checkins provides a rich picture of the social environment. With embedded computing becoming even more ubiquitous, Sensor Webs will provide opportunistic sensing of the physical environment. Geospatial filtering is one of the most effective methods to extracting information from these big data streams. These streams will continue to grow, e.g., mobile 3D video at incredibly high resolution. Data Fusion to combine multiple data sources will create new capabilities many based on geospatial processing.How can we realize the full potential of these technological capabilities in regards to geospatial? We can envision a lot of upside with the technology, but at what cost to privacy and rights? How should policy, privacy and rights be included in the conversations and deployments of these technologies and the resultant data? What role will ambient and participatory crowdsourcing play? A goal of our technology development must be to reduce the apparent tradeoff between surveillance for public safety vs. interests and rights of people. Technology development will continue to be a social activity based on geospatial APIs and standards for mobile platforms from organizations like W3C, OGC, IETF, and OMA. Development of these technologies are a basis for the critical outcomes, e.g, in creating Smart Cities including Smart Energy. Crowdsourcing from mobile platforms and M2M-based sensors webs will provide a basis for humanity to better understand our world and make critical decisions about the livability of our future..},
booktitle = {Proceedings of the 3rd International Conference on Computing for Geospatial Research and Applications},
articleno = {8},
numpages = {1},
location = {Washington, D.C., USA},
series = {COM.Geo '12}
}

@inproceedings{10.1145/3308560.3317049,
author = {H. Money, William and Cohen, Stephen},
title = {Leveraging AI and Sensor Fabrics to Evolve Smart City Solution Designs},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317049},
doi = {10.1145/3308560.3317049},
abstract = {Cities have entered the age of the sensor and located sensors everywhere over and under cities. The sensors monitor a host of factors that assess City operations and life such as air quality, noise, city services and traffic. Further, the sensors have “gone mobile” with announcements of situation aware mobile sensor platforms designed for city-level security and public safety. These wearable sensor platforms combine video, audio, and location data with Internet of Things (IoT) capabilities. However, the many sensors and functional platforms have not yet made the cities employing these many diverse sensors truly Smart. We are analyzing why the success toward the Smart city is limited, or late in coming. The explanations for the constrained effectiveness are assigned to many factors, but one of significance can be teased from a long-accepted explanation that associates data, information, and knowledge. Smart Cities need to effectively use the sensor data and the information assembled from these interpreted and organized data to create knowledge that serves the city and its people by answering and resolving key problems and questions. But the systems and analytic models needed to associate these data from many sensors have yet to be designed, constructed, and proven in the complex cities of today. Thus, the data (and information from the diverse sensors) lacks crucial integration and coordination for decisions and sense-making. While these sensor-based systems were, and in many cases are meeting some intended functionally discrete goals, they appear to be better described as data collection tools feeding centralized analytical engines. They are point solutions with specialized or targeted sensors feeding specialized solutions. This is a significant limiting factor in a city's drive to improve the quality of life and the efficiency of the services a city provides to its stakeholders. In this paper we present current trends in Smart City development, emerging issues with data and complexity growth, and proposes a mean to leverage the advancing technologies to address the integration problem.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {117–122},
numpages = {6},
keywords = {Sensors, Smart City, Sensor Fabric, Artificial Intelligence},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1109/ICDCSW.2005.68,
author = {Bardram, Jakob Eyvind and Bunde-Pedersen, Jonathan},
title = {IASO " An Activity-Based Computing Platform for Wearable Computing},
year = {2005},
isbn = {076952328505},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICDCSW.2005.68},
doi = {10.1109/ICDCSW.2005.68},
abstract = {Displaying and navigating complex information on wearable computers is very different from accessing the some information while sitting at your desk. Limitations in hardware, screen-size, input- and output devices are not met with changes in the software running the wearable system. We propose using the ABC-framework as a middleware layer providing collaboration, adaptation, activity sharing and context-awareness for wearable applications. Furthermore, enhancements in the user-interface and interaction techniques overcome some of the hardware limitations. The proposed system makes wearable computers usable by e.g. emergency workers, policemen and firefighters.},
booktitle = {Proceedings of the Fifth International Workshop on Smart Appliances and Wearable Computing - Volume 05},
pages = {484–490},
numpages = {7},
series = {ICDCSW '05}
}

@inproceedings{10.1145/2897839.2956558,
author = {Keenan, Thomas P.},
title = {Have We Found the Key to the Smart Community?},
year = {2016},
isbn = {9781450342827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897839.2956558},
doi = {10.1145/2897839.2956558},
abstract = {The concept of being a Smart Community has been at least since 1999 when the Intelligent Community Forum (ICF) chose Singapore as its first Intelligent Community of the Year. The ICF's criteria have been refined over the years, but they still seek out places that "understand the enormous challenges of the Broadband Economy, and have taken conscious steps to create an economy capable of prospering in it."But what does that really mean in a world where cardboard VR viewers are given away as conference swag, high school students are creating augmented reality tours of their schools, and citizens can report a pothole, or a messy neighbor, on an app like SeeClickFix?There is also an important confluence between the fields of "Smart Communities" and the concept of "Safe and Secure Communities". A community cannot really be considered "smart" if its habitants do not field "safe" going about their daily routines.On the flip side, many of the tools that will enable and enhance public safety (sensor based networks, big data analytics, public participation in decision making) have significant security and privacy implications. An innovative new program is being created at the University of Calgary to help in "Designing Smart and Secure Communities" while preserving privacy and avoiding a "Big Brother" world which would be antithetical to the goals of being a Smart Community.Thoughtful experts in this field believe that we can have many for of the benefits of being a smart community, including enhanced safety, without having to give up too much of our personal privacy. They also acknowledge that this is a tricky balance to strike, and that it will be hard work.There are excellent examples from around the world of cities that have found innovative ways to involve their citizens in a meaningful way in decisions that affect their lives. It will also consider some "platform technologies" such as non-financial applications of the blockchain, which can be used to build trust and confidence in civic applications.Things can also go horribly wrong when citizen engagement projects are poorly designed and implemented. As a creepy cautionary tale, and a warning about what might be coming down the road, we just have to look at the controversial use of "DNA Shaming" in Hong Kong to catch spitters and litterbugs.},
booktitle = {ACM SIGGRAPH 2016 Talks},
articleno = {89},
numpages = {1},
location = {Anaheim, California},
series = {SIGGRAPH '16}
}

@inproceedings{10.1145/3241539.3270097,
author = {Banerjee, Suman},
title = {Edge Computing with ParaDrop Tutorial},
year = {2018},
isbn = {9781450359030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241539.3270097},
doi = {10.1145/3241539.3270097},
abstract = {This half-day tutorial will explore edge computing through hands-on development activities using a physical edge computing platform. With the recent trends in the networking ecosystem-the rise of IoT devices, high bandwidth wireless, and powerful, energy-efficient, and inexpensive computation-edge computing is a promising technology for highly interactive and immersive environments. This tutorial aims to promote awareness of the possibilities of edge computing in general and introduce attendees to the tools that they can use to begin exploring this space. The tutorial's activities will give attendees hands-on experience with the ParaDrop edge computing platform developed at the University of Wisconsin-Madison. We hope to impart working knowledge about edge computing and our vision for its future. Recent developments in home wireless networks such as the proliferation of connected devices (e.g. Internet-of-Things) and gigabit wireless (e.g. 802.11ac) create an environment where edge computing can truly enhance applications. The availability of low latency and high bandwidth at the network edge but not necessarily end-to-end to the cloud, can enable interesting new applications involving video (e.g. augmented reality), low latency sensor-actuator coordination, and other public safety or educational applications. However, before such applications emerge, there needs to be a platform available for them. We argue that the availability of widespread platforms and standards for edge computing will unleash a new wave of innovation much like the creation of app markets for mobile devices. The hardware exists, e.g. in the form of millions of always-on Wi-Fi routers in homes and businesses, but it is not very programmable. With our tutorial, we aim to foster awareness of the possibilities that exist with edge computing and promote the advancement of research toward good standards for its realization. We introduce the ParaDrop platform as an open source demonstration of our vision for edge computing. We have been single-mindedly focused on researcher and developer needs in creating the ParaDrop platform. As such, applications that are already written to be run as cloud services can be easily modified to run on the ParaDrop platform in order to benefit from running at the network edge. Applications are not required to be written in a highly specialized language, and applications are able to leverage the rapidly growing software ecosystem surrounding Docker. Our tutorial will give researchers a hands-on experience with the platform and knowledge that they will be able to use beyond the workshop.},
booktitle = {Proceedings of the 24th Annual International Conference on Mobile Computing and Networking},
pages = {657},
numpages = {1},
location = {New Delhi, India},
series = {MobiCom '18}
}

@inproceedings{10.1109/CHASE.2017.120,
author = {Wu, Xiaopei and Dunne, Robert and Yu, Zhifeng and Shi, Weisong},
title = {STREMS: A Smart Real-Time Solution toward Enhancing EMS Prehospital Quality},
year = {2017},
isbn = {9781509047215},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CHASE.2017.120},
doi = {10.1109/CHASE.2017.120},
abstract = {Emergency medical service (EMS) systems are public services that provide quick response, transportation as well as appropriate emergency medical care to the emergent patient. For EMS, every second is critical. Unfortunately, current EMS systems have many challenges: lack effective communication between EMS providers and hospital professionals, less attention on care quality and limited resources of medical equipment and personnel. Motivated by this, in this paper, we explore the use of wearable sensing, smart mobile device as well as video technology to propose STREMS: an efficient smart real-time prehospital communication system for EMS. Specifically, we first introduce a cost-effective wearable physiological sensing solution to support multi-dimensional telemetry monitoring for an ambulance operating at as Basic Life Support, a type of EMS service level without sophisticated medical equipment or paramedics. Then we propose to build a cloud-based real-time data sharing platform, enabling automated streaming all gathered prehospital data (e.g., vital signs, EKG and image/short videos about accident scene) to the hospital prior to ambulance arrival, thus giving a more complete figure about the incoming patient. This can significantly decrease the handoff time and improve the efficiency at the hospital. Additionally, a live point to point video communication is proposed to support EMS telemedicine to enhance prehospital care quality through directly video conversation to assist EMS providers in consultation, triage, early medical examination and treatment. We implemented STREMS as an Android mobile app and evaluated its feasibility over the broadband cellular network in the city of Detroit. In a moving context, our results demonstrate STREMS can successfully deliver 100% of emergency data to the hospital in less than 1.5s, on average 0.75s for reporting a new case and 0.05s for health data. As the live video with 1280 \texttimes{} 720 pixel resolution, STREMS only works when the vehicle speed is less than 40MPH.},
booktitle = {Proceedings of the Second IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies},
pages = {365–372},
numpages = {8},
keywords = {prehospital care quality, wearable sensing, emergency medical service (EMS), real-time prehospital communication},
location = {Philadelphia, Pennsylvania},
series = {CHASE '17}
}

@inproceedings{10.1145/2535597.2535608,
author = {Siu, Teresa and Herskovic, Valeria},
title = {SidebARs: Improving Awareness of off-Screen Elements in Mobile Augmented Reality},
year = {2013},
isbn = {9781450322003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2535597.2535608},
doi = {10.1145/2535597.2535608},
abstract = {In a high-stress situation, such as an emergency, first responders (e.g. police, firefighters) require relevant information to be delivered in a timely, efficient way. Augmented reality seems like a natural way for emergency responders to find relevant information that is close to them. However, due to the limited angle and distance seen through the camera, many relevant points will be off-screen, making it difficult to quickly find the needed information. Several approaches for this problem have been proposed in previous works, however, most are designed for 2D maps, and those proposed for augmented reality do not allow users to quickly find a certain type of point of interest. We studied the emergency response scenario through a development project and several focus groups. Then, we implemented SidebARs: a prototype that implements two sidebars that allow users to quickly find the relevant information they are interested in, combined with layer filters and a slide bar to set a radius of interest. This visualization technique not only gives users awareness about the distance and direction of relevant points of interest, but also about their type. This paper presents the design and implementation of this prototype. A preliminary evaluation with firefighters found it to be a promising mechanism to find information during an emergency.},
booktitle = {Proceedings of the 2013 Chilean Conference on Human - Computer Interaction},
pages = {36–41},
numpages = {6},
keywords = {emergency response, awareness, off-screen POIs, augmented reality},
location = {Temuco, Chile},
series = {ChileCHI '13}
}

@inproceedings{10.1007/978-3-030-64823-7_3,
author = {Fruhling, Ann and Hall, Margeret and Medcalf, Sharon and Yoder, Aaron},
title = {Designing a Real-Time Integrated First Responder Health and Environmental Monitoring Dashboard},
year = {2020},
isbn = {978-3-030-64822-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64823-7_3},
doi = {10.1007/978-3-030-64823-7_3},
abstract = {Between 2007 and 2016, there were 144,002 HAZMAT incidents on US highways, with damage totaling nearly $600&nbsp;M. The top two incident types in the past three years involved flammable-combustible liquids and corrosive materials. In 2016, 38% of firefighter fatality was a result of sudden cardiac death, making it one of the two leading causes of death among firefighters. Heat-related illness is directly linked to adverse cardiovascular events [5], but when detected early, recovery is likely. We propose a new system called REaCH: Real-Time Emergency Communication System for HAZMAT Incidents. The REaCH system will include real-time health monitoring of first responders through wearable devices that capture individual health parameters and exposure to hazardous materials. Individual health data and HAZMAT exposure data will be transmitted to a dashboard that integrates all of the information for the Incident Commander to monitor. The Incident Commander can evaluate if individuals need to be removed from the scene when his/her health status is being compromised.},
booktitle = {Designing for Digital Transformation. Co-Creating Services with Citizens and Industry: 15th International Conference on Design Science Research in Information Systems and Technology, DESRIST 2020, Kristiansand, Norway, December 2–4, 2020, Proceedings},
pages = {28–34},
numpages = {7},
keywords = {First responder health, Design science, Environmental monitoring system, HAZMAT incidents, Real-time integrated dashboard},
location = {Kristiansand, Norway}
}

@inproceedings{10.1145/3145690.3145743,
author = {Sakashita, Mose and Sato, Yuta and Ebisu, Ayaka and Kawahara, Keisuke and Hashizume, Satoshi and Muramatsu, Naoya and Ochiai, Yoichi},
title = {Haptic Marionette: Wrist Control Technology Combined with Electrical Muscle Stimulation and Hanger Reflex},
year = {2017},
isbn = {9781450354059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3145690.3145743},
doi = {10.1145/3145690.3145743},
abstract = {Many devices and systems that directly control a user's hands have been proposed in previous studies. As a method for controlling a user's wrist, Hanger Reflex and Electrical Muscle Stimulation is often used. We propose a method combined Electrical muscle stimulation and Hanger Reflex. We use Hanger Reflex to elicit the supination and pronation, and EMS to cause the flexion and extension. We believe that the proposed method of this study contributes to the exploration of new devices and applications on the fields of haptics, virtual and augmented reality, mobile and wearable interfaces.},
booktitle = {SIGGRAPH Asia 2017 Posters},
articleno = {33},
numpages = {2},
keywords = {hanger reflex, EMS, feedback},
location = {Bangkok, Thailand},
series = {SA '17}
}

@inproceedings{10.1007/978-3-031-32883-1_38,
author = {Rebol, Manuel and Steinmaurer, Alexander and Gamillscheg, Florian and Pietroszek, Krzysztof and G\"{u}tl, Christian and Ranniger, Claudia and Hood, Colton and Rutenberg, Adam and Sikka, Neal},
title = {CPR Emergency Assistance Through Mixed Reality Communication},
year = {2023},
isbn = {978-3-031-32882-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-32883-1_38},
doi = {10.1007/978-3-031-32883-1_38},
abstract = {We design and evaluate a mixed reality real-time communication system for remote assistance during CPR emergencies. Our system allows an expert to guide a first responder, remotely, on how to give first aid. RGBD cameras capture a volumetric view of the local scene including the patient, the first responder, and the environment. The volumetric capture is augmented onto the remote expert’s view to spatially guide the first responder using visual and verbal instructions. We evaluate the mixed reality communication system in a research study in which participants face a simulated emergency. The first responder moves the patient to the recovery position and performs chest compressions as well as mouth-to-mask ventilation. Our study compares mixed reality against videoconferencing-based assistance using CPR performance measures, cognitive workload surveys, and semi-structured interviews. We find that more visual communication including gestures and objects is used by the remote expert when assisting in mixed reality compared to videoconferencing. Moreover, the performance and the workload for the first responder during simulation does not differ significantly between the two technologies.},
booktitle = {Augmented Intelligence and Intelligent Tutoring Systems: 19th International Conference, ITS 2023, Corfu, Greece, June 2–5, 2023, Proceedings},
pages = {415–429},
numpages = {15},
keywords = {CPR, Mixed Reality, Remote collaboration},
location = {Corfu, Greece}
}

@inproceedings{10.1145/3411763.3451772,
author = {Zhang, Jiali and Feng, He and Ngeh, Chee Jen and Raiti, John and Wang, Yuntao and Goncalves, Paulo and Sarymbekova, Gulnara and Wagner, Linda E and James, Jenna and Albee, Paul and Thiagarajan, Jay},
title = {Designing a Smart Helmet for Wildland Firefighters to Avoid Dehydration by Monitoring Bio-Signals},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451772},
doi = {10.1145/3411763.3451772},
abstract = {Smart Helmet is a new wearable device to monitor wildland firefighters’ real-time bio-signal data and alert potential health issues, i.e., dehydration. In this paper, we applied the human-centered design method to develop Smart Helmet for firefighters. We initially conducted multiple rounds of primary research to collect user needs and the deployment constraints by interviewing 80 firefighters. Targeted on dehydration caused by heat exhaustion and overexertion, we developed a smart helmet prototype, named FireWorks, with an array of sensors collecting the firefighter’s bio-signals, including body temperature, heart rate, and motions. When abnormal bio-signal levels are detected, the alert system will notify the firefighter and their supervisor. The notification is achieved by an on-device algorithm that predicts imminent health risks. Further, we designed a mobile application to display real-time and historical bio-signal data as well as alert users about potential dehydration issues. In the end, we ran user evaluation studies and iterated the prototype based on user feedback, and we ran the functional evaluation to make sure all the implemented functions work properly.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {357},
numpages = {6},
keywords = {health sensing, sensor, wearable, Firefighting, dehydration, bio-signal},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.14236/ewic/HCI2018.116,
author = {Devine, Michael and Bond, Raymond and Simms, Victoria and Boyce, Karen and Kerr, Daniel},
title = {Mapping the Health, Safety and Wellbeing Challenges of Firefighting to Wearable Devices},
year = {2018},
publisher = {BCS Learning &amp; Development Ltd.},
address = {Swindon, GBR},
url = {https://doi.org/10.14236/ewic/HCI2018.116},
doi = {10.14236/ewic/HCI2018.116},
abstract = {Firefighting remains a dangerous job and those employed face many challenges to health, safety and wellbeing. The risks faced range from immediate threat to life and limb to long term, repeated exposure to hazardous environments and arduous working conditions. Many of these hazards can be mitigated by recording exposure to risks, processing risk data, improving information flow and increasing situational awareness. Cyber-physical systems and interactive smart wearables offer a smart and seamless suite of tools that can help achieve these goals and allow emergency responders to work more efficiently and safely. Here we examine how data from wearable devices can be used to improve health, safety and wellbeing of the firefighter.},
booktitle = {Proceedings of the 32nd International BCS Human Computer Interaction Conference},
articleno = {116},
numpages = {5},
keywords = {human-data interaction, smart firefighting, biometrics},
location = {Belfast, United Kingdom},
series = {HCI '18}
}

@inproceedings{10.1109/BDCloud.2014.62,
author = {Karunarathne, M. Sajeewani and Jones, Samuel A. and Ekanayake, Samitha W. and Pathirana, Pubudu N.},
title = {Remote Monitoring System Enabling Cloud Technology upon Smart Phones and Inertial Sensors for Human Kinematics},
year = {2014},
isbn = {9781479967193},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/BDCloud.2014.62},
doi = {10.1109/BDCloud.2014.62},
abstract = {Stroke is a common neurological condition which is becoming increasingly common as the population ages. This entails healthcare monitoring systems suitable for home use, with remote access for medical professionals and emergency responders. The mobile phone is becoming the easy access tool for self-evaluation of health, but it is hindered by inherent problems including computational power and storage capacity. This research proposes a novel cloud based architecture of a biomedical system for a wearable motion kinematic analysis system which mitigates the above mentioned deficiencies of mobile devices. The system contains three subsystems: 1. Bio Kin WMS for measuring the acceleration and rotation of movement 2. Bio Kin Mobi for Mobile phone based data gathering and visualization 3. Bio Kin Cloud for data intensive computations and storage. The system is implemented as a web system and an android based mobile application. The web system communicates with the mobile application using an encrypted data structure containing sensor data and identifiable headings. The raw data, according to identifiable headings, is stored in the Amazon Relational Database Service which is automatically backed up daily. The system was deployed and tested in Amazon Web Services.},
booktitle = {Proceedings of the 2014 IEEE Fourth International Conference on Big Data and Cloud Computing},
pages = {137–142},
numpages = {6},
keywords = {Inertial Sensors, Mobile Applications, Sport, Physiotherapy, Human Activity Monitoring, Cloud Computing},
series = {BDCLOUD '14}
}

@inproceedings{10.1109/ICCPS.2018.00047,
author = {Preum, Sarah and Shu, Sile and Ting, Jonathan and Lin, Vincent and Williams, Ronald and Stankovic, John and Alemzadeh, Homa},
title = {Towards a Cognitive Assistant System for Emergency Response},
year = {2018},
isbn = {9781538653012},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICCPS.2018.00047},
doi = {10.1109/ICCPS.2018.00047},
abstract = {This abstract presents our preliminary results on development of a cognitive assistant system for emergency response that aims to improve situational awareness and safety of first responders. This system integrates a suite of smart wearable sensors, devices, and analytics for real-time collection and analysis of in-situ data from incident scene and providing dynamic data-driven insights to responders on the most effective response actions to take.},
booktitle = {Proceedings of the 9th ACM/IEEE International Conference on Cyber-Physical Systems},
pages = {347–348},
numpages = {2},
keywords = {signal processing, natural language processing, medical emergency, cognitive assistant system, EMS},
location = {Porto, Portugal},
series = {ICCPS '18}
}

@inproceedings{10.1145/3173574.3174020,
author = {Lopes, Pedro and You, Sijing and Ion, Alexandra and Baudisch, Patrick},
title = {Adding Force Feedback to Mixed Reality Experiences and Games Using Electrical Muscle Stimulation},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174020},
doi = {10.1145/3173574.3174020},
abstract = {We present a mobile system that enhances mixed reality experiences and games with force feedback by means of electrical muscle stimulation (EMS). The benefit of our approach is that it adds physical forces while keeping the users' hands free to interact unencumbered-not only with virtual objects, but also with physical objects, such as props and appliances. We demonstrate how this supports three classes of applications along the mixed-reality continuum: (1) entirely virtual objects, such as furniture with EMS friction when pushed or an EMS-based catapult game. (2) Virtual objects augmented via passive props with EMS-constraints, such as a light control panel made tangible by means of a physical cup or a balance-the-marble game with an actuated tray. (3) Augmented appliances with virtual behaviors, such as a physical thermostat dial with EMS-detents or an escape-room that repurposes lamps as levers with detents. We present a user-study in which participants rated the EMS-feedback as significantly more realistic than a no-EMS baseline.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {wearable, mr, hololens, body i/o, haptics, mixed reality, ar, augmented reality, electrical muscle stimulation},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3489410.3489428,
author = {El Raheb, Katerina and Soulis, Athanasios and Nastos, Dimitris and Lougiakis, Christos and Roussou, Maria and Christopoulos, Dimitris and Sofianopoulos, George and Papagiannis, Spyros and R\"{u}ggeberg, Jim and Katsikaris, Lucas and R\"{u}ggeberg, Julien},
title = {Eliciting Requirements for a Multisensory EXtended Reality Platform for Training and Informal Learning},
year = {2021},
isbn = {9781450385787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489410.3489428},
doi = {10.1145/3489410.3489428},
abstract = {Aiming at bridging the gap between the recent advancements in eXtended Reality (XR) research and real-world scenarios, in this paper we describe the first steps of an iterative user-centered methodology developed to elicit user requirements and to design the scenarios for a multi-sensory collaborative XR platform, in the framework of the BRIDGES project. The platform aims to be customizable and flexible, and is intended for use in different pedagogical contexts, instantiated by two pilot scenarios: a) XR training for first-responders and fire brigade staff at international airports and b) XR informal learning experiences addressed to visitors of museums and cultural centers. Through a series of workshops and focus groups with users from relevant organizations, we collected a total of nearly 100 pedagogical, technological, experiential, operational and other user needs from within these two different contexts, and discuss here the challenges and limitations but also the opportunities that were encountered.},
booktitle = {CHI Greece 2021: 1st International Conference of the ACM Greek SIGCHI Chapter},
articleno = {18},
numpages = {8},
keywords = {User-Centered Design, User Requirements, Training, Informal Learning, eXtended Reality},
location = {Online (Athens, Greece), Greece},
series = {CHI Greece 2021}
}

@inproceedings{10.1145/3152896.3152901,
author = {Tadic, S. and Vurdelja, L. and Vukajlovic, M. and Rossi, Claudio},
title = {Localization of Emergency First Responders Using UWB/GNSS with Cloud-Based Augmentation: Short Paper},
year = {2017},
isbn = {9781450354240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152896.3152901},
doi = {10.1145/3152896.3152901},
abstract = {We1 propose a real-time system for indoor/outdoor localization of first responders in emergency situations based on networking of wearable devices and cloud-based GNSS (Global Navigation Satellite Systems) augmentation. We present outdoor tests that validate this concept. With good network geometry, accuracy of the indoor positioning is similar to the accuracy commonly achieved outdoors by GNSS.},
booktitle = {Proceedings of the First CoNEXT Workshop on ICT Tools for Emergency Networks and DisastEr Relief},
pages = {24–25},
numpages = {2},
keywords = {wearable, global navigation satellite systems, positioning, emergency, localization, ultra-wideband, first responders},
location = {Incheon, Republic of Korea},
series = {I-TENDER '17}
}

@inproceedings{10.1007/978-3-030-85607-6_72,
author = {Mentler, Tilo and Palanque, Philippe and Boll, Susanne and Johnson, Chris and Van Laerhoven, Kristof},
title = {Control Rooms in Safety Critical Contexts: Design, Engineering and Evaluation Issues: IFIP WG 13.5 Workshop at INTERACT 2021},
year = {2021},
isbn = {978-3-030-85606-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-85607-6_72},
doi = {10.1007/978-3-030-85607-6_72},
abstract = {Human-Computer Interaction (HCI) research has been focussing on the design of new interaction techniques and the understanding of people and the way they interact with computing devices and new technologies. The ways in which the work is performed with these interactive technologies has arguably been less of a focus. This workshop aims at addressing this specific aspect of Human-Computer Interaction in the control rooms domain. Control rooms are crucial elements of safety-critical infrastructures (e.g., crisis management, emergency medical services, fire services, power supply, or traffic management). They have been studied in terms of Human-Computer Interaction with respect to routine and emergency operations, human-machine task allocation, interaction design and evaluation approaches for more than 30&nbsp;years. However, they are dynamic and evolving environments with, for instance, the gradual introduction of higher levels of automation/autonomy. While state of the art control rooms are still characterized by stationary workstations with several smaller screens and large wall-mounted displays, introducing mobile and wearable devices as well as IoT solutions could enable more flexible and cooperative ways of working. The workshop aims at understanding how recent technologies in HCI could change the way control rooms are designed, engineered and operated. This workshop is organized by the IFIP WG 13.5 on Human Error, Resilience, Reliability and Safety in System Development.},
booktitle = {Human-Computer Interaction – INTERACT 2021: 18th IFIP TC 13 International Conference, Bari, Italy, August 30 – September 3, 2021, Proceedings, Part V},
pages = {530–535},
numpages = {6},
keywords = {Security, UX, Pervasive computing, Resilience, Usability, Safety, Control room, Dependability},
location = {Bari, Italy}
}

@inproceedings{10.1145/2968219.2979136,
author = {Duente, Tim and Pfeiffer, Max and Rohs, Michael},
title = {On-Skin Technologies for Muscle Sensing and Actuation},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2979136},
doi = {10.1145/2968219.2979136},
abstract = {Electromyography (EMG) and electrical muscle stimulation (EMS) are promising technologies for muscle sensing and actuation in wearable interfaces. The required electrodes can be manufactured to form a thin layer on the skin. We discuss requirements and approaches for EMG and EMS as on-skin technologies. In particular, we focus on fine-grained muscle sensing and actuation with an electrode grid on the lower arm. We discuss a prototype, scenarios, and open issues.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {933–936},
numpages = {4},
keywords = {wearable, electrical muscle stimulation, electromyography, electrode grid, mobile haptic output},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/3409251.3411723,
author = {Detjen, Henrik and Geisler, Stefan and Schneegass, Stefan},
title = {“Help, Accident Ahead!”: Using Mixed Reality Environments in Automated Vehicles to Support Occupants After Passive Accident Experiences},
year = {2020},
isbn = {9781450380669},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409251.3411723},
doi = {10.1145/3409251.3411723},
abstract = {Currently, car assistant systems mainly try to prevent accidents. Increasing built-in car technology also extends the potential applications in vehicles. Future cars might have virtual windshields that augment the traffic or individual virtual assistants interacting with the user. In this paper, we explore the potential of an assistant system that helps the car’s occupants to calm down and reduce stress when they experience an accident in front of them. We present requirements from a discussion (N = 11) and derive a system design from them. Further, we test the system design in a video-based simulator study (N = 43). Our results indicate that an accident support system increases perceived control and trust and helps to calm down the user.},
booktitle = {12th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {58–61},
numpages = {4},
keywords = {Accidents, Automated Vehicles, Augmented Reality, Mixed Reality, Virtual Windshields, First Responders., Stress},
location = {Virtual Event, DC, USA},
series = {AutomotiveUI '20}
}

@inproceedings{10.1007/978-3-319-27926-8_24,
author = {Lazreg, Mehdi Ben and Radianti, Jaziar and Granmo, Ole-Christoffer},
title = {A Bayesian Network Model for Fire Assessment and Prediction},
year = {2015},
isbn = {9783319279251},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-27926-8_24},
doi = {10.1007/978-3-319-27926-8_24},
abstract = {Smartphones and other wearable computers with modern sensor technologies are becoming more advanced and widespread. This paper proposes exploiting those devices to help the firefighting operation. It introduces a Bayesian network model that infers the state of the fire and predicts its future development based on smartphone sensor data gathered within the fire area. The model provides a prediction accuracy of 84.79\"{\i} % and an area under the curve of 0.83. This solution had also been tested in the context of a fire drill and proved to help firefighters assess the fire situation and speed up their work.},
booktitle = {Revised Selected Papers of the First International Workshop on Machine Learning, Optimization, and Big Data - Volume 9432},
pages = {269–279},
numpages = {11},
keywords = {Bayesian network, Smartphone sensors, Indoor fire}
}

@inproceedings{10.1145/2935334.2935348,
author = {Pfeiffer, Max and Duente, Tim and Rohs, Michael},
title = {Let Your Body Move: A Prototyping Toolkit for Wearable Force Feedback with Electrical Muscle Stimulation},
year = {2016},
isbn = {9781450344081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935334.2935348},
doi = {10.1145/2935334.2935348},
abstract = {Electrical muscle stimulation (EMS) is a promising wearable haptic output technology as it can be miniaturized considerably and delivers a wide range of haptic output. However, prototyping EMS applications is challenging. It requires detailed knowledge and skills about hardware, software, and physiological characteristics. To simplify prototyping with EMS in mobile and wearable situations we present the Let Your Body Move toolkit. It consists of (1) a hardware control module with Bluetooth communication that uses off-the-shelf EMS devices as signal generators, (2) a simple communications protocol to connect mobile devices, and (3) a set of control applications as starting points for EMS prototyping. We describe EMS-specific parameters, electrode placements on the skin, and user calibration. The toolkit was evaluated in a workshop with 10 researchers in haptics. The results show that the toolkit allows to quickly generate non-trivial prototypes. The hardware schematics and software components are available as open source software.},
booktitle = {Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {418–427},
numpages = {10},
keywords = {toolkit, force feedback, wearable, EMS, mobile, electrical muscle stimulation, prototyping, haptic feedback},
location = {Florence, Italy},
series = {MobileHCI '16}
}

@inproceedings{10.1109/SMC42975.2020.9283338,
author = {Tiwari, Abhishek and Cassani, Raymundo and Gagnon, Jean-Fran\c{c}ois and Lafond, Daniel and Tremblay, S\'{e}bastien and Falk, Tiago H.},
title = {Movement Artifact-Robust Mental Workload Assessment During Physical Activity Using Multi-Sensor Fusion},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SMC42975.2020.9283338},
doi = {10.1109/SMC42975.2020.9283338},
abstract = {Mental workload assessment is of great importance for safety critical applications, especially in situations that involve physical demands, such as with first responders (e.g., paramedics, firefighters, or police officers). Advancements in physiological signal monitoring with wearable sensors have made way for real-time mental workload assessment using physiological signals. However, these models have typically been conducted in controlled laboratory settings and rely on a single physiological modality. As a result, such models often experience a drop in performance due to movement artifacts introduced in real-life conditions. In this paper, we demonstrate that a multi-modal mental workload model not only improves measurement accuracy, but can also increase robustness against physical activity artifacts. To this end, an experiment was conducted where mental workload and physical activity levels were modulated simultaneously while physiological data was collected from 48 participants using off-the-shelf wearable devices. Results show improved mental workload assessment with multi-modal fusion under varying physical activity conditions.},
booktitle = {2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
pages = {3471–3477},
numpages = {7},
location = {Toronto, ON}
}

@inproceedings{10.5555/851037.856538,
author = {Gandy, Maribeth and Starner, Thad and Auxier, Jake and Ashbrook, Daniel},
title = {The Gesture Pendant: A Self-Illuminating, Wearable, Infrared Computer Vision System for Home Automation Control and Medical Monitoring},
year = {2000},
isbn = {0769507956},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we present a wearable device for control of home automation systems via hand gestures. This solution has many advantages over traditional home automation interfaces in that those with loss of vision, motor skills, and mobility can use it. By combining other sources of context with the pendant, we can reduce the number and complexity of gestures while maintaining functionality. As users input gestures, the system can also analyze their movements for pathological tremors. This information can then be used for medical diagnosis, therapy, and emergency services. Currently, the Gesture Pendant can recognize control gestures with an accuracy of 95% and user-defined gestures with an accuracy of 97% it can detect tremors above 2HZ within plus or minus 0.1 Hz.},
booktitle = {Proceedings of the 4th IEEE International Symposium on Wearable Computers},
pages = {87},
keywords = {input device, enabling technology, wearable computing, home automation, computer vision, medical monitoring, tremor, gesture recognition},
series = {ISWC '00}
}

@inproceedings{10.1007/978-3-319-91467-1_19,
author = {Taylor, Glenn and Deschamps, Anthony and Tanaka, Alyssa and Nicholson, Denise and Bruder, Gerd and Welch, Gregory and Guido-Sanz, Francisco},
title = {Augmented Reality for Tactical Combat Casualty Care Training},
year = {2018},
isbn = {978-3-319-91466-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-91467-1_19},
doi = {10.1007/978-3-319-91467-1_19},
abstract = {Combat Life Savers, Combat Medics, Flight Medics, and Medical Corpsman are the first responders of the battlefield, and their training and skill maintenance is of preeminent importance to the military. While the instructors that train these groups are exceptional, the simulations of battlefield wounds are extremely simple and static, typically consisting of limited moulage with sprayed-on fake blood. These simple presentations often require the imagination of the trainee and the hard work of the instructor to convey a compelling scenario to the trainee. Augmented Reality (AR) tools offer a new and potentially valuable tool for portraying dynamic, high-fidelity visual representation of wounds to a trainee who is still able to see and operate in their real environment. To enhance medical training with more realistic hands-on experiences, we are working to develop the Combat Casualty Care Augmented Reality Intelligent Training System (C3ARESYS). C3ARESYS is our concept for an AR-based training system that aims to provide more realistic multi-sensory depictions of wounds that evolve over time and adapt to the trainee interventions. This paper describes our work to date in identifying requirements for such a training system, current state of the art and limitations in commercial augmented reality tools, and our technical approach in developing a portable training system for medical trainees.},
booktitle = {Augmented Cognition: Users and Contexts: 12th International Conference, AC 2018, Held as Part of HCI International 2018, Las Vegas, NV, USA, July 15-20, 2018, Proceedings, Part II},
pages = {227–239},
numpages = {13},
keywords = {Medical training, Augmented reality, Tactical combat casualty care, Moulage},
location = {Las Vegas, NV, USA}
}

@inproceedings{10.1145/2559206.2581171,
author = {Crowley, Danielle Ellyse and Murphy, Robin R. and McNamara, Ann and McLaughlin, Tim D. and Duncan, Brittany Anne},
title = {AR Browser for Points of Interest in Disaster Response in UAV Imagery},
year = {2014},
isbn = {9781450324748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559206.2581171},
doi = {10.1145/2559206.2581171},
abstract = {This work in progress describes AerialAR, a global positioning system (GPS) augmented reality (AR) application for mobile devices that automatically labels points of interest (POI) in unmanned aerial vehicle (UAV) imagery. This has important implications for assisting emergency responders. Existing AR applications for UAVs provide the pilot with navigational situational awareness such as terrain features; AerialAR locates and labels mission-relevant points such as schools that may need to be evacuated or hospitals to transport victims to. Locating POI in UAV imagery poses more challenges than those addressed by typical AR browsers on smartphones. The UAV operates at different altitudes as opposed to handheld devices and the UAV camera can tilt over a wide range of angles rather than simply facing forward. AerialAR overcomes these issues by developing a set of equations that translate UAV telemetry and field of view (fov) into a projection onto a Google Map. The map can then be queried for categories of POI. The current version calculates the POI distance and angles with an average error of 0.04% as compared to the Haversine and Rhumb line equations for the distance between the UAV location projected on the ground and the POI on the Google Map. Future work will complete AerialAR by processing UAV video in real-time on mobile devices.},
booktitle = {CHI '14 Extended Abstracts on Human Factors in Computing Systems},
pages = {2173–2178},
numpages = {6},
keywords = {visualization, gps, unmanned aerial vehicles, augmented reality},
location = {Toronto, Ontario, Canada},
series = {CHI EA '14}
}

@inproceedings{10.1145/2851581.2890238,
author = {Pfeiffer, Max and Duente, Tim and Rohs, Michael},
title = {A Wearable Force Feedback Toolkit with Electrical Muscle Stimulation},
year = {2016},
isbn = {9781450340823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851581.2890238},
doi = {10.1145/2851581.2890238},
abstract = {Electrical muscle stimulation (EMS) is a promising wearable haptic output technology as it can be miniaturized and delivers a wide range of tactile and force output. However, prototyping EMS applications is currently challenging and requires detailed knowledge about EMS. We present a toolkit that simplifies prototyping with EMS and serves as a starting point for experimentation and user studies. It consists of (1) a hardware control module that uses off-the-shelf EMS devices as safe signal generators, (2) a simple communication protocol, and (3) a set of control applications for prototyping. The interactivity allows hands-on experimentation with our sample control applications.},
booktitle = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {3758–3761},
numpages = {4},
keywords = {prototyping, toolkit, hapticfeedback, mobile, electrical muscle stimulation, EMS, force feedback, wearable},
location = {San Jose, California, USA},
series = {CHI EA '16}
}

@inproceedings{10.1145/3411763.3443421,
author = {Schlosser, Paul},
title = {Head-Worn Displays for Emergency Medical Services},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3443421},
doi = {10.1145/3411763.3443421},
abstract = {In the prehospital environment, head-worn displays (HWDs) could support paramedics and emergency physicians during complex tasks and procedures. Previously, HWDs have been used in emergency medical service (EMS) contexts to support triage, telemedicine, patient monitoring, and patient localization. However, research on HWDs in EMS has three limitations: (1) HWD applications have not been developed based on field research of prehospital operations and training, (2) there are few guidelines that direct HWD deployment and application design, and (3) HWD applications seldom have been tested in randomized controlled trials. Therefore, it is unclear how HWDs affect EMS work and patient outcomes. During my PhD studies, I am investigating the potential of HWDs in EMS. I am addressing the limitations of previous research by conducting a literature review, a field study, design workshops, and a controlled evaluation study. The ultimate aims of this research are to benefit the work of EMS staff and to improve patient safety.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {67},
numpages = {5},
keywords = {Augmented reality, Head-worn display, Emergency medical services},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3307334.3328607,
author = {Choi, Hong-Beom and Lim, Keun-Woo and Ko, Young-Bae},
title = {Sensor Localization System for AR-Assisted Disaster Relief Applications (Poster)},
year = {2019},
isbn = {9781450366618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307334.3328607},
doi = {10.1145/3307334.3328607},
abstract = {In this poster, we propose a sensor localization system assisted by wireless communication and augmented reality (AR) suitable for disaster relief applications. Generally, disaster environments are considered extremely hazardous and deteriorated, with unpredictable effects to human mobility and digital devices. To maximize the safety and efficiency of first responders, deployment of wireless sensors are of utmost importance, as sensor nodes can provide sensing information as well as location information. We analyze the issues and challenges that need to be tackled for high accuracy localization of sensor nodes in such environments, and then propose a system that we plan to develop in the near future.},
booktitle = {Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {526–527},
numpages = {2},
keywords = {sensor localization, visual odometry, disaster relief},
location = {Seoul, Republic of Korea},
series = {MobiSys '19}
}

@inproceedings{10.1145/3341162.3343784,
author = {Van Kleunen, Lucy and Holton, Joel and Strawn, Daniel and Voida, Stephen},
title = {Designing Navigation Aides for Wildland Firefighters},
year = {2019},
isbn = {9781450368698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341162.3343784},
doi = {10.1145/3341162.3343784},
abstract = {In this research, we explored the design of navigation technology for wildland firefighters. We worked within a set of empirically informed design constraints to create prototypes of a wearable system that provides peripheral navigation cues via visual and haptic feedback. We used physical and interactive prototypes of this system as technology probes to provoke discussions with wildland firefighters about their navigation and location technology needs. Our pilot study results indicate that our prototypes helped to uncover ideas for future technical work in the domain of wildland firefighting, as well as on mobile and wearable navigation systems, more broadly.},
booktitle = {Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers},
pages = {226–229},
numpages = {4},
keywords = {wearable technology, firefighting, technology probes, navigation},
location = {London, United Kingdom},
series = {UbiComp/ISWC '19 Adjunct}
}

@inproceedings{10.1109/IROS51168.2021.9636090,
author = {Walker, Michael and Chen, Zhaozhong and Whitlock, Matthew and Blair, David and Szafir, Danielle Albers and Heckman, Christoffer and Szafir, Daniel},
title = {A Mixed Reality Supervision and Telepresence Interface for Outdoor Field Robotics},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IROS51168.2021.9636090},
doi = {10.1109/IROS51168.2021.9636090},
abstract = {Collaborative human-robot field operations rely on timely decision-making and coordination, which can be challenging for heterogeneous teams operating in large-scale deployments. In this work, we present the design of an immersive, mixed reality (MR) interface to support sense-making and situational awareness based on the data collection capabilities of both human and robotic team members. Our solution integrates state-of-the-art methods in environment mapping and MR so that users may gain rapid insights regarding the working environment, the current and previous locations of human and robot team members, and the environment data such team members have collected. We describe the implementation of our system, share lessons learned in collaborating with emergency responders throughout our design process, and offer a vision for the use of immersive displays for human-robot field team deployments in large-scale outdoor environments.},
booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
pages = {2345–2352},
numpages = {8},
location = {Prague, Czech Republic}
}

@inproceedings{10.1109/CSSim.2009.43,
author = {Sturm, Nadine and Rainer, Karin and Chroust, Gerhard and Roth, Markus},
title = {Simulation as a New Approach to First Responders Training},
year = {2009},
isbn = {9780769537955},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CSSim.2009.43},
doi = {10.1109/CSSim.2009.43},
abstract = {The perception and awareness of chemical, biological, radiological and nuclear ("CBRN") emergencies is rising. These hazards are not directly detectable by human senses and thus no inborn reactions exist. As a consequence, special tools to detect these dangers have to be utilized. Since these dangers tend to affect large areas, it is necessary to establish standardized, coherent “Best Practices”, which have to be specifically trained under realistic but safe conditions. Modern technology allows simulating complex scenarios. The goal of the SimRad project is the user centered development and utilization of training and communication tools for all aspects of effective management of emergency situations, including team coordination. Regarding the process flow of First Responders intervention, emergency activities can be dissected into individual sub processes. This provides the basis for a purposeful optimization of individual activities through simulations, ranging from rough approximations to realistic simulations using Mixed Reality technology.},
booktitle = {Proceedings of the 2009 International Conference on Computational Intelligence, Modelling and Simulation},
pages = {159–163},
numpages = {5},
keywords = {Emergency Response, First Responder, CBRN, Simulation},
series = {CSSIM '09}
}

@inproceedings{10.5555/2093889.2093963,
author = {Onut, Iosif Viorel (Vio) and Aldridge, Don and Mindel, Marcellus and Perelgut, Stephen},
title = {2nd Workshop on Smart Surveillance System Applications},
year = {2011},
publisher = {IBM Corp.},
address = {USA},
abstract = {Motivation and Justification:Automatic recognition of people and their activities has very important social implications, because it is related to the extremely sensitive topic of civil liberties. Society needs to address this issue of automatic recognition and find a balanced solution that is able to meet its various needs and concerns. In the post 9/11 period, population security and safety considerations have given rise to research needs for identification of threatening human activities and emotional behaviours.Timely identification of human intent is one of the most challenging areas of "all-hazards" risk assessment in the protection of critical infrastructure, business continuity planning and community safety. The "all-hazards" approach is used extensively by the public and private sector, including Public Safety Canada (PS Canada -- formerly PSEPC), Emergency Management Ontario (EMO), US Federal Emergency Management Agency (FEMA) and US Department of Homeland Security (DHS).There is a clear need for industry and the research community to addresses fundamental issues involved in the prevention of human-made disasters, namely the variable context-dependent, real-time detection/identification of potential threatening behaviour of humans, acting individually or in crowded environments.Such an industry and academia forum will have to discuss development and commercialization of new multimodal (video and infrared, voice and sound, RFID and perimeter intrusion) intelligent sensor technologies for location and socio-cultural context-aware security risk assessment and decision support in human-crowd surveillance applications in environments such as school campuses, hospitals, shopping centers, subways or railway stations, airports, sports and artistic arenas etc. Due to the complexity of the surveillance task there is a clear need for the development of a distributed intelligent surveillance system architecture, which combines visual and audio surveillance based on wireless sensor nodes equipped with video or infrared (IR) cameras, audio detectors, or other object detection and motion sensors with location aware wireless sensor network solutions. The integration of visual, sound and radio tracking methods results in a highly intelligent, proactive, and adaptive surveillance and security solution sensor networks. Task-directed sensor data collection and observation planning algorithms need to be developed to allow for a more elastic and efficient use of the inherently limited sensing and processing capabilities. Each task a sensor has to carry out determines the nature and the level of the information that is actually needed. There is a need for "selective environment perception" methods that focus on object parameters that are important for the specific decision to be made for the task at hand and avoid wasting effort to process irrelevant data.Multisensor data fusion techniques should be investigated for the dynamic integration of the multi-thread flow of information provided by the heterogeneous network of surveillance sensors into a coherent multimodal model of the monitored human crowd.In the context of crowds, robust tracking of people represents an important challenge. The numerous sources of occlusions and the large diversity of interactions that might occur make difficult the long-term tracking of a particular individual over an extended period of time and using a network of sensors. Realtime image processing and computer-vision algorithms need to be studied for the identification, tracking and recognition of gait and other relevant body-language patterns of the human agents who can be deemed of interest for security reasons. Real-time signal processing algorithms have to be designed for the identification and evaluation of environmental and human-subject multimodal parameters (such as human gait, gestures, facial emotions, human voice, background sound, ambient light, etc.) that provide the contextual information for the specific surveillance activity.A multidisciplinary, context-aware, situation-assessment system, including human behaviour, cognitive psychology, multicultural sociology, learning systems, artificial intelligence, distributed multimedia and software design elements, has to be ultimately developed for the real-time evaluation of the activity and emotional behaviour of the human subjects identified as being potentially of security interest in the monitored dynamic environment.The development of such a complex system requires the seamless integration of new and improved surveillance techniques and methodologies supporting both functional and non functional requirements for surveillance networks. Functional requirements are signal processing functions and data fusion, archiving and tracking human behaviours, assessment and interpretation functions of the data, and supporting human decision makers, among others. Non-functional requirements include interoperability, scalability, availability, and manageability.The partial and heterogeneous sensor-views of the environment have to fuse into a coherent Virtualized Reality Environment (VRE) model of the explored environment. Being based on information about real/physical world objects and phenomena, as captured by a variety of sensors, VREs have more "real content" than the pure Virtual Reality environments entirely based on computer simulations. The VREs model of the explored environment allows human operators to combine their intrinsic reactive-behavior with higher-order world model representations of the immersive VRE systems.A synthetic environment will eventually be needed to provide efficient multi granularity-level function-specific feedback and human-computer interaction interfaces for the human users who are the final assessors and decision makers in the specific security monitoring situation.An ideal system should provide efficient multi granularity-level function-specific feedback for the human users who are the final assessors and decision makers in the specific security monitoring situation.The rate at which surveillance systems can currently disseminate data to evaluate new threats is mainly limited due to the developed and implemented nature of existing systems and their limited ability to operate with other systems. IBM's Service-Oriented Architecture (SOA) provides the much needed deployment ready solution which supports the integration of external systems developed by diverse industrial and institutional partners.},
booktitle = {Proceedings of the 2011 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {382–384},
numpages = {3},
location = {Toronto, Ontario, Canada},
series = {CASCON '11}
}

@inproceedings{10.5555/2399776.2399815,
author = {Onut, Iosif Viorel},
title = {3rd Workshop on Smart Surveillance System Applications},
year = {2012},
publisher = {IBM Corp.},
address = {USA},
abstract = {Automatic recognition of people and their activities has very important social implications, because it is related to the extremely sensitive topic of civil liberties. Society needs to address this issue of automatic recognition and find a balanced solution that is able to meet its various needs and concerns. In the post 9/11 period, population security and safety considerations have given rise to research needs for identification of threatening human activities and emotional behaviours.Timely identification of human intent is one of the most challenging areas of "all-hazards" risk assessment in the protection of critical infrastructure, business continuity planning and community safety. The "all-hazards" approach is used extensively by the public and private sector, including Public Safety Canada (PS Canada -- formerly PSEPC), Emergency Management Ontario (EMO), US Federal Emergency Management Agency (FEMA) and US Department of Homeland Security (DHS).There is a clear need for industry and the research community to addresses fundamental issues involved in the prevention of human-made disasters, namely the variable context-dependent, real-time detection/identification of potential threatening behaviour of humans, acting individually or in crowded environments.Such an industry and academia forum will have to discuss development and commercialization of new multimodal (video and infrared, voice and sound, RFID and perimeter intrusion) intelligent sensor technologies for location and socio-cultural context-aware security risk assessment and decision support in human-crowd surveillance applications in environments such as school campuses, hospitals, shopping centers, subways or railway stations, airports, sports and artistic arenas etc. Due to the complexity of the surveillance task, there is a clear need for the development of a distributed intelligent surveillance system architecture, which combines visual and audio surveillance based on wireless sensor nodes equipped with video or infrared (IR) cameras, audio detectors, or other object detection and motion sensors with location aware wireless sensor network solutions. The integration of visual, sound and radio tracking methods results in a highly intelligent, proactive, and adaptive surveillance and security solution sensor networks. Task-directed sensor data collection and observation planning algorithms need to be developed to allow for a more elastic and efficient use of the inherently limited sensing and processing capabilities. Each task a sensor has to carry out determines the nature and the level of the information that is actually needed. There is a need for "selective environment perception" methods that focus on object parameters that are important for the specific decision to be made for the task at hand and avoid wasting effort to process irrelevant data.Multisensor data fusion techniques should be investigated for the dynamic integration of the multi-thread flow of information provided by the heterogeneous network of surveillance sensors into a coherent multimodal model of the monitored human crowd.In the context of crowds, robust tracking of people represents an important challenge. The numerous sources of occlusions and the large diversity of interactions that might occur make difficult the long-term tracking of a particular individual over an extended period of time and using a network of sensors. Realtime image processing and computer-vision algorithms need to be studied for the identification, tracking and recognition of gait and other relevant body-language patterns of the human agents who can be deemed of interest for security reasons. Real-time signal processing algorithms have to be designed for the identification and evaluation of environmental and human-subject multimodal parameters (such as human gait, gestures, facial emotions, human voice, background sound, ambient light, etc.) that provide the contextual information for the specific surveillance activity.A multidisciplinary, context-aware, situation-assessment system, including human behaviour, cognitive psychology, multicultural sociology, learning systems, artificial intelligence, distributed multimedia and software design elements, has to be ultimately developed for the real-time evaluation of the activity and emotional behaviour of the human subjects identified as being potentially of security interest in the monitored dynamic environment.The development of such a complex system requires the seamless integration of new and improved surveillance techniques and methodologies supporting both functional and non functional requirements for surveillance networks. Functional requirements are signal processing functions and data fusion, archiving and tracking human behaviours, assessment and interpretation functions of the data, and supporting human decision makers, among others. Non-functional requirements include interoperability, scalability, availability, and manageability.The partial and heterogeneous sensor-views of the environment have to fuse into a coherent Virtualized Reality Environment (VRE) model of the explored environment. Being based on information about real/physical world objects and phenomena, as captured by a variety of sensors, VREs have more "real content" than the pure Virtual Reality environments entirely based on computer simulations. The VREs model of the explored environment allows human operators to combine their intrinsic reactive-behaviour with higher-order world model representations of the immersive VRE systems.A synthetic environment will eventually be needed to provide efficient multi granularity-level function-specific feedback and human-computer interaction interfaces for the human users who are the final assessors and decision makers in the specific security monitoring situation.An ideal system should provide efficient multi granularity-level function-specific feedback for the human users who are the final assessors and decision makers in the specific security monitoring situation.The rate at which surveillance systems can currently disseminate data to evaluate new threats is mainly limited due to the developed and implemented nature of existing systems and their limited ability to operate with other systems. IBM's Service-Oriented Architecture (SOA) provides the much needed deployment ready solution which supports the integration of external systems developed by diverse industrial and institutional partners.},
booktitle = {Proceedings of the 2012 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {262–264},
numpages = {3},
location = {Toronto, Ontario, Canada},
series = {CASCON '12}
}

@inproceedings{10.1109/ICIT.2014.73,
author = {Kumar, Manoj},
title = {Security Issues and Privacy Concerns in the Implementation of Wireless Body Area Network},
year = {2014},
isbn = {9781479980840},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICIT.2014.73},
doi = {10.1109/ICIT.2014.73},
abstract = {The rapid growth of elderly population is a global concern and a burden on healthcare services as special health challenges appear for this segment of the population. An innovative way of patient monitoring is possible due to the recent advancements in electronics that have emerged with a number of devices which can provide continuous, real time remote healthcare monitoring to the patients even if they are freely moving around and not in hospital beds. The constant miniaturization of these electronic devices has made it possible to wear these sensors either on the clothing or body or even implanted inside the body. An unprecedented growth of smart phones and Internet technology all over the world would be a boon in this area. The recorded information, sent by these wearable sensors can be collected locally using some PDA or mobile phone. These collected medical data values may be analyzed in brief against the stored threshold values using an app on the mobile phone in real time. Collective Information may be transmitted to a centralized server periodically or immediately in case of emergency medical response required in life critical situations. The submitted medical information is used for clinical diagnosis &amp; experts' advice and long term storage in healthcare database for future references. In this paper we discuss architecture of the Healthcare System and asses the security issues and privacy concerns while collecting patient medical data from sensors to mobile device and further submitting this data to the centralized server. The security and privacy protection of sensitive and private patient medical data is a major unsolved concern and a break into the system is possible. We also discuss the other challenges in the implementation of WBAN and provide a conclusion.},
booktitle = {Proceedings of the 2014 International Conference on Information Technology},
pages = {58–62},
numpages = {5},
keywords = {Bluetooth Security, WBAN, Web Security, Cryptographic algorithms, GSM Security},
series = {ICIT '14}
}

@inproceedings{10.1007/11839569_50,
author = {Song, Won Jay and Ha, Im Sook and Choi, Mun Kee},
title = {System Architecture and Economic Value-Chain Models for Healthcare Privacy and Security Control in Large-Scale Wireless Sensor Networks},
year = {2006},
isbn = {354038619X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11839569_50},
doi = {10.1007/11839569_50},
abstract = {In this paper, we have designed and modeled the ubiquitous RFID healthcare system architecture and framework workflow, which are described by six classified core players or subsystems, and have also analyzed by an economic value-chain model. They consist of the patient and wearable ECG sensor, network service, healthcare service, emergency service, and PKI service providers. To enhance the security level control for the patient’s medical privacy, individual private and public keys should be stored on smart cards. All the patient and service providers in the proposed security control architecture should have suitable secure private and public keys to access medical data and diagnosis results with RFID/GPS tracking information for emergency service. By enforcing the requirements of necessary keys among the patient and service providers, the patient’s ECG data can be protected and effectively controlled over the open medical directory service. Consequently, the proposed architecture for ubiquitous RFID healthcare system using the smart card terminal is appropriate to build up medical privacy policies in future ubiquitous sensor networking and home networking environments. In addition, we have analyzed an economic value-chain model based on the proposed architecture consisting of RFID, GPS, PDA, ECG sensor, and smart card systems in large-scale wireless sensor networks and have also derived customer needs in the proposed service architecture using the value-chain model. Therefore, we also conclude that the business and technology issues for the service providers should exist in the networks.},
booktitle = {Proceedings of the Third International Conference on Autonomic and Trusted Computing},
pages = {511–520},
numpages = {10},
location = {Wuhan, China},
series = {ATC'06}
}

@inproceedings{10.1007/11875567_18,
author = {Song, Won Jay and Cho, Moon Kyo and Ha, Im Sook and Choi, Mun Kee},
title = {Healthcare System Architecture, Economic Value, and Policy Models in Large-Scale Wireless Sensor Networks},
year = {2006},
isbn = {3540457623},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11875567_18},
doi = {10.1007/11875567_18},
abstract = {In this paper, we have designed and modeled the ubiquitous RFID healthcare system architecture and framework workflow, which are described by six classified core players or subsystems, and have also analyzed by an economic value-chain model. They consist of the patient and wearable ECG sensor, network service, healthcare service, emergency service, and PKI service providers. To enhance the security level control for the patient's medical privacy, individual private and public keys should be stored on smart cards. All the patient and service providers in the proposed security control architecture should have suitable secure private and public keys to access medical data and diagnosis results with RFID/GPS tracking information for emergency service. By enforcing the requirements of necessary keys among the patient and service providers, the patient's ECG data can be protected and effectively controlled over the open medical directory service. Consequently, the proposed architecture for ubiquitous RFID healthcare system using the smart card terminal is appropriate to build up medical privacy policies in future ubiquitous sensor networking and home networking environments. In addition, we have analyzed an economic value-chain model based on the proposed architecture consisting of RFID, GPS, PDA, ECG sensor, and smart card systems in large-scale wireless sensor networks and have also analyzed two market derivers – customer demands and technology – in the proposed service architecture using the value-chain model. Finally, policy modeling for privacy and security protection for customers, service providers, and regulatory agency is considered to promote beneficial utilization of the collected healthcare data and derived new business of healthcare applications.},
booktitle = {Proceedings of the 25th International Conference on Computer Safety, Reliability, and Security},
pages = {233–246},
numpages = {14},
location = {Gdansk, Poland},
series = {SAFECOMP'06}
}

@inproceedings{10.1145/3131785.3131806,
author = {Lopes, Pedro and Baudisch, Patrick},
title = {Demonstrating Interactive Systems Based on Electrical Muscle Stimulation},
year = {2017},
isbn = {9781450354196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131785.3131806},
doi = {10.1145/3131785.3131806},
abstract = {We provide a hands-on demonstration of the potential of interactive systems based on electrical muscle stimulation (EMS). These wearable devices allow attendees, for example, to physically learn how to manipulate objects they never seen before, feel walls and forces in virtual reality, and so forth. In our demo we plan to not only demonstrate several of these EMS-based prototypes but also to provide instructions and free hardware for people to conduct their first projects using EMS.},
booktitle = {Adjunct Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology},
pages = {47–49},
numpages = {3},
keywords = {wearable, body I/O, haptics, electrical muscle stimulation},
location = {Qu\'{e}bec City, QC, Canada},
series = {UIST '17 Adjunct}
}

@inproceedings{10.1145/3098279.3098546,
author = {Duente, Tim and Pfeiffer, Max and Rohs, Michael},
title = {Zap++: A 20-Channel Electrical Muscle Stimulation System for Fine-Grained Wearable Force Feedback},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3098546},
doi = {10.1145/3098279.3098546},
abstract = {Electrical muscle stimulation (EMS) has been used successfully in HCI to generate force feedback and simple movements both in stationary and mobile settings. However, many natural limb movements require the coordinated actuation of multiple muscles. Off-the-shelf EMS devices are typically limited in their ability to generate fine-grained movements, because they only have a low number of channels and do not provide full control over the EMS parameters. More capable medical devices are not designed for mobile use or still have a lower number of channels and less control than is desirable for HCI research. In this paper we present the concept and a prototype of a 20-channel mobile EMS system that offers full control over the EMS parameters. We discuss the requirements of wearable multi-electrode EMS systems and present the design and technical evaluation of our prototype. We further outline several application scenarios and discuss safety and certification issues.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {1},
numpages = {13},
keywords = {wearable, wearable force feedback, electrical muscle stimulation, mobile, mobile haptic output, electrode grid},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/2836041.2841215,
author = {Hahn, J\"{u}rgen and Ludwig, Bernd and Wolff, Christian},
title = {Augmented Reality-Based Training of the PCB Assembly Process},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2841215},
doi = {10.1145/2836041.2841215},
abstract = {In this paper we propose an augmented reality (AR) based assistance system for reliably teaching the assembly process of printed circuit boards (PCB) to workers by using a smart glass running a self-developed software. The system is operated freehand by looking at QR-Codes and highlights a component's retrieval location and installation point in the user's field of vision by using four markers. A study executed in a production line of an Electronics Manufacturing Services (EMS)-company resulted in an errorless performance of each individual participant who was equipped with the system. This paper describes the related work, concept and implementation of the software as well as the conducted study and its results. Finally a conclusion summarizes the success of the system and hints at future work.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {395–399},
numpages = {5},
keywords = {industrial assembly, mobile information processing, augmented reality},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1109/CBMS.2008.59,
author = {Leijdekkers, Peter and Gay, Val\'{e}rie},
title = {A Self-Test to Detect a Heart Attack Using a Mobile Phone and Wearable Sensors},
year = {2008},
isbn = {9780769531656},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CBMS.2008.59},
doi = {10.1109/CBMS.2008.59},
abstract = {This paper describes a heart attack self-test application for a mobile phone that allows potential victims, without the intervention of a medical specialist, to quickly assess whether they are having a heart attack. Heart attacks can occur anytime and anywhere. Using pervasive technology such as a mobile phone and a small wearable ECG sensor it is possible to collect the user's symptoms and to detect the onset of a heart attack by analysing the ECG recordings. If the application assesses that the user is at risk, it will urge the user to call the emergency services immediately. If the user has a cardiac arrest the application will automatically determine the current location of the user and alert the ambulance services and others to the person's location.},
booktitle = {Proceedings of the 2008 21st IEEE International Symposium on Computer-Based Medical Systems},
pages = {93–98},
numpages = {6},
keywords = {wearable sensors, mobile health monitoring, heart attack selft-test, mobile health application},
series = {CBMS '08}
}

@inproceedings{10.1145/3384657.3384780,
author = {Goto, Takashi and Das, Swagata and Wolf, Katrin and Lopes, Pedro and Kurita, Yuichi and Kunze, Kai},
title = {Accelerating Skill Acquisition of Two-Handed Drumming Using Pneumatic Artificial Muscles},
year = {2020},
isbn = {9781450376037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384657.3384780},
doi = {10.1145/3384657.3384780},
abstract = {While computers excel at augmenting user's cognitive abilities, only recently we started utilizing their full potential to enhance our physical abilities. More and more wearable force-feedback devices have been developed based on exoskeletons, electrical muscle stimulation (EMS) or pneumatic actuators. The latter, pneumatic-based artificial muscles, are of particular interest since they strike an interesting balance: lighter than exoskeletons and more precise than EMS. However, the promise of using artificial muscles to actually support skill acquisition and training users is still lacking empirical validation.In this paper, we unveil how pneumatic artificial muscles impact skill acquisition, using two-handed drumming as an example use case. To understand this, we conducted a user study comparing participants' drumming performance after training with the audio or with our artificial-muscle setup. Our haptic system is comprised of four pneumatic muscles and is capable of actuating the user's forearm to drum accurately up to 80 bpm. We show that pneumatic muscles improve participants' correct recall of drumming patterns significantly when compared to auditory training.},
booktitle = {Proceedings of the Augmented Humans International Conference},
articleno = {12},
numpages = {9},
keywords = {Force-feedback, pneumatic artificial muscles (PAMs), motor learning},
location = {Kaiserslautern, Germany},
series = {AHs '20}
}

@inproceedings{10.1145/3532721.3535567,
author = {Tanaka, Yudai and Nishida, Jun and Lopes, Pedro},
title = {Demonstration of Electrical Head Actuation: Enabling Interactive Systems to Directly Manipulate Head Orientation},
year = {2022},
isbn = {9781450393638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532721.3535567},
doi = {10.1145/3532721.3535567},
abstract = {We demonstrate a novel interface concept in which interactive systems directly manipulate the user’s head orientation. We implement this using electrical-muscle-stimulation (EMS) of the neck muscles, which turns the head around its yaw (left/right) and pitch (up/down) axis. At SIGGRAPH 2022 Emerging Techinologies, we will demonstrate how this technology enables novel interactions via two example applications: (1) finding different visual targets in mixed reality while the system actuates the user’s head orientation to guide their point-of-view; (2) a VR roller coaster application where the user’s head nods up as the ride accelerates.},
booktitle = {ACM SIGGRAPH 2022 Emerging Technologies},
articleno = {1},
numpages = {2},
location = {Vancouver, BC, Canada},
series = {SIGGRAPH '22}
}

@inproceedings{10.1007/978-3-031-15553-6_18,
author = {Praschl, Christoph and Thiele, Erik and Krauss, Oliver},
title = {Utilization of Geographic Data for the Creation of Occlusion Models in the Context of Mixed Reality Applications},
year = {2022},
isbn = {978-3-031-15552-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-15553-6_18},
doi = {10.1007/978-3-031-15553-6_18},
abstract = {Emergency responder training can benefit from outdoor use of Mixed Reality (MR) devices to make trainings more realistic and allow simulations that would otherwise not be possible due to safety risks or cost-effectiveness. But outdoor use of MR requires knowledge of the topography and objects in the area to enable accurate interaction of the real world trainees experience and the virtual elements that are placed in them. An approach utilizing elevation data and geographic information systems to create effective occlusion models is shown, that can be used in such outdoor training simulations. The initial results show that this approach enables accurate occlusion and placement of virtual objects within an urban environment. This improves immersion and spatial perception for trainees. In the future, improvements of the approach are planned with on the fly updates to outdated information in the occlusion models.},
booktitle = {Extended Reality: First International Conference, XR Salento 2022, Lecce, Italy, July 6–8, 2022, Proceedings, Part II},
pages = {236–253},
numpages = {18},
keywords = {Elevation data, Geographic information service, Mixed Reality, Occlusion},
location = {Lecce, Italy}
}

@inproceedings{10.1145/3616195.3616220,
author = {Elliot, Alan and McGregor, Iain},
title = {FASS: Firefighter Audio Safety Systems},
year = {2023},
isbn = {9798400708183},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616195.3616220},
doi = {10.1145/3616195.3616220},
abstract = {A series of auditory cues were designed to assist firefighters with navigation and general safety in a fire emergency. Firefighters must maintain situational awareness at all times and this can be lost with disorientation, which is one of the main causes of injury and even death. Disorientation can be caused by restricted vision due to heavy smoke, a lack of familiarity with the surroundings as well as hearing and communication difficulties caused by the intensity of the fireground sounds. Five professional firefighters were interviewed to identify ways in which auditory affordances could be used to support their work. Existing sounds from both the emergency environment and those generated by firefighting equipment were assessed to determine their importance in maintaining situational awareness. Noise reduction technology was investigated, to assess its potential use in limiting the levels of noise exposure experienced. A series of auditory cues were designed to address the issues that were found using binaural spatialization and Augmented Reality methods. A prototype system was presented to firefighters to determine its effectiveness. The firefighters found that noise reduction would be effective in improving their situational awareness and ability to communicate effectively. Additionally, the firefighters found that spatially placed auditory cues had the potential to be effective in navigation and orientation in a fire emergency. The findings suggest that the use of noise reduction and auditory affordances have the potential to improve situational awareness for firefighters, increase safety and potentially save lives.},
booktitle = {Proceedings of the 18th International Audio Mostly Conference},
pages = {177–184},
numpages = {8},
location = {Edinburgh, United Kingdom},
series = {AM '23}
}

@inproceedings{10.1145/3132272.3132288,
author = {Daiber, Florian and Kosmalla, Felix and Wiehr, Frederik and Kr\"{u}ger, Antonio},
title = {FootStriker: A Wearable EMS-Based Foot Strike Assistant for Running},
year = {2017},
isbn = {9781450346917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132272.3132288},
doi = {10.1145/3132272.3132288},
abstract = {Today, ambitious amateur athletes often do not have access to professional coaching but still invest great effort in becoming faster runners. Apart from a pure increase in the quantitative training load, a change of the running technique, e.g. transitioning from heel striking to mid-/forefoot running, can be highly effective and usually prevents knee-related injuries.In this demo, we present a self-contained wearable that detects heel striking while running with a pressure-sensitive insole. Heel striking is corrected in real-time to mid/forefoot running by applying electrical muscle stimulation (EMS) on the calf muscle. We further discuss potential scenarios for EMS-based training in interactive spaces. The device will be worn and demonstrated by the presenter but if possible, the device can also be tested directly by the conference attendees.},
booktitle = {Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces},
pages = {421–424},
numpages = {4},
keywords = {Running, Motor Learning, Real-time Assistance, In-situ Feedback, Wearables, Electrical Muscle Stimulation, Sports Training, Motor Skills, Real-time Feedback},
location = {Brighton, United Kingdom},
series = {ISS '17}
}

@inproceedings{10.1145/2968219.2972714,
author = {Baratchi, Mitra and Teunissen, Lennart and Ebben, Peter and Teeuw, Wouter and Laarhuis, Jan and van Steen, Maarten},
title = {Towards Decisive Garments for Heat Stress Risk Detection},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2972714},
doi = {10.1145/2968219.2972714},
abstract = {One of the numerous applications of wearable computers is providing safety in occupations where heat-related injuries are prevalent. Core temperature, as a parameter that cannot be measured by on-body sensors is a variable that is specifically interesting for realizing such applications. In the context of the design of a sensor-shirt that can be used by firefighters, in this paper we study the importance of different types of sensor measurements and their placement for estimating core temperature. We propose a model for inferring the dangerous states of core temperature. Our evaluation results show that our model can to a great extent estimate hazardous situations caused by heat accumulation.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {1095–1100},
numpages = {6},
keywords = {heat stress, wearable computers, smart-shirt, machine learning, ambulatory physiological monitoring, sensor-shirt},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/3568444.3568460,
author = {Faltaous, Sarah and Koelle, Marion and Schneegass, Stefan},
title = {From Perception to Action: A Review and Taxonomy on Electrical Muscle Stimulation in HCI},
year = {2022},
isbn = {9781450398206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568444.3568460},
doi = {10.1145/3568444.3568460},
abstract = {The last decade has witnessed an increase in Electrical Muscle Stimulation (EMS) research. Myriads of application scenarios have been proposed and toolkits, as well as procedures, have been presented to the research community. While these individual explorations provide a benefit to the research community, the overarching view on the topic is still unclear which helps to understand how such technology can be used in the future. To tackle this, we conduct a systematic literature survey of research utilizing EMS as a wearable output method. We investigate the published research (N = 151) by analyzing the EMS systems’ setup, calibration, and evaluation. We create a taxonomy of the application scenarios that categorize them along two dimensions (i.e., (1) action &lt;&gt; perception and (2) augmentation &lt;&gt; induction). Further, we distill open research challenges from our results that can help to stimulate and guide research on EMS in the future.},
booktitle = {Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia},
pages = {159–171},
numpages = {13},
keywords = {EMS, Taxonomy, Literature Survey, Electrical Muscle Stimulation},
location = {Lisbon, Portugal},
series = {MUM '22}
}

@inproceedings{10.5555/1513605.1513730,
author = {Bourbakis, Nikolaos and Gallagher, John},
title = {A Synergistic Co-Operative Framework of Health Diagnostic Systems for People with Disabilities and the Elderly: A Case Study},
year = {2008},
isbn = {9789606766855},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {Millions of people worldwide have (or will due to aging) experienced some kind of unexpected disability throughout their lives. For this population health care costs increase, quality of life and productivity decline, and in many cases family members serve as primary care assistants. Early detection and diagnosis of critical health changes could enable prevention of most of these problems, saving billions of dollars annually (Newsweek, 2006 - "Fixing the American Hospital" [31,32]). Early detection, however, requires continual vigilance. Due to the nature of their conditions or the lack of training and experience, many among this population are patients of either disinclined or unable to detect and report the critical observations that could make a difference. A common solution is for health care professionals to monitor patients directly or via relatively crude patient data collection devices; however, that solution does not scale to large populations. New generation of inexpensive, unobtrusive wearable/ implanted devices automatically detect critical changes on their health condition. These devices will not be simple data collection appliances, nor will they only report variations from sampled population norms. Rather, they will learn individual user baselines and employ advanced detection and diagnostics to discover problems autonomously and signal medical professionals for further assistance. These wearable/implanted systems will be engineered to integrate seamlessly both with portable equipment carried by first-responders and with fixed-location systems installed in hospitals. In addition to providing advanced detection in field, our devices will continually capture data, organize it into customized patient and condition models, and communicate each patient's unique information to first-responders and hospital personnel. This paper describes the fundamental steps for the systematic development and co-operation of three layers of health diagnosis systems. These explorations will result in new fundamental knowledge and catalyze a revolution in low-cost, high-reliability, ubiquitously deployable automated health detection and diagnostic equipment.},
booktitle = {Proceedings of the 12th WSEAS International Conference on Computers},
pages = {730–735},
numpages = {6},
location = {Heraklion, Greece},
series = {ICCOMP'08}
}

@inproceedings{10.1145/2836041.2836047,
author = {Alavesa, Paula and Ojala, Timo},
title = {Street Art Gangs: Location Based Hybrid Reality Game},
year = {2015},
isbn = {9781450336055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2836041.2836047},
doi = {10.1145/2836041.2836047},
abstract = {We present a location based mixed reality game called Street Art Gangs that we have developed to explore the playful appropriation of the hybrid reality comprising of a city center and its detailed virtual replica represented as a 3D virtual model. In the real streets SAG is played with a mobile phone app that allows tagging predefined locations around the city to claim their ownership and busting nearby players of competing gangs. The virtual game world is viewed with a PC app that allows observing the current owners of taggable locations, the locations of other players, and the locations of patrolling virtual policemen busting players. We have developed two incremental versions of SAG that have been evaluated with tournaments in the wild. We conceptualize the findings of the tournaments with de Souza e Silva's theoretical framework for hybrid reality games. Our findings suggest that players preferred to play the game on real city streets while the added value of the virtual game world remained marginal. The size of the game both in terms of area and the number of taggable locations turned out to have a major impact on gameplay.},
booktitle = {Proceedings of the 14th International Conference on Mobile and Ubiquitous Multimedia},
pages = {64–74},
numpages = {11},
keywords = {hybrid reality games, hybrid space, location based games, pervasive gaming, virtual worlds},
location = {Linz, Austria},
series = {MUM '15}
}

@inproceedings{10.1145/3025453.3025829,
author = {Nishida, Jun and Suzuki, Kenji},
title = {BioSync: A Paired Wearable Device for Blending Kinesthetic Experience},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025829},
doi = {10.1145/3025453.3025829},
abstract = {We present a novel, paired, wearable system for combining the kinesthetic experiences of two persons. These devices allow users to sense and combine muscle contraction and joint rigidity bi-directionally. This is achieved through kinesthetic channels based on electromyogram (EMG) measurement and electrical muscle stimulation (EMS). We developed a pair of wearable kinesthetic input-output (I/O) devices called bioSync that uses specially designed electrodes to perform biosignal measurement and stimulation simultaneously on the same electrodes.In a user study, participants successfully evaluated the strength of their partners' muscle contractions while exerting their own muscles. We confirmed that the pair of devices could help participants synchronize their hand movements through tapping, without visual and auditory feedback. The proposed interpersonal kinesthetic communication system can be used to enhance interactions such as clinical gait rehabilitation and sports training, and facilitate sharing of physical experiences with Parkinson's patients, thereby enhancing understanding of the physical challenges they face in daily life.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {3316–3327},
numpages = {12},
keywords = {rehabilitation, blending kinesthetic experience, electromyogram signals, electrical muscle stimulation},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{10.1145/1029632.1029661,
author = {Kaye, Joseph 'Jofish'},
title = {Olfactory Display},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029661},
doi = {10.1145/1029632.1029661},
abstract = {The last twenty years have seen enormous leaps forward in computers' abilities to generate sound and video. What happens when computers can produce scents on demand? In this talk, I present three approaches to this question. I first look at human olfactory processing: what is our olfactory bandwidth, and what are the limitations of our sense of smell? I then explore the use of scent to accompany other media, from historical examples like Sense-o-Rama and Aromarama, to more recent work including firefighter training systems, augmented gaming, and food and beverage applications. Finally, I look at the possibilities of olfactory output as an ambient display medium. I conclude with an overview of current computer-controlled olfactory output devices: off the shelf solutions for incorporating scent into user interface applications.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {163},
numpages = {1},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/3472749.3474759,
author = {Nith, Romain and Teng, Shan-Yuan and Li, Pengyu and Tao, Yujie and Lopes, Pedro},
title = {DextrEMS: Increasing Dexterity in Electrical Muscle Stimulation by Combining It with Brakes},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474759},
doi = {10.1145/3472749.3474759},
abstract = {Electrical muscle stimulation (EMS) is an emergent technique that miniaturizes force feedback, especially popular for untethered haptic devices, such as mobile gaming, VR, or AR. However, the actuation displayed by interactive systems based on EMS is coarse and imprecise. EMS systems mostly focus on inducing movements in large muscle groups such as legs, arms, and wrists; whereas individual finger poses, which would be required, for example, to actuate a user's fingers to fingerspell even the simplest letters in sign language, are not possible. The lack of dexterity in EMS stems from two fundamental limitations: (1) lack of independence: when a particular finger is actuated by EMS, the current runs through nearby muscles, causing unwanted actuation of adjacent fingers; and, (2) unwanted oscillations: while it is relatively easy for EMS to start moving a finger, it is very hard for EMS to stop and hold that finger at a precise angle; because, to stop a finger, virtually all EMS systems contract the opposing muscle, typically achieved via controllers (e.g., PID)—unfortunately, even with the best controller tuning, this often results in unwanted oscillations. To tackle these limitations, we propose dextrEMS, an EMS-based haptic device featuring mechanical brakes attached to each finger joint. The key idea behind dextrEMS is that while the EMS actuates the fingers, it is our mechanical brake that stops the finger in a precise position. Moreover, it is also the brakes that allow dextrEMS to select which fingers are moved by EMS, eliminating unwanted movements by preventing adjacent fingers from moving. We implemented dextrEMS as an untethered haptic device, weighing only 68g, that actuates eight finger joints independently (metacarpophalangeal and proximal interphalangeal joints for four fingers), which we demonstrate in a wide range of haptic applications, such as assisted fingerspelling, a piano tutorial, guitar tutorial, and a VR game. Finally, in our technical evaluation, we found that dextrEMS outperformed EMS alone by doubling its independence and reducing unwanted oscillations.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {414–430},
numpages = {17},
keywords = {dexterity, force feedback, haptics, electrical muscle stimulation, exoskeleton},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/2984751.2985743,
author = {Tamaki, Emi and Chan, Terence and Iwasaki, Ken},
title = {UnlimitedHand: Input and Output Hand Gestures with Less Calibration Time},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985743},
doi = {10.1145/2984751.2985743},
abstract = {Numerous devices that either track hand gestures or provide haptic feedback have been developed with the aim of manipulating objects within Virtual Reality(VR) and Augmented Reality(AR) environments. However, these devices implement lengthy calibration processes to ease out individual differences. In this research, a wearable device that simultaneously recognizes hand gestures and outputs haptic feedback: UnlimitedHand is suggested. Photo-reflectors are placed over specific muscle groups on the forearm to read in hand gestures. For output, electrodes are placed over the same muscles to control the user's hand movements. Both sensors and electrodes target main muscle groups responsible for moving the hand. Since the positions of these muscle groups are common between humans, UnlimitedHand is able to reduce the time spent on performing calibration.},
booktitle = {Adjunct Proceedings of the 29th Annual ACM Symposium on User Interface Software and Technology},
pages = {163–165},
numpages = {3},
keywords = {haptic sensation, vr(virtual reality), photo-reflector array, hand gesture, fes(functional electric stimulation), ar(augmented reality), ems(electric muscle stimulation), electric stimulation},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/3491101.3519904,
author = {Tanaka, Yudai and Nishida, Jun and Lopes, Pedro},
title = {Demonstrating Electrical Head Actuation: Enabling Interactive Systems to Directly Manipulate Head Orientation},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3519904},
doi = {10.1145/3491101.3519904},
abstract = {We demonstrate a novel interface concept in which interactive systems directly manipulate the user's head orientation. We implement this using electrical-muscle-stimulation (EMS) of the neck muscles, which turns the head around its yaw (left/right) and pitch (up/down) axis. As the first exploration of EMS for head actuation, we characterized which muscles can be robustly actuated. Then, we demonstrated how it enables interactions not possible before by building a range of applications, such as (1) directly changing the user's head orientation to locate objects in AR; (2) a sound controller that uses neck movements as both input and output; (3) synchronizing head orientations of two users, which enables a user to communicate head nods to another user while listening to music; and (4) rendering force feedback from VR punches on the head by actuating the user's neck.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {182},
numpages = {5},
keywords = {Electrical Muscle Stimulation, Virtual Reality, Augmented Reality, Haptics},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{10.1145/3491102.3501910,
author = {Tanaka, Yudai and Nishida, Jun and Lopes, Pedro},
title = {Electrical Head Actuation: Enabling Interactive Systems to Directly Manipulate Head Orientation},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501910},
doi = {10.1145/3491102.3501910},
abstract = {We propose a novel interface concept in which interactive systems directly manipulate the user's head orientation. We implement this using electrical-muscle-stimulation (EMS) of the neck muscles, which turns the head around its yaw (left/right) and pitch (up/down) axis. As the first exploration of EMS for head actuation, we characterized which muscles can be robustly actuated. Second, we evaluated the accuracy of our system for actuating participants' head orientation towards static targets and trajectories. Third, we demonstrated how it enables interactions not possible before by building a range of applications, such as (1) synchronizing head orientations of two users, which enables a user to communicate head nods to another user while listening to music, and (2) directly changing the user's head orientation to locate objects in AR. Finally, in our second study, participants felt that our head actuation contributed positively to their experience in four distinct applications.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {262},
numpages = {15},
keywords = {Virtual Reality, Haptics, Augmented Reality, Electrical Muscle Stimulation},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3131785.3131828,
author = {Fortin, Pascal E. and Blum, Jeffrey R. and Cooperstock, Jeremy R.},
title = {Raising the Heat: Electrical Muscle Stimulation for Simulated Heat Withdrawal Response},
year = {2017},
isbn = {9781450354196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131785.3131828},
doi = {10.1145/3131785.3131828},
abstract = {Virtual Reality (VR) has numerous mechanisms for making a virtual scene more compellingly real. Most effort has been focused on visual and auditory techniques for immersive environments, although some commercial systems now include relatively crude haptic effects through handheld controllers or haptic suits. We present results from a pilot experiment demonstrating the use of Electrical Muscle Stimulation (EMS) to trick participants into thinking a surface is dangerously hot even though it is below 50C. This is accomplished by inducing an artificial heat withdrawal reflex by contracting the participant's bicep shortly after contact with the virtual hot surface. Although the effects of multiple experimental confounds need to be quantified in future work, results so far suggest that EMS could potentially be used to modify temperature perception in VR and AR contexts. Such an illusion has applications for VR gaming as well as emergency response and workplace training and simulation, in addition to providing new insights into the human perceptual system.},
booktitle = {Adjunct Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology},
pages = {137–139},
numpages = {3},
keywords = {thermal interface, perception, electrical muscle stimulation},
location = {Qu\'{e}bec City, QC, Canada},
series = {UIST '17 Adjunct}
}

@inproceedings{10.1007/978-3-031-35748-0_10,
author = {Reisher, Elizabeth and Jonnalagadda, Soundarya and Fruhling, Ann},
title = {Applying the Trajectories Conceptual Framework: A Case Study of an IoT Health Data Monitoring Application},
year = {2023},
isbn = {978-3-031-35747-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-35748-0_10},
doi = {10.1007/978-3-031-35748-0_10},
abstract = {We present a case study on the design and development of an IoT application, called Real-time Emergency Communication System for HAZMAT Incidents (REaCH). The REaCH application utilizes the latest IoT technologies to capture and monitor individual health data (heart rate, ECG, heat index) and provides an interactive dashboard to assist incident commanders to evaluate if a firefighter needs to be removed from the scene due to a potential threat to their health and well-being. In our study we examined the design and development of a dashboard utilizing the Trajectories Conceptual Framework (TCF) and Action Research (AR). The aim of this paper is to analyze if TCF can guide the development of a dashboard from research to industry. Our study consists of four cycles that span from the requirement gathering phase to delivering a minimal viable product. We conclude that the integration of TCF and AR provides a solid approach to evaluate the design and develop a real-time health monitoring IoT dashboard. We also propose the addition of a Developer Trajectory to address the technical aspects and help guide both the front-end and back-end developers through the various stages of the applications development using IoT technology.},
booktitle = {Digital Human Modeling and Applications in Health, Safety, Ergonomics and Risk Management: 14th International Conference, DHM 2023, Held as Part of the 25th HCI International Conference, HCII 2023, Copenhagen, Denmark, July 23–28, 2023, Proceedings, Part II},
pages = {138–153},
numpages = {16},
keywords = {TCF, dashboard design, HCI, Action Research, trajectories framework, IoT},
location = {Copenhagen, Denmark}
}

@article{10.1007/s10055-020-00436-8,
author = {Koutitas, George and Smith, Scott and Lawrence, Grayson},
title = {Performance Evaluation of AR/VR Training Technologies for EMS First Responders},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {1},
issn = {1359-4338},
url = {https://doi.org/10.1007/s10055-020-00436-8},
doi = {10.1007/s10055-020-00436-8},
abstract = {The first responder training sector presents crucial difficulties on adopting “future of work” online training principles because physical (muscle) memory is considered as important as cognitive memory. It is obvious that physical memory cannot be obtained by existing screen- and paper-based trainings. This paper presents a novel training framework for first responders that leverages augmented reality and virtual reality technologies. The framework incorporates novel design thinking processes that are implemented for the design of the training experiences. In addition, a qualitative and quantitative analysis of various metrics such as performance, time on task, accuracy and learning rate are developed to analyze the effectiveness of the proposed framework. A special use case of the emergency medical services called the ambulance bus is investigated and it is shown that the proposed training methodology improved the accuracy of the first responders by a factor of 46\% and the speed on executing tasks by 29\%.},
journal = {Virtual Real.},
month = {mar},
pages = {83–94},
numpages = {12},
keywords = {Learning technologies, Augmented reality, First responders, Virtual reality, Evaluation, Training}
}

@article{10.1007/s11042-015-2955-0,
author = {Sebillo, Monica and Vitiello, Giuliana and Paolino, Luca and Ginige, Athula},
title = {Training Emergency Responders through Augmented Reality Mobile Interfaces},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {75},
number = {16},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-015-2955-0},
doi = {10.1007/s11042-015-2955-0},
abstract = {In the domain of emergency management, in addition to the constant technical skill-upgrade required by the nature of the humanitarian context, the importance of an appropriate training is widely recognized. In particular, giving responders information technology skills so that they are well prepared to address health, security and managerial concerns represents a key factor by which the goal of an efficient and effective humanitarian emergency response can be pursued. In this paper we propose the adoption of augmented reality mobile interfaces to enhance the training efficacy for on-site crisis preparedness activities. The system we propose originated from the idea to allow trainees to exploit Augmented Reality (AR) interaction and become quickly familiar with the mobile technology adopted today in emergency response activities.},
journal = {Multimedia Tools Appl.},
month = {aug},
pages = {9609–9622},
numpages = {14},
keywords = {Information sharing, Mobile interfaces, AR-based training applications, Situation awareness, Emergency management}
}

@article{10.4018/IJISCRAM.2015070101,
author = {Berndt, Henrik and Mentler, Tilo and Herczeg, Michael},
title = {Optical Head-Mounted Displays in Mass Casualty Incidents: Keeping an Eye on Patients and Hazardous Materials},
year = {2015},
issue_date = {July 2015},
publisher = {IGI Global},
address = {USA},
volume = {7},
number = {3},
issn = {1937-9390},
url = {https://doi.org/10.4018/IJISCRAM.2015070101},
doi = {10.4018/IJISCRAM.2015070101},
abstract = {Optical head-mounted displays OHMDs could support members of emergency medical services in responding to and managing mass casualty incidents. In this contribution, the authors describe the human-centered design of two applications for supporting the triage process as well as the identification of hazardous materials. They were evaluated with members of emergency medical services and civil protection units. In this regard, challenges and approaches to human-computer interaction with OHMDs in crisis response and management are discussed. The conclusion is drawn that often mentioned advantages of OHMDs like hands-free interaction alone will not lead to usable solutions for safety-critical domains. Interaction design needs to be carefully considered right down to the last detail.},
journal = {Int. J. Inf. Syst. Crisis Response Manag.},
month = {jul},
pages = {1–15},
numpages = {15},
keywords = {Triage, Optical Head-Mounted Displays, Google Glass, Safety-Critical Human-Computer Systems, Human-Computer Interaction, Mass Casualty Incident}
}

@article{10.4018/IJDCF.2020010103,
author = {Bouafif, Hana and Kamoun, Faouzi and Iqbal, Farkhund},
title = {Towards a Better Understanding of Drone Forensics: A Case Study of Parrot AR Drone 2.0},
year = {2020},
issue_date = {Jan 2020},
publisher = {IGI Global},
address = {USA},
volume = {12},
number = {1},
issn = {1941-6210},
url = {https://doi.org/10.4018/IJDCF.2020010103},
doi = {10.4018/IJDCF.2020010103},
abstract = {Unmanned aerial vehicles (drones) have gained increased popularity as their innovative uses continue to expand across various fields. Despite their numerous beneficial uses, drones have unfortunately been misused, through many reported cases, to launch illegal and sometimes criminal activities that pose direct threats to individuals, organizations, public safety and national security. These threats have recently led law enforcement agencies and digital forensic investigators to pay special attention to the forensic aspects of drones. This important research topic, however, remains underexplored. This study aims to further explore drone forensics in terms of challenges, forensic investigation procedures and experimental results through a forensic investigation study performed on a Parrot AR drone 2.0. In this study, the authors present new insights on drone forensics in terms of forensic approaches, access to drone's digital containers and the retrieval of key information that can assist digital forensic investigators establish ownership, recuperate flight data and gain access to media files.},
journal = {Int. J. Digit. Crime For.},
month = {jan},
pages = {35–57},
numpages = {23},
keywords = {Forensic Investigation, Drone Forensics, Drone, Unmanned Aerial System, Unmanned Aerial Vehicle, UAV}
}

@article{10.1109/TITB.2010.2047727,
author = {Curone, D. and Tognetti, A. and Secco, E. L. and Anania, G. and Carbonaro, N. and De Rossi, D. and Magenes, G.},
title = {Heart Rate and Accelerometer Data Fusion for Activity Assessment of Rescuers during Emergency Interventions},
year = {2010},
issue_date = {May 2010},
publisher = {IEEE Press},
volume = {14},
number = {3},
issn = {1089-7771},
url = {https://doi.org/10.1109/TITB.2010.2047727},
doi = {10.1109/TITB.2010.2047727},
abstract = {The current state of the art in wearable electronics is the integration of very small devices into textile fabrics, the so-called "smart garment." The ProeTEX project is one of many initiatives dedicated to the development of smart garments specifically designed for people who risk their lives in the line of duty such as fire fighters and Civil Protection rescuers. These garments have integrated multipurpose sensors that monitor their activities while in action. To this aim, we have developed an algorithm that combines both features extracted from the signal of a triaxial accelerometer and one ECG lead. Microprocessors integrated in the garments detect the signal magnitude area of inertial acceleration, step frequency, trunk inclination, heart rate (HR), and HR trend in real time. Given these inputs, a classifier assigns these signals to nine classes differentiating between certain physical activities (walking, running, moving on site), intensities (intense, mild, or at rest) and postures (lying down, standing up). Specific classes will be identified as dangerous to the rescuer during operation, such as, "subject motionless lying down" or "subject resting with abnormal HR." Laboratory tests were carried out on seven healthy adult subjects with the collection of over 4.5 h of data. The results were very positive, achieving an overall classification accuracy of 88.8\%},
journal = {Trans. Info. Tech. Biomed.},
month = {may},
pages = {702–710},
numpages = {9},
keywords = {smart protective textiles, accelerometer, heart rate (HR), sensor fusion, Accelerometer, wearable electronics}
}

@article{10.1155/2020/8891664,
author = {Guo, Naixuan and Luo, Junzhou and Ling, Zhen and Yang, Ming and Wu, Wenjia and Gu, Xiaodan and Xu, Xiaolong},
title = {A Novel IM Sync Message-Based Cross-Device Tracking},
year = {2020},
issue_date = {2020},
publisher = {John Wiley \&amp; Sons, Inc.},
address = {USA},
volume = {2020},
issn = {1939-0114},
url = {https://doi.org/10.1155/2020/8891664},
doi = {10.1155/2020/8891664},
abstract = {Cybercrime is significantly growing as the development of internet technology. To mitigate this issue, the law enforcement adopts network surveillance technology to track a suspect and derive the online profile. However, the traditional network surveillance using the single-device tracking method can only acquire part of a suspect’s online activities. With the emergence of different types of devices (e.g., personal computers, mobile phones, and smart wearable devices) in the mobile edge computing (MEC) environment, one suspect can employ multiple devices to launch a cybercrime. In this paper, we investigate a novel cross-device tracking approach which is able to correlate one suspect’s different devices so as to help the law enforcement monitor a suspect’s online activities more comprehensively. Our approach is based on the network traffic analysis of instant messaging (IM) applications, which are typical commercial service providers (CSPs) in the MEC environment. We notice a new habit of using IM applications, that is, one individual logs in the same account on multiple devices. This habit brings about devices’ receiving sync messages, which can be utilized to correlate devices. We choose five popular apps (i.e., WhatsApp, Facebook Messenger, WeChat, QQ, and Skype) to prove our approach’s effectiveness. The experimental results show that our approach can identify IM messages with high F1-scores (e.g., QQ’s PC message is 0.966, and QQ’s phone message is 0.924) and achieve an average correlating accuracy of 89.58\% of five apps in an 8-people experiment, with the fastest correlation speed achieved in 100 s.},
journal = {Sec. and Commun. Netw.},
month = {jan},
numpages = {15}
}

@article{10.1109/MCOM.2016.7452265,
author = {Butun, Ismail and Erol-Kantarci, Melike and Kantarci, Burak and Song, Houbing},
title = {Cloud-Centric Multi-Level Authentication as a Service for Secure Public Safety Device Networks},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {54},
number = {4},
issn = {0163-6804},
url = {https://doi.org/10.1109/MCOM.2016.7452265},
doi = {10.1109/MCOM.2016.7452265},
abstract = {With the advances in IoT, future public safety responders will be well armed with devices that pump data between on-site responders and command centers, carrying useful information about the event scene, the status of a mission, and helping critical decisions to be made in real time. In addition, wearable and on-body sensors will monitor the vital signals and well being of the responders. These connected devices or the so-called IoT surrounding public safety responders generate highly vulnerable data, where security breaches may have life threatening consequences. Authentication of responder devices is essential in order to control access to public safety networks. Most of the existing authentication schemes do not scale well with the large number of devices of IoT, and are not fast enough to work during time-critical public safety missions. On the other hand, for general IoT services, cloud-based solutions provide unlimited resources for storing and accessing IoT data. However, the cloud may have some implications for sensitive data that are collected for public safety. Therefore, authentication solutions are desired to integrate well into the cloud environment. In this article, we propose cloud-centric, multi-level authentication as a service approach that addresses scalability and time constraints, and demonstrate its effectiveness. We draw future research directions for secure public safety networks in the presence of IoT devices and the cloud.},
journal = {Comm. Mag.},
month = {apr},
pages = {47–53},
numpages = {7}
}

@article{10.1007/s11042-019-08488-y,
author = {Gao, Guangwei and Wang, Yannan and Huang, Pu and Chang, Heyou and Lu, Huimin and Yue, Dong},
title = {Locality-Constrained Feature Space Learning for Cross-Resolution Sketch-Photo Face Recognition},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {21–22},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-08488-y},
doi = {10.1007/s11042-019-08488-y},
abstract = {Matching sketch facial images to mug-shot images have crucial significance in law enforcement and digital entertainment. Conventional methods always assume that both the sketch and photo face images have the same resolutions. However, in real criminal detection, the target facial sketches obtained by the artist usually have different resolutions against the source photos in the mug-shot database. In this paper, we propose a locality-constrained feature space learning (LCFSL) method to address the above cross-resolution sketch-photo facial images matching problem. The proposed LCFSL approach not only build bridge to associate cross-domain face images, but also can learn resolution robust representation features for cross-resolution sketch-photo face recognition purpose. After common feature space learning, we simply use nearest neighbor classifier to perform recognition based on the projected features obtained from sketch-photo faces with different resolutions. Experiments conducted on CUHK student database and AR database have shown the effectiveness and superiority of our method to some state-of-the-art face recognition approaches.},
journal = {Multimedia Tools Appl.},
month = {jun},
pages = {14903–14917},
numpages = {15},
keywords = {Locality-constrained, Face recognition, Feature learning, Cross-resolution}
}

@article{10.1016/j.eswa.2009.07.011,
author = {Mporas, Iosif and Kocsis, Otilia and Ganchev, Todor and Fakotakis, Nikos},
title = {Robust Speech Interaction in Motorcycle Environment},
year = {2010},
issue_date = {March, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {37},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2009.07.011},
doi = {10.1016/j.eswa.2009.07.011},
abstract = {Aiming at robust spoken dialogue interaction in motorcycle environment, we investigate various configurations for a speech front-end, which consists of speech pre-processing, speech enhancement and speech recognition components. These components are implemented as agents in the Olympus/RavenClaw framework, which is the core of a multimodal dialogue interaction interface of a wearable solution for information support of the motorcycle police force on the move. In the present effort, aiming at optimizing the speech recognition performance, different experimental setups are considered for the speech front-end. The practical value of various speech enhancement techniques is assessed and, after analysis of their performances, a collaborative scheme is proposed. In this collaborative scheme independent speech enhancement channels operate in parallel on a common input and their outputs are fed to the multithread speech recognition component. The outcome of the speech recognition process is post-processed by an appropriate fusion technique, which contributes for a more accurate interpretation of the input. Investigating various fusion algorithms, we identified the Adaboost.M1 algorithm as the one performing best. Utilizing the fusion collaborative scheme based on the Adaboost.M1 algorithm, significant improvement of the overall speech recognition performance was achieved. This is expressed in terms of word recognition rate and correctly recognized words, as accuracy gain of 8.0\% and 5.48\%, respectively, when compared to the performance of the best speech enhancement channel, alone. The advance offered in the present work reaches beyond the specifics of the present application, and can be beneficial to spoken interfaces operating in non-stationary noise environments.},
journal = {Expert Syst. Appl.},
month = {mar},
pages = {1827–1835},
numpages = {9},
keywords = {Speech enhancement, Data fusion, Agent-based system, Speech recognition, Motorcycle environment}
}

@article{10.1016/j.amc.2017.07.058,
author = {Shang, Kun and Huang, Zheng-Hai and Liu, Wanquan and Li, Zhi-Ming},
title = {A Single Gallery-Based Face Recognition Using Extended Joint Sparse Representation},
year = {2018},
issue_date = {March 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {320},
number = {C},
issn = {0096-3003},
url = {https://doi.org/10.1016/j.amc.2017.07.058},
doi = {10.1016/j.amc.2017.07.058},
abstract = {For many practical face recognition problems, such as law enforcement, e-passport, ID card identification, and video surveillance, there is usually only a single sample per person enrolled for training, meanwhile the probe samples can usually be captured on the spot, it is possible to collect multiple face images per person. This is a new face recognition problem with many challenges, and we name it as the single-image-to-image-set face recognition problem (ISFR). In this paper, a customized dictionary-based face recognition approach is proposed to solve this problem using the extended joint sparse representation. We first learn a customized variation dictionary from the on-location probing face images, and then propose the extended joint sparse representation, which utilizes the information of both the customized dictionary and the gallery samples, to classify the probe samples. Finally we compare the proposed method with the related methods on several popular face databases, including Yale, AR, CMU-PIE, Georgia, Multi-PIE and LFW databases. The experimental results show that the proposed method outperforms most of these popular face recognition methods for the ISFR problem.},
journal = {Appl. Math. Comput.},
month = {mar},
pages = {99–115},
numpages = {17},
keywords = {Dictionary learning, Customized dictionary, Face recognition, Extended joint sparse representation, Single image to image set}
}

@article{10.1007/s10044-018-0755-7,
author = {Radman, Abduljalil and Suandi, Shahrel Azmin},
title = {Markov Random Fields and Facial Landmarks for Handling Uncontrolled Images of Face Sketch Synthesis},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {1},
issn = {1433-7541},
url = {https://doi.org/10.1007/s10044-018-0755-7},
doi = {10.1007/s10044-018-0755-7},
abstract = {Face sketch synthesis has drawn great attention in many computer vision applications such as law enforcement and digital entertainment. The majority of existing face sketch synthesis techniques are exemplar-based techniques, where a set of training photo---sketch pairs are first divided into patches. For an input photo patch from the face to be synthesized, k similar photo patches are found from the training set. The corresponding sketch patch of the best match is then selected to be synthesized. In such techniques, a multiscale Markov random fields (MRF) model is utilized for synthesizing a sketch using candidate sketch patches; having observed that techniques tend to fail with face photos acquired in uncontrolled imaging conditions like pose and lighting variations. For example, some structures along the lower part of the face sketch contour get lost due to ignoring the global face shape information and illumination changes. In this paper, we propose a reliable face sketch synthesis method based on MRF model and facial landmarks, called MRF-FL that can maintain further structures with uncontrolled face photos. Besides matching the input photo with training photo, the input photo and training sketch are also matched based on the facial landmarks so as to enhance face sketch structures around the lower part of face sketch contour. Experimental results showed that the proposed MRF-FL achieves superior performance compared with recent face sketch synthesis methods on CUHK and AR face sketch databases.},
journal = {Pattern Anal. Appl.},
month = {feb},
pages = {259–271},
numpages = {13},
keywords = {Markov random fields, Face sketch synthesis, Facial landmarks, Face recognition}
}

@article{10.1109/TMC.2020.3007740,
author = {Shang, Jiacheng and Chen, Si and Wu, Jie and Yin, Shu},
title = {ARSpy: Breaking Location-Based Multi-Player Augmented Reality Application for User Location Tracking},
year = {2022},
issue_date = {Feb. 2022},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {21},
number = {2},
issn = {1536-1233},
url = {https://doi.org/10.1109/TMC.2020.3007740},
doi = {10.1109/TMC.2020.3007740},
abstract = {Augmented reality (AR) applications that overlay the perception of the real world with digitally generated information are on the cusp of commercial viability. AR has appeared in several commercial platforms like Microsoft HoloLens and smartphones. They extend the user experience beyond two dimensions and supplement the normal 3D world of a user. A typical location-based multi-player AR application works through a three-step process, wherein the system collects sensory data from the real world, identifies objects based on their context, and finally, renders information on top of senses of a user. However, because these AR applications frequently exchange data with users, they have exposed new individual and public safety issues. In this paper, we develop ARSpy, a user location tracking system solely based on network traffic information of the user, and we test it on location-based multi-player AR applications. We demonstrate the effectiveness and efficiency of the proposed scheme via real-world experiments on 12 volunteers and show that we could obtain the geolocation of any target with high accuracy. We also propose three mitigation methods to mitigate these side channel attacks. Our results reveal a potential security threat in current location-based multi-player AR applications and serve as a critical security reminder to a vast number of AR users.},
journal = {IEEE Transactions on Mobile Computing},
month = {feb},
pages = {433–447},
numpages = {15}
}

@article{10.1109/MCOM.2017.1600443CM,
author = {Orsino, Antonino and Ometov, Aleksandr and Fodor, Gabor and Moltchanov, Dmitri and Militano, Leonardo and Andreev, Sergey and Yilmaz, Osman N. C. and Tirronen, Tuomas and Torsner, Johan and Araniti, Giuseppe and Iera, Antonio and Dohler, Mischa and Koucheryavy, Yevgeni},
title = {Effects of Heterogeneous Mobility on D2D- and Drone-Assisted Mission-Critical MTC in 5G},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {55},
number = {2},
issn = {0163-6804},
url = {https://doi.org/10.1109/MCOM.2017.1600443CM},
doi = {10.1109/MCOM.2017.1600443CM},
abstract = {mcMTC is starting to play a central role in the industrial Internet of Things ecosystem and have the potential to create high-revenue businesses, including intelligent transportation systems, energy/ smart grid control, public safety services, and high-end wearable applications. Consequently, in the 5G of wireless networks, mcMTC have imposed a wide range of requirements on the enabling technology, such as low power, high reliability, and low latency connectivity. Recognizing these challenges, the recent and ongoing releases of LTE systems incorporate support for lowcost and enhanced coverage, reduced latency, and high reliability for devices at varying levels of mobility. In this article, we examine the effects of heterogeneous user and device mobility -- produced by a mixture of various mobility patterns -- on the performance of mcMTC across three representative scenarios within a multi-connectivity 5G network. We establish that the availability of alternative connectivity options, such as D2D links and drone-assisted access, helps meet the requirements of mcMTC applications in a wide range of scenarios, including industrial automation, vehicular connectivity, and urban communications. In particular, we confirm improvements of up to 40 percent in link availability and reliability with the use of proximate connections on top of the cellular-only baseline.},
journal = {Comm. Mag.},
month = {feb},
pages = {79–87},
numpages = {9}
}

@article{10.1109/TCSVT.2016.2615445,
author = {Chakraborty, Anirban and Mandal, Bappaditya and Yuan, Junsong},
title = {Person Reidentification Using Multiple Egocentric Views},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {1051-8215},
url = {https://doi.org/10.1109/TCSVT.2016.2615445},
doi = {10.1109/TCSVT.2016.2615445},
abstract = {Development of a robust and scalable multicamera surveillance system is the need of the hour to ensure public safety and security. Being able to reidentify and track one or more targets over multiple nonoverlapping camera field of views in a crowded environment remains an important and challenging problem because of occlusions, large change in the viewpoints, and illumination across cameras. However, the rise of wearable imaging devices has led to new avenues in solving the reidentification (re-id) problem. Unlike static cameras, where the views are often restricted or low resolution and occlusions are common scenarios, egocentric/first person views (FPVs) mostly get zoomed in, unoccluded face images. In this paper, we present a person re-id framework designed for a network of multiple wearable devices. The proposed framework builds on commonly used facial feature extraction and similarity computation methods between camera pairs and utilizes a data association method to yield globally optimal and consistent re-id results with much improved accuracy. Moreover, to ensure its utility in practical applications where a large amount of observations are available every instant, an online scheme is proposed as a direct extension of the batch method. This can dynamically associate new observations to already observed and labeled targets in an iterative fashion. We tested both the offline and online methods on realistic FPV video databases, collected using multiple wearable cameras in a complex office environment and observed large improvements in performance when compared with the state of the arts.},
journal = {IEEE Trans. Cir. and Sys. for Video Technol.},
month = {mar},
pages = {484–498},
numpages = {15}
}

@article{10.1016/j.asoc.2023.110613,
author = {Li, Pengrui and Zhang, Yongqing and Liu, Shihong and Lin, Liqi and Zhang, Haokai and Tang, Tian and Gao, Dongrui},
title = {An EEG-Based Brain Cognitive Dynamic Recognition Network for Representations of Brain Fatigue},
year = {2023},
issue_date = {Oct 2023},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {146},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2023.110613},
doi = {10.1016/j.asoc.2023.110613},
journal = {Appl. Soft Comput.},
month = {oct},
numpages = {15},
keywords = {5-D Brain Cognitive Dynamic Recognition Network, 3-D brain global cognitive power map, 1-D brain local cognitive array, Electroencephalogram, The brain region}
}

@article{10.1016/j.knosys.2019.05.033,
author = {Wang, Xing and Zhang, Bob and Yang, Meng and Ke, Kangyin and Zheng, Weishi},
title = {Robust Joint Representation with Triple Local Feature for Face Recognition with Single Sample per Person},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {181},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.05.033},
doi = {10.1016/j.knosys.2019.05.033},
journal = {Know.-Based Syst.},
month = {oct},
numpages = {16},
keywords = {Robust joint representation, Face recognition, Triple local feature, Single sample per person}
}

@article{10.1109/MPRV.2004.18,
author = {Lorincz, Konrad and Malan, David J. and Fulford-Jones, Thaddeus R. F. and Nawoj, Alan and Clavel, Antony and Shnayder, Victor and Mainland, Geoffrey and Welsh, Matt and Moulton, Steve},
title = {Sensor Networks for Emergency Response: Challenges and Opportunities},
year = {2004},
issue_date = {October 2004},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {3},
number = {4},
issn = {1536-1268},
url = {https://doi.org/10.1109/MPRV.2004.18},
doi = {10.1109/MPRV.2004.18},
abstract = {Sensor networks are an emerging technology consisting of extremely low-power, small, and low-cost nodes integrating sensing, computation, and communication capabilities. This article explores their use in emergency medical care and disaster response and highlights some challenges that must be addressed in these environments' instrumentation. Specifically, the authors focus on the need for a common protocol and software framework to integrate a range of devices into a disaster response scenario, including wearable vital sign sensors, handheld computers, and location-tracking tags. Their architecture, CodeBlue, provides naming and discovery, robust-routing, and security services, specifically tailored to critical-care applications. To model and test this architecture, they've developed small, low-power pulse oximetry and two-lead EKG sensors, as well as a robust radio frequency-based location tracking system that can survive significant tracking infrastructure failures. By allowing continuous, real-time, noninvasive, wireless monitoring and tracking of multiple patients and first responders, these technologies have the potential to revolutionize emergency care.},
journal = {IEEE Pervasive Computing},
month = {oct},
pages = {16–23},
numpages = {8},
keywords = {CodeBlue, Emergency Response, Vital Sign Sensors, MoteTrack, Wireless Sensor Networks, Robust Location Tracking}
}

@article{10.4018/IJCWT.305859,
author = {Rossetti, Pietro and Garzia, Fabio and Genco, Nicola Silverio and Sacchetti, Antonio},
title = {IoT and Edge Computing as Enabling Technologies of Human Factors Monitoring in CBRN Environment},
year = {2022},
issue_date = {Jul 2022},
publisher = {IGI Global},
address = {USA},
volume = {12},
number = {2},
issn = {1947-3435},
url = {https://doi.org/10.4018/IJCWT.305859},
doi = {10.4018/IJCWT.305859},
abstract = {Human Factor (HF) monitoring under the critical CBRN environment reduces the likelihood of errors or injuries by first responders who carry out operations within an unknown workspace. Indeed, thanks to the monitoring, analysis and assessment of physical and mental workload and manual handling of equipements by first responders, it is possible to increase safety, efficiency and effectiveness. The IoT and Edge computing, contextualizing the collected data, promises to enhance the CBRN situational awareness by working on the HFs aspects with a view to context-aware reasoning. IoT and edge computing enabling technologies envisage operators needs and behaviours by gathering information about biophysiological conditions, emotional state and operational data by first responders.This study aims to introduce the edge computer for data fusion in tactical networks and computational services, fully integrated in IoT solutions of remote monitoring of HFs. These latter, related to Human Performance and health of responders, may prove useful as innovative tools in CBRN incident management.},
journal = {Int. J. Cyber Warf. Terror.},
month = {jul},
pages = {1–20},
numpages = {20},
keywords = {IoT, CBRN Response, Internet of Things, Edge Computer, Human Factor, Context Awareness, Wearable, First Responder, Smart Monitoring, CBRN, Preparedness}
}

@article{10.4018/IJTHI.317749,
author = {Sever, Filip},
title = {Assessment of the Building Situation Tool Adoption Among Firefighters},
year = {2023},
issue_date = {Aug 2023},
publisher = {IGI Global},
address = {USA},
volume = {19},
number = {1},
issn = {1548-3908},
url = {https://doi.org/10.4018/IJTHI.317749},
doi = {10.4018/IJTHI.317749},
abstract = {Technology is a standard tool that first responders use in their assessment and planning during disasters. Despite the considerable number of hardware and software solutions adopted, first responders still often rely on paper plans when examining indoor disasters. The purpose of this research is to investigate the technical competencies of firefighters and test the building situation tool (BUST) to replace the paper plans. A mixed method approach was used to assess the technology self-efficacy and gather insight into perceived usefulness, ease of use, and the user experience from the firefighters (N=20). The findings show a sufficient level of competency, and that first time users prefer guided instructions, clarity in the user interface, controls, and options to customize the user interface. The findings have practical implications for the future development of BUST and its adoption to the workflow of firefighters.},
journal = {Int. J. Technol. Hum. Interact.},
month = {feb},
pages = {1–17},
numpages = {17},
keywords = {Technology Self-Efficacy, Disaster Management, Workplace Learning, Perceived Ease of Use, User Experience, Indoor Disaster Management, Perceived Usefulness}
}

@article{10.1155/2022/9174756,
author = {Su, Xiao and Chen, Nan and Hernandez, Juan Vicente Capella},
title = {Intelligent Information Service System of Smart Library Based on Virtual Reality and Eye Movement Technology},
year = {2022},
issue_date = {2022},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2022},
issn = {1058-9244},
url = {https://doi.org/10.1155/2022/9174756},
doi = {10.1155/2022/9174756},
abstract = {An innovative system provides intelligent services in the library to both users and the terminal. Compared to the core digital reading room, they can make wise judgments on the retrieval and use of information assets. The implementation of succeeding value management based on the latest technological tools is required for learning to provide knowledge services and fulfil its role as convergence is capable of reacting to varied data needs. The major obstacles to digital libraries are lack of planning and software, import restrictions on equipment, inadequately skilled staff, lack of standards, and a refusal to cooperate. This paper introduces the Eye Movement Technology (EMT) for the intelligent information service system of a smart library (SL) based on virtual reality (VR) strengths as an automated library management system for cost savings and increased output. EMT technology that can identify a person’s appearance and monitor something to look at in real-time is recognized as wearable technology. Visual field position and the gaze vectors for each eye are converted into a data stream that includes the focus origin. Most of the library’s operations can be managed by librarians; in a word, this system allows them to keep track of all of their books’ transactions. The research in EMT-SL technology in the intelligent library system has developed to the point that a wider audience can use it. As a result, eye-tracking in reading rooms and information science research are expected to rise in the future; with the advancement of VR, the computer-generated modelling of images is experienced through special electrical devices, and people can be visible in different climates using traditional VR and augmented reality.},
journal = {Sci. Program.},
month = {jan},
numpages = {12}
}

@article{10.3233/THC-213009,
author = {Li, Fen and Shankar, Achyut and Santhosh Kumar, B. and Balamurugan, S. and Muthu, BalaAnand and Peng, Sheng-Lung and Abd Wahab, Mohd Helmy},
title = {Fog-Internet of Things-Assisted Multi-Sensor Intelligent Monitoring Model to Analyze the Physical Health Condition},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {29},
number = {6},
issn = {0928-7329},
url = {https://doi.org/10.3233/THC-213009},
doi = {10.3233/THC-213009},
journal = {Technol. Health Care},
month = {jan},
pages = {1319–1337},
numpages = {19},
keywords = {health monitoring system, cloud computing, fog computing, multi-sensor system. physical health condition, IoT}
}

@article{10.1016/j.chb.2015.07.040,
author = {Yilmaz, Rabia M.},
title = {Educational Magic Toys Developed with Augmented Reality Technology for Early Childhood Education},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {0747-5632},
url = {https://doi.org/10.1016/j.chb.2015.07.040},
doi = {10.1016/j.chb.2015.07.040},
abstract = {Shaping children's experience, enhancing their imagination and affecting their behaviors, toys have great importance. Recently, toys have gained a digital characteristic and many children have tended to use them. For this reason, educational magic toys (EMT) were developed with augmented reality technology in this study. It is called as EMT because virtual objects such as story animations, 3D objects and flash animations appear on the toys. EMT has included puzzles, flash cards and match cards to teach animals, fruits, vegetables, vehicles, objects, professions, colors, numbers and shapes for average 5-6 age children in Early Childhood Education. The aim of this study is to reveal teachers' and children's opinions on EMT, to determine children's behavioral patterns and their cognitive attainment, and the relationship between them while playing EMT. Mix method was used and the sample consisted of 30 teachers and 33 children aged 5-6 in early childhood education. As data collection tools, a survey, an observation and interview form were used. This study revealed that teachers and children liked EMT activity. In addition, children interactively played with these toys but not had high cognitive attainment. From this point, we can say that these toys can be effectively used in early childhood education. However, collaborative and interactive learning with these toys should be provided. Moreover, this study will provide an important contribution, present a new educational AR application, and fill the gap in the educational technology field. Educational magic toys were developed by augmented reality technology for children.A new educational AR application was revealed as educational magic toys.All teachers and children liked educational magic toys in early childhood education.Children interactively played with educational magic toys.Children had not high cognitive attainment while playing educational magic toys.},
journal = {Comput. Hum. Behav.},
month = {jan},
pages = {240–248},
numpages = {9},
keywords = {Augmented reality, Early childhood education, Toys}
}

@article{10.1016/j.compeleceng.2022.108323,
author = {Ponnan, Suresh and Theivadas, J Robert and VS, HemaKumar and Einarson, Daniel},
title = {Driver Monitoring and Passenger Interaction System Using Wearable Device in Intelligent Vehicle},
year = {2022},
issue_date = {Oct 2022},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {103},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2022.108323},
doi = {10.1016/j.compeleceng.2022.108323},
journal = {Comput. Electr. Eng.},
month = {oct},
numpages = {11},
keywords = {Electronic health system (ehs), Internet of vehicle, Wearable sensors, SQL database, Driver's biological monitoring, Artificial intelligence}
}

@article{10.1007/s11227-022-04453-z,
author = {Nayak, Janmenjoy and Meher, Saroj K. and Souri, Alireza and Naik, Bighnaraj and Vimal, S.},
title = {Extreme Learning Machine and Bayesian Optimization-Driven Intelligent Framework for IoMT Cyber-Attack Detection},
year = {2022},
issue_date = {Sep 2022},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {13},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-022-04453-z},
doi = {10.1007/s11227-022-04453-z},
abstract = {The Internet of Medical Things (IoMT) is a bionetwork of allied medical devices, sensors, wearable biosensor devices, etc. It is gradually reforming the healthcare industry by leveraging its capabilities to improve personalized healthcare services by enabling seamless communication of medical data. IoMT facilitates prompt emergency responses and provides improved quality of medical services with minimum cost. With the advancement of modern technology, progressively ubiquitous medical devices raise critical security and data privacy concerns through resource constraints and open connectivity. Vulnerabilities in IoMT devices allow unauthorized access for potential entry into healthcare and sensitive personal data. In addition, the patient may experience severe physical damage with the attack on IoMT devices. To provide security to IoMT devices and privacy to patient data, we have proposed a novel IoMT framework with the hybridization of Bayesian optimization and extreme learning machine (ELM). The proposed model derives encouraging performance with enhanced accuracy in decision-making process compared to similar state-of-the-art methods.},
journal = {J. Supercomput.},
month = {sep},
pages = {14866–14891},
numpages = {26},
keywords = {IoT security, Extreme learning machine, Bayesian optimization, IoMT}
}

@article{10.1109/MPRV.2017.2940953,
author = {Lopes, Pedro and Baudisch, Patrick},
title = {Immense Power in a Tiny Package: Wearables Based on Electrical Muscle Stimulation},
year = {2017},
issue_date = {2017},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {16},
number = {3},
issn = {1536-1268},
url = {https://doi.org/10.1109/MPRV.2017.2940953},
doi = {10.1109/MPRV.2017.2940953},
abstract = {Electrical muscle stimulation (EMS) devices have been used in rehabilitation medicine since the 1960s to regenerate lost motor functions, but, more recently, researchers have started to experiment with EMS to create interactive systems. EMS miniaturizes well, easily lending itself to pervasive computing use cases--particularly those involving mobile and wearable devices. As the authors discuss, EMS provides researchers with the technical means to create devices even smaller than current wearable devices. The authors illustrate this by comparing some of their own prototypes based on EMS with traditional approaches involving mechanical actuators.},
journal = {IEEE Pervasive Computing},
month = {jan},
pages = {12–16},
numpages = {5}
}

@article{10.1016/j.eswa.2023.120914,
author = {Tian, Sijie and Zhang, Yaoyu and Feng, Yuchun and Elsagan, Nour and Ko, Yoon and Mozaffari, M. Hamed and Xi, Dexen D.Z. and Lee, Chi-Guhn},
title = {Time Series Classification, Augmentation and Artificial-Intelligence-Enabled Software for Emergency Response in Freight Transportation Fires},
year = {2023},
issue_date = {Dec 2023},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {233},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2023.120914},
doi = {10.1016/j.eswa.2023.120914},
journal = {Expert Syst. Appl.},
month = {dec},
numpages = {22},
keywords = {Self-supervised learning, Decision support systems, Applied machine learning, Time series classification, Time series augmentation}
}

@article{10.1155/2021/6710074,
author = {Alsamhi, Saeed Hamood and Almalki, Faris A. and AL-Dois, Hatem and Shvetsov, Alexey V. and Ansari, Mohammad Samar and Hawbani, Ammar and Gupta, Sachin Kumar and Lee, Brian and Panic, Stefan},
title = {Multi-Drone Edge Intelligence and SAR Smart Wearable Devices for Emergency Communication},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/6710074},
doi = {10.1155/2021/6710074},
abstract = {Disasters, either manmade or natural, call for rapid and timely actions. Due to disaster, all of the communication infrastructures are destroyed, and there is no way for connection between people in disaster and others outside the disaster range. Drone technology is the critical technology for delivering communication services and guiding people and monitoring the unwanted effects of a disaster. The collaboration of advanced technologies can reduce life losses, save people’s lives, and manage the disaster crisis. The network performance of collaboration between the Internet of Things (IoT) and drone edge intelligence can help gather and process data, extend the wireless coverage area, deliver medical emergencies, provide real-time information about the emergency, and gather data from areas that are impossible for humans to reach. In this paper, we focus on the network performance for efficient collaboration of drone edge intelligence and smart wearable devices for disaster management. We focus mainly on network connectivity parameters for improving real-time data sharing between the drone edge intelligence and smart wearable devices. The relevant parameters that are considered in this study include delay, throughput, and the load from drone edge intelligence. It is further shown that network performance can have significant improvement when the abovementioned parameters are correctly optimised, and the improved performance can significantly improve the guiding/coordinating of search and rescue (SAR) teams effectively and efficiently.},
journal = {Wirel. Commun. Mob. Comput.},
month = {jan},
numpages = {12}
}

@article{10.1177/1046878105284450,
author = {Dugdale, Julie and Pallamin, Nico and Pavard, Bernard},
title = {An Assessment of a Mixed Reality Environment: Toward an Ethnomethodological Approach},
year = {2006},
issue_date = {June 2006},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {37},
number = {2},
issn = {1046-8781},
url = {https://doi.org/10.1177/1046878105284450},
doi = {10.1177/1046878105284450},
abstract = {Training firefighters is a difficult process in which emotions and nonverbal behaviors play an important role. The authors have developed a mixed reality environment for training a small group of firefighters, which takes into account these aspects. The assessment of the environment was made up of three phases: assessing the virtual agents to gauge their expressiveness, assessing the contextual virtual environment to see if it provides the same decision-making support as that found in the real world, and verifying indexcality to support agent interaction.},
journal = {Simul. Gaming},
month = {jun},
pages = {226–244},
numpages = {19},
keywords = {nonverbal communication, methodology, indexicality, firefighting, empirical study, verification, training tool}
}

@article{10.1007/s10458-015-9286-4,
author = {Ramchurn, Sarvapali D. and Wu, Feng and Jiang, Wenchao and Fischer, Joel E. and Reece, Steve and Roberts, Stephen and Rodden, Tom and Greenhalgh, Chris and Jennings, Nicholas R.},
title = {Human---Agent Collaboration for Disaster Response},
year = {2016},
issue_date = {January   2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {30},
number = {1},
issn = {1387-2532},
url = {https://doi.org/10.1007/s10458-015-9286-4},
doi = {10.1007/s10458-015-9286-4},
abstract = {In the aftermath of major disasters, first responders are typically overwhelmed with large numbers of, spatially distributed, search and rescue tasks, each with their own requirements. Moreover, responders have to operate in highly uncertain and dynamic environments where new tasks may appear and hazards may be spreading across the disaster space. Hence, rescue missions may need to be re-planned as new information comes in, tasks are completed, or new hazards are discovered. Finding an optimal allocation of resources to complete all the tasks is a major computational challenge. In this paper, we use decision theoretic techniques to solve the task allocation problem posed by emergency response planning and then deploy our solution as part of an agent-based planning tool in real-world field trials. By so doing, we are able to study the interactional issues that arise when humans are guided by an agent. Specifically, we develop an algorithm, based on a multi-agent Markov decision process representation of the task allocation problem and show that it outperforms standard baseline solutions. We then integrate the algorithm into a planning agent that responds to requests for tasks from participants in a mixed-reality location-based game, called AtomicOrchid, that simulates disaster response settings in the real-world. We then run a number of trials of our planning agent and compare it against a purely human driven system. Our analysis of these trials show that human commanders adapt to the planning agent by taking on a more supervisory role and that, by providing humans with the flexibility of requesting plans from the agent, allows them to perform more tasks more efficiently than using purely human interactions to allocate tasks. We also discuss how such flexibility could lead to poor performance if left unchecked.},
journal = {Autonomous Agents and Multi-Agent Systems},
month = {jan},
pages = {82–111},
numpages = {30},
keywords = {Human---agent collectives, Disaster response, Human---agent interaction}
}

@article{10.1016/j.robot.2022.104071,
author = {Bailon-Ruiz, Rafael and Bit-Monnot, Arthur and Lacroix, Simon},
title = {Real-Time Wildfire Monitoring with a Fleet of UAVs},
year = {2022},
issue_date = {Jun 2022},
publisher = {North-Holland Publishing Co.},
address = {NLD},
volume = {152},
number = {C},
issn = {0921-8890},
url = {https://doi.org/10.1016/j.robot.2022.104071},
doi = {10.1016/j.robot.2022.104071},
journal = {Robot. Auton. Syst.},
month = {jun},
numpages = {16},
keywords = {Remote sensing, Wildfire monitoring, UAV, Multi-robot planning}
}

@article{10.1016/j.cie.2016.05.023,
author = {Lin, Kuo-Yi and Chien, Chen-Fu and Kerh, Rhoann},
title = {UNISON Framework of Data-Driven Innovation for Extracting User Experience of Product Design of Wearable Devices},
year = {2016},
issue_date = {September 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {99},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2016.05.023},
doi = {10.1016/j.cie.2016.05.023},
abstract = {A data mining and data-driven framework is constructed to extract user preferences effectively.An empirical study was conducted to derive useful rules for product form of wearable devices.Specific rules are employed to support product design based on user experience.The proposed approach was validated and implemented in real settings. For consumer products, the time-to-market pressure and market share competition are intensive due to the shortening product life cycles. Product form design that contributes to the user experience (UX) is critical to distinguish the product from others. However, few studies have been done for exploring the relationship between UX and the design of product form. To fill the gaps, this study aims to propose a UNISON framework for data-driven innovation to capture the user experience and preference among the factors of product form designs to derive useful rules. An empirical study was conducted for the product design of wearable devices of a world leading Electronics Manufacturing Service (EMS) company with experimental designs of the subjects with different backgrounds to extract their UX to derive design rules. The results have shown practical viability of the proposed approach to assist the designers to develop product design strategies based on the consumer characteristics and the product UX to launch the preferred products to the corresponding customers to enhance customer satisfaction.},
journal = {Comput. Ind. Eng.},
month = {sep},
pages = {487–502},
numpages = {16},
keywords = {UNISON framework, Product design, Data mining, User experience, Data-driven innovation, Wearable devices}
}

@article{10.1007/s00779-011-0374-4,
author = {Krum, David M. and Suma, Evan A. and Bolas, Mark},
title = {Augmented Reality Using Personal Projection and Retroreflection},
year = {2012},
issue_date = {January   2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-011-0374-4},
doi = {10.1007/s00779-011-0374-4},
abstract = {The support of realistic and flexible training simulations for military, law enforcement, emergency response, and other domains has been an important motivator for the development of augmented reality technology. An important vision for achieving this goal has been the creation of a versatile "stage" for physical, emotional, and cognitive training that combines virtual characters and environments with real world elements, such as furniture and props. This paper presents REFLCT, a mixed reality projection framework that couples a near-axis personal projector design with tracking and novel retroreflective props and surfaces. REFLCT provides multiple users with personalized, perspective-correct imagery that is uniquely composited for each user directly into and onto a surrounding environment, without any optics positioned in front of the user's eyes or face. These characteristics facilitate team training experiences which allow users to easily interact with their teammates while wearing their standard issue gear. REFLCT can present virtual humans who can make deictic gestures and establish eye contact without the geometric ambiguity of a typical projection display. It can also display perspective-correct scenes that require a realistic approach for detecting and communicating potential threats between multiple users in disparate locations. In addition to training applications, this display system appears to be well matched with other user interface and application domains, such as asymmetric collaborative workspaces and personal information guides.},
journal = {Personal Ubiquitous Comput.},
month = {jan},
pages = {17–26},
numpages = {10},
keywords = {Retroreflective screens, Head-mounted projection, Augmented reality, Training, Pico-projector}
}

@article{10.5555/3190867.3190871,
author = {Demir, Fatih and Ahmad, Salman and Calyam, Prasad and Jiang, Duo and Huang, Rui and Jahnke, Isa},
title = {A Next-Generation Augmented Reality Platform for Mass Casualty Incidents (MCI)},
year = {2017},
issue_date = {August 2017},
publisher = {Usability Professionals' Association},
address = {Bloomingdale, IL},
volume = {12},
number = {4},
abstract = {It is vitally important to coordinate resources, information sharing, and two-way communication between medical incident commanders (ICs) and first medical responders (paramedics) at mass casualty incidents (MCI) sites. Information at the time of disasters also needs to be effectively analyzed and presented through intelligent user interfaces. Such interfaces need to be easy-to-use by ICs to foster critical decisions that can potentially reduce mortality rates. In this paper, we present Panacea's Cloud™---a next-generation multiple casualty management system. This system has been iteratively developed and refined based on user experience research driven methodology that employed a mixed methods approach, including the views of clinical experts. Panacea's Cloud™ is an example of a next generation MCI system that has an intelligent dashboard that integrates Internet-of-Things (IoT) technologies such as wearable devices and augmented reality technology (AR), virtual beacons, and sensor network nodes. It supports coordination between ICs and paramedics. Our research demonstrates how IoT-based web applications, especially AR and the use of smart glasses, can be futuristically designed for purposes of smart healthcare applications that have effective and efficient communication capabilities. General design recommendations for next generation multiple casualty management system development include incorporating Situational Awareness features, a Synchronous Map View system, a Hands-Free Communication service with AR and smart glasses, Digital Notes, and resilient Wi-Fi networks.},
journal = {J. Usability Studies},
month = {aug},
pages = {193–214},
numpages = {22},
keywords = {augmented reality, eye tracking, disaster management systems, satisfaction, IoT-based web application, smart glasses, effectiveness, UX research for design refinement, efficiency, triage, next-generation incident management system}
}

@article{10.1145/3611067,
author = {Zhou, Shuo and Segawa, Norihisa},
title = {Method of Electrical Muscle Stimulation for Training FPS Game Players in the Timing of Shots},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3611067},
doi = {10.1145/3611067},
abstract = {In first-person shooter (FPS) games, players often need to calculate the timing of their shots for moving targets in advance based on the speed and relative position of those targets. For most players, the accurate timing of shots requires extended gaming experience and training. Unlike professional players, hobby players have less time to compete and train, do not receive professional game guidance, and cannot afford expensive gaming devices. According to previous studies, wearable devices with electrical stimulation can effectively control the muscles with rapid reactions. Here, we provide a method for training the timing of gaming shooters using electrical muscle stimulation (EMS). We believe that training with EMS is feasible and effective, allowing players to quickly learn and improve their gaming experience. To test the feasibility and effectiveness of our approach, we first tested the player's reaction time when using the EMS device to ensure that it did not have a negative impact on the player. Participants were then trained in a custom FPS game with three methods: EMS, non-EMS and EMS-only. The results showed a more significant increase in the average hit rate of EMS-trained participants compared to those trained using the other two methods. Thus, our study shows the possibility of using EMS devices as a training medium for a custom FPS game.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {421},
numpages = {19},
keywords = {Game control, EMS, Sensorimotor, Electrical Muscle Stimulation, Training, First-person shooter game, Motion Perception}
}

@article{10.1145/3053332,
author = {Hassan, Mahmoud and Daiber, Florian and Wiehr, Frederik and Kosmalla, Felix and Kr\"{u}ger, Antonio},
title = {FootStriker: An EMS-Based Foot Strike Assistant for Running},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3053332},
doi = {10.1145/3053332},
abstract = {In running, knee-related injuries are very common. The main cause are high impact forces when striking the ground with the heel first. Mid- or forefoot running is generally known to reduce impact loads and to be a more efficient running style. In this paper, we introduce a wearable running assistant, consisting of an electrical muscle stimulation (EMS) device and an insole with force sensing resistors. It detects heel striking and actuates the calf muscles during the flight phase to control the foot angle before landing. We conducted a user study, in which we compared the classical coaching approach using slow motion video analysis as a terminal feedback to our proposed real-time EMS feedback. The results show that EMS actuation significantly outperforms traditional coaching, i.e., a decreased average heel striking rate, when using the system. As an implication, EMS feedback can generally be beneficial for the motor learning of complex, repetitive movements.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {mar},
articleno = {2},
numpages = {18},
keywords = {real-time assistance, wearables, motor learning, motor skills, real-time feedback, sports training, wearable devices, Electrical muscle stimulation, in-situ feedback, online feedback, running}
}

