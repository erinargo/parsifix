@article{SAARINEN200435,
title = {Multientity Rescue System},
journal = {IFAC Proceedings Volumes},
volume = {37},
number = {7},
pages = {35-40},
year = {2004},
note = {1st IFAC Symposium on Telematics Applications in Automation and Robotics (TA 2004), Espoo, Finland, 21-23 June 2004},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)32120-1},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017321201},
author = {Jari Saarinen and Jussi Suomela and Aarne Halme and Jirí Pavlícek},
keywords = {Augmented reality, telematics, human navigation, telepresence, localisation, mapping, SLAM},
abstract = {This paper presents methods for a cooperative rescue operation in partially or totally unknown areas with human and robotic explorers. Cooperation is based on a common presence i.e. a common understanding of the environment for both humans and robots. The common presence is generated by continuous mapping data supplied by robotic and human entities. Mapping data is pre-processed and transmitted to the mission controller/operator, who finally filters the data to a common presence, which is transmitted continuously back to both entities. The studied methods are human navigation without artificial beacons, human and robotic SLAM, cooperative localization and cooperative map/model building for common presence. Methods are developed, tested and integrated in a European Community research project called Pe LoTe.}
}
@article{SOLOVEY2021102508,
title = {CODA: Mobile interface for enabling safer navigation of unmanned aerial vehicles in real-world settings},
journal = {International Journal of Human-Computer Studies},
volume = {145},
pages = {102508},
year = {2021},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2020.102508},
url = {https://www.sciencedirect.com/science/article/pii/S1071581920301105},
author = {Erin T. Solovey and Kimberly J. Ryan and M.L. Cummings},
keywords = {Unmanned aerial vehicle, Micro aerial vehicle, Mobile, Collision avoidance, Drone},
abstract = {Recent legislation opening commercial operations to small drones, also known as Micro Aerial Vehicles (MAVs) under 55 lbs, is leading to increased use across many facets of society including first responders, news agencies, commercial entities, and hobbyists. However, the widespread use of such systems will be limited unless a few key hurdles are overcome, namely improved usability and support for safe operation in dynamic environments. To this end, we present the Collision and Obstacle Detection and Alerting (CODA) display, a novel interface that enables safe piloting of MAVs with a mobile device in obstacle-dense real-world settings. We describe the system design, architecture, and development. In addition, we present observations from a proof-of-concept technical demonstration and an empirical study with minimally-trained users. Results showed that CODA reduced collisions and improved operator awareness of obstacle proximity, confidence going around corners and perceived performance in avoiding walls and obstacles.}
}
@article{CASCAVILLA2023107260,
title = {Counter-terrorism in cyber–physical spaces: Best practices and technologies from the state of the art},
journal = {Information and Software Technology},
volume = {161},
pages = {107260},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107260},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923001143},
author = {Giuseppe Cascavilla and Damian A. Tamburri and Francesco Leotta and Massimo Mecella and WillemJan {Van Den Heuvel}},
keywords = {Internet of Things, Cyber physical spaces, Public spaces, Cyber threat intelligence, Topic modeling, Topological data analysis, Protection public spaces, Smart city},
abstract = {Context:
The demand for protection and security of physical spaces and urban areas increased with the escalation of terroristic attacks in recent years. We envision with the proposed cyber–physical systems and spaces, a city that would indeed become a smarter urbanistic object, proactively providing alerts and being protective against any threat.
Objectives:
This survey intend to provide a systematic multivocal literature survey comprised of an updated, comprehensive and timely overview of state of the art in counter-terrorism cyber–physical systems, hence aimed at the protection of cyber–physical spaces. Hence, provide guidelines to law enforcement agencies and practitioners providing a description of technologies and best practices for the protection of public spaces.
Methods:
We analyzed 112 papers collected from different online sources, both from the academic field and from websites and blogs ranging from 2004 till mid-2022.
Results:
(a) There is no one single bullet-proof solution available for the protection of public spaces. (b) From our analysis we found three major active fields for the protection of public spaces: Information Technologies, Architectural approaches, Organizational field. (c) While the academic suggest best practices and methodologies for the protection of urban areas, the market did not provide any type of implementation of such suggested approaches, which shows a lack of fertilization between academia and industry.
Conclusion:
The overall analysis has led us to state that there is no one single solution available, conversely, multiple methods and techniques can be put in place to guarantee safety and security in public spaces. The techniques range from architectural design to rethink the design of public spaces keeping security into account in continuity, to emerging technologies such as AI and predictive surveillance.}
}
@article{CAMPANILE2020102120,
title = {Performance evaluation of a fog WSN infrastructure for emergency management},
journal = {Simulation Modelling Practice and Theory},
volume = {104},
pages = {102120},
year = {2020},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2020.102120},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X20300599},
author = {Lelio Campanile and Marco Gribaudo and Mauro Iacono and Michele Mastroianni},
keywords = {Emergency management, Performance evaluation, Fog computing, Edge computing, Wireless sensor networks, Internet of things, ns-3, Simulation},
abstract = {Advances in technology and the rise of new computing paradigms, such as Fog computing, may boost the definition of a new generation of advanced support services in critical applications. In this paper we explore the possibilities of a Wireless Sensor Network support (WSN) for a Fog computing system in an emergency management architecture that has been previously presented. Disposable intelligent wireless sensors, capable of processing tasks locally, are deployed and used to support and protect the intervention of a squad of firemen equipped with augmented reality and life monitoring devices to provide an environmental monitoring system and communication infrastructure,in the framework of a next-generation, cloud-supported emergency management system. Simulation is used to explore the design parameter space and dimension the workloads and the extension of the WSN, according to an adaptive behavior of the resulting Fog computing system that varies workloads to save the integrity of the WSN.}
}
@article{FANFAROVA2017160,
title = {Simulation tool for fire and rescue services},
journal = {Procedia Engineering},
volume = {192},
pages = {160-165},
year = {2017},
note = {12th international scientific conference of young scientists on sustainable, modern and safe transport},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.06.028},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817325742},
author = {Adelaida Fanfarová and Ladislav Mariš},
keywords = {simulation tool, fire and rescue services, serious games, software, model},
abstract = {The authors of the contribution present the possibility of using modern simulation tools based on computer software - applications for the needs of emergency responders, especially for fire fighters. They point out different features of simulation technologies and recommend their implementation in the process of lifelong preparation, training and education of the members of Fire and Rescue Services as the new trend for the comprehensive improvement of preparedness and safety of fire fighters and rescuers. The contribution also presents basic research supported by institutional grant project of the University of Žilina. In agreement with research results, the authors propose a new simulation model design. This model can be used for designing and programming serious games and software for education of fire fighters. It is the first time the simulation model has been designed with active cooperation and support of the Fire and Rescue Services in Slovakia.}
}
@article{CONGES2023104002,
title = {Situational awareness and decision-making in a crisis situation: A crisis management cell in virtual reality},
journal = {International Journal of Disaster Risk Reduction},
volume = {97},
pages = {104002},
year = {2023},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2023.104002},
url = {https://www.sciencedirect.com/science/article/pii/S221242092300482X},
author = {Aurélie Conges and Loïc Breard and William Patruno and Anouar Ouro-Sao and Nicolas Salatge and Audrey Fertier and Matthieu Lauras and Jacob Graham and Frédérick Benaben},
keywords = {Crisis management, Situational awareness, Decision-support, Virtual reality},
abstract = {In a constantly evolving and hyper-connected world, the crises we are facing are changing, exceeding borders, and impacting every sector of activity. More data can be gathered from the crisis site, arriving in a large flow of various types from multiple sources such as sensors, open data, or social media. This huge amount of data coming continuously from the crisis site leads to an ever-changing situational awareness that must be built and understood as quickly as it is evolving to ensure a relevant and effective crisis response. To build a shared situational awareness, decision-makers can gather in a room where they can share and compare the information they receive from the crisis site to build a common operational picture that they can use to make decisions and implement them: this is a crisis management cell. However, the tools in traditional crisis management cells may not be able to keep up with the new requirements induced by these changes: we need modular and dynamic tools able to adapt to the crisis and the needs of the crisis managers while ensuring collaboration and information sharing not only between the stakeholders but also with remote experts that might be needed to understand the situation. We need to start conceiving the future crisis management cell that would meet the new exigencies of crisis management. This paper proposes our version of that future crisis management cell, using virtual reality to provide a dynamic and modular crisis management cell linked to artificial intelligence and decision-support systems.}
}
@article{ARANDAGARCIA2023,
title = {New communication tool for basic life support training: smart glasses. A quasi-experimental study},
journal = {Medicina Intensiva (English Edition)},
year = {2023},
issn = {2173-5727},
doi = {https://doi.org/10.1016/j.medine.2023.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S2173572723001790},
author = {Silvia Aranda-García and Martín Otero-Agra and Carlos Berlanga-Macías and Antonio Rodríguez-Núñez and Roberto Barcala-Furelos and Júlia Domingo and Adriana Seijas-Vijande and Felipe Fernández-Méndez},
keywords = {Smart glasses, Out-of-hospital cardiac arrest, Bystander, Cardiopulmonary resuscitation, Basic life support, Smart glasses, Parada cardiorrespiratoria extrahospitalaria, Testigo, Reanimación cardiopulmonary, Soporte vital básico},
abstract = {Aim
To analyze the effectiveness of a teaching-learning methodology for teletraining in basic life support (BLS) based on communication through smart glasses.
Design
Pilot quasi-experimental non-inferiority study.
Participants
Sixty college students.
Interventions
Randomization of the participants in: tele-training through smart glasses (SG) and traditional training (C) groups. Both training sessions were very brief (less than 8 min) and included the same BLS content. In SG, the instructor trained through a video call with smart glasses.
Main variables of interest
The BLS protocol, the use of AED, the quality of resuscitation and the response times were evaluated.
Results
In most of the BLS protocol variables, the resuscitation quality and performance times, there were no statistically significant differences between groups. There were significant differences (in favor of the SG) in the assessment of breathing (SG: 100%, C: 81%; p = 0.013), the not-to-touch warning before applying the shock (SG: 79%, C: 52%; p = 0.025) and compressions with correct recoil (SG: 85%, C: 32%; p = 0.008).
Conclusions
Laypeople BLS-AED brief tele-training through smart glasses could potentially be, at least, as effective as traditional training methods. In addition, smart glasses could be more advantageous than traditional teaching for certain points of the BLS protocol and chest compressions quality, probably due to the capability of real-time visualization of images which supports the BLS sequence. Augmented reality supported teaching should be considered for BLS training, although caution is required in extrapolating findings, and further in-depth studies are needed to confirm its potential role depending on concrete target populations and environments.
Resumen
Objetivo
Analizar la efectividad de una metodología de enseñanza-aprendizaje de teleformación en soporte vital básico (SVB) basada en la comunicación a través de smart glasses.
Diseño
Estudio piloto cuasi experimental de no inferioridad.
Participantes
Sesenta estudiantes universitarios.
Intervenciones
Aleatorización de los participantes en: grupo de teleformación a través de smart glasses (SG) y formación tradicional (C). Ambas sesiones de entrenamiento fueron muy breves (<8 minutos) e incluyeron el mismo contenido en de SVB. En SG, el entrenamiento fue comunicándose a través de una videollamada con smart glasses.
Variables de interés principales
Se evaluó el protocolo del SVB, el uso de DEA, la calidad de la reanimación y los tiempos de actuación.
Resultados
En la mayoría de las variables del protocolo del SVB, la calidad de la reanimación y los tiempos de ejecución, no hubo diferencias estadísticamente significativas entre grupos. Hubo mejor actuación de SG al valorar la respiración (SG: 100%, C: 81%; p = 0,013), el avisar antes de la descarga del DEA (SG: 79%, C: 52%; p = 0,025) y las compresiones con buena reexpansión (SG: 85%, C: 32%; p = 0,008).
Conclusiones
El tele-entrenamiento en SVB-DEA para legos con smart glasses podría llegar a ser, al menos, tan efectivo como un método tradicional de enseñanza. Además, las smart glasses podrían ser más ventajosas para ciertos aspectos del protocolo del SVB y la calidad de las compresiones, probablemente debido a la capacidad de visualización de imágenes en tiempo real. La enseñanza basada en la realidad aumentada debe considerarse para la capacitación en SVB, aunque se requiere tanto cautela en la extrapolación de hallazgos como estudios futuros con mayor profundidad.}
}
@article{BOGUSLAWSKI2022103066,
title = {3D building interior modelling for navigation in emergency response applications},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {114},
pages = {103066},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2022.103066},
url = {https://www.sciencedirect.com/science/article/pii/S1569843222002540},
author = {Pawel Boguslawski and Sisi Zlatanova and Dariusz Gotlib and Michał Wyszomirski and Miłosz Gnat and Piotr Grzempowski},
keywords = {3D modelling, Indoor model, Indoor navigation, Emergency response, Spatial data representation},
abstract = {The aim of this paper is to present a broad perspective on applications dealing with 3D building models and indoor navigation during an emergency. This is an immense topic, which has been considerably investigated by researchers from different fields of expertise: construction, cognition, spatial information science, cartography, robotics, mathematics, engineering, etc. It is not possible to provide one, complete approach to the problem, because it largely depends on the application. However, some solutions are prevailing, and they are adopted by others. In this paper there is an attempt made to signalise specific directions and trends related to the topic. They include building model requirements, navigation network generation and representation, and 3D visualisation of models and navigation paths. These issues are important in the process of rapid model generation, which is suitable for decision support and situational awareness systems for first responders. The authors highlight issues that may be investigated by young researchers or may be considered as alternatives by more experienced ones. The paper is therefore the aim of this paper is to summarise the contemporary research conducted in various research teams, as well as to present author’s own observations and experiences.}
}
@article{PODSHIVALOV2022100440,
title = {Improving implementation of the Blockchain technology in real estate registration},
journal = {The Journal of High Technology Management Research},
volume = {33},
number = {2},
pages = {100440},
year = {2022},
issn = {1047-8310},
doi = {https://doi.org/10.1016/j.hitech.2022.100440},
url = {https://www.sciencedirect.com/science/article/pii/S1047831022000153},
author = {Tikhon P. Podshivalov},
keywords = {Dispute, Enterprise, Property rights, Online arbitration, Cadastral registration},
abstract = {Difficulties are caused by the introduction of the Blockchain in the field of registration of rights to real estate and real estate turnover since this requires the establishment of a balance between public and private interests. The article proposes a set of measures, the implementation of which would improve the implementation of the Blockchain technology to modernize the registration of rights to real estate and real estate turnover. The main hypothesis of the study is the assumption that the wider use of the Blockchain will increase the reliability of the register of real estate rights and make the circulation of real estate more secure. During the research were used the comparative legal research method, the method of legal modeling, the method of economic analysis of law. The article identified and characterized the vectors of modernization of real estate registration using the Blockchain have been identified and characterized. The regional aspect of introducing Blockchain technology for registering rights to real estate and improving legal regulation has been considered. The use of virtual reality and augmented reality technology for real estate accounting has been proposed, through the creation of a virtual real estate registry. The possibility of using online arbitration when challenging the right to real estate, which is registered using Blockchain technology, has been argued. The main directions of legal regulation of Smart factory and Digital Enterprise have been proposed.}
}
@article{DOOLY201588,
title = {UUV's in Maritime Spill Response Exercise Cathach},
journal = {IFAC-PapersOnLine},
volume = {48},
number = {2},
pages = {88-93},
year = {2015},
note = {4th IFAC Workshop onNavigation, Guidance and Controlof Underwater VehiclesNGCUV 2015},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2015.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S2405896315002542},
author = {Gerard Dooly and Edin Omerdic and Joseph Coleman and Jośe Braga and Filipe Ferreira and James Hayes and Hugh Conlon and João Sousa and Daniel Toal},
keywords = {Autonomous Vehicles, Navigation Systems, Motor Control, Communication Systems, Sonar Imaging, Incident Response, Harmful Noxious Substances},
abstract = {Exercise Cathach was a large training exercise which utilised and evaluated the use of Unmanned Underwater Vehicles (UUV's) and sensors in a maritime spill Incident involving oil and harmful and noxious substances (HNS). The exercise was a first in terms of the level of robotic systems deployed to assist in survey, surveillance and inspection roles, assessing the scene in real-time before committing first responders for respond and recover operations. The evaluation showed the effectiveness of these technologies to operate in harsh conditions and the efficiency and usability of the real-time data from operations in the field.}
}
@incollection{20241006,
title = {Index},
editor = {Gregory Ciottone},
booktitle = {Ciottone's Disaster Medicine (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {New Delhi},
pages = {1006-1044},
year = {2024},
isbn = {978-0-323-80932-0},
doi = {https://doi.org/10.1016/B978-0-323-80932-0.00290-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323809320002901}
}
@article{HUDA2022103341,
title = {Survey on computation offloading in UAV-Enabled mobile edge computing},
journal = {Journal of Network and Computer Applications},
volume = {201},
pages = {103341},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103341},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522000108},
author = {S.M. Asiful Huda and Sangman Moh},
abstract = {With the increasing growth of internet-of-things (IoT) devices, effective computation performance has become a critical issue. Many services provided by IoT devices (e.g., augmented reality, location-tracking, traffic systems, and autonomous driving) require intensive real-time data processing, which demands powerful computational resources. Mobile edge computing (MEC) has been introduced to effectively handle this problem reliably over the internet. The inclusion of a MEC server allows computationally intensive tasks to be offloaded from IoT devices. However, communication overhead and delays are major drawbacks. With the advantages of high mobility and low cost, unmanned aerial vehicles (UAVs) can mitigate this issue by acting as MEC servers. The offloading decisions for such scenarios involve service latency, energy/power consumption, and execution delays. For this reason, this study reviews UAV-enabled MEC solutions in which offloading was the focus of research. We compare the algorithms qualitatively to assess features and performance. Finally, we discuss open issues and research challenges in terms of design and implementation.}
}
@article{ARANDAGARCIA2023100391,
title = {Augmented reality training in basic life support with the help of smart glasses. A pilot study},
journal = {Resuscitation Plus},
volume = {14},
pages = {100391},
year = {2023},
issn = {2666-5204},
doi = {https://doi.org/10.1016/j.resplu.2023.100391},
url = {https://www.sciencedirect.com/science/article/pii/S2666520423000346},
author = {Silvia Aranda-García and Martín Otero-Agra and Felipe Fernández-Méndez and Ernesto Herrera-Pedroviejo and Marc Darné and Roberto Barcala-Furelos and Antonio Rodríguez-Núñez},
keywords = {Learning, Laypeople, Smart Glasses, Cardiac arrest, Resuscitation, Remote area, Telemedicine},
abstract = {Introduction
Laypeople should be trained in basic life support and traditional and innovative methodologies may help to obtain this goal. However, there is a knowledge gap about the ideal basic life support training methods. Smart glasses could have a role facilitating laypeople learning of basic life support.
Aim
To analyze the potential impact on basic life support learning of a very brief training supported by smart glasses video communication.
Methods
Twelve laypeople were basic life support tele-trained by means of smart glasses by an instructor in this pilot study. During training (assisted trough smart glasses) and after the training (unassisted) participants’ performance and quality of basic life support and automated external defibrillation procedure were assessed on a standardized simulated scenario.
Results
After the training all participants were able to deliver good quality basic life support, with results comparable to those obtained when real time remotely guided by the instructor through the smart glasses. Mean chest compression rate was significantly higher when not guided (113 /min vs. 103 /min, p = 0.001). When not assisted, the participants spent less time delivering the sequential basic life support steps than when assisted while training.
Conclusions
A very brief remote training supported by instructor and smart glasses seems to be an effective educational method that could facilitate basic life support learning by laypeople. This technology could be considered in cases where instructors are not locally available or in general in remote areas, providing basic internet connection is available. Smart glasses could also be useful for laypeople rolling-refreshers.}
}
@article{TSAKANIKAS2018736,
title = {Video surveillance systems-current status and future trends},
journal = {Computers & Electrical Engineering},
volume = {70},
pages = {736-753},
year = {2018},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2017.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0045790617311813},
author = {Vassilios Tsakanikas and Tasos Dagiuklas},
keywords = {Surveillance, Computer vision, Image analysis, Analytics, Image features, Surveillance analytics, Cloud Computing, Fog Computing, Edge Computing},
abstract = {Within this survey an attempt is made to document the present status of video surveillance systems. The main components of a surveillance system are presented and studied thoroughly. Algorithms for image enhancement, object detection, object tracking, object recognition and item re-identification are presented. The most common modalities utilized by surveillance systems are discussed, putting emphasis on video, in terms of available resolutions and new imaging approaches, like High Dynamic Range video. The most important features and analytics are presented, along with the most common approaches for image / video quality enhancement. Distributed computational infrastructures are discussed (Cloud, Fog and Edge Computing), describing the advantages and disadvantages of each approach. The most important deep learning algorithms are presented, along with the smart analytics that they utilize. Augmented reality and the role it can play to a surveillance system is reported, just before discussing the challenges and the future trends of surveillance.}
}
@article{WEI2019276,
title = {Vehicle engine classification using normalized tone-pitch indexing and neural computing on short remote vibration sensing data},
journal = {Expert Systems with Applications},
volume = {115},
pages = {276-286},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.07.073},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418304974},
author = {Jie Wei and Chi-Him Liu and Zhigang Zhu and Lindsay R. Cain and Vincent J. Velten},
keywords = {Remote sensing, Fourier transforms, Band-pass filters, Indexing, Classification algorithms, Machine learning, Neural networks},
abstract = {As a non-invasive and remote sensor, a Laser Doppler Vibrometer (LDV) has found a broad spectrum of applications. It is a remote, non-line-of-sight sensor to detect threats more reliably and provide increased security protection, which is of utmost importance to military and law enforcement applications. However, the use of the LDV in situation surveillance, especially in vehicle classification, lacks systematic investigations as to its phenomenological and statistical properties. In this work, we aim to identify vehicles by their engine types within a very short period of time to yield a practical expert and intelligent system to classify vehicle engines remotely using laser sensors. Based on our preliminary success on the use of tone-pitch indexes (TPI) over these data, a new normalized tone-pitch indexing (nTPI) scheme is developed to capture engine periodic vibrations by various engine types with vibration data over a much shorter period (from 1.25 to 0.2 s), which makes it possible to monitor slowly moving vehicles around 15 miles per hour. We also exploit the learning power of neural computing, including artificial neural network (ANN), Deep Belief nets (DBN), Stacked Auto-Encoder (SAE), and Convolutional Neural Networks (CNN). To apply a CNN, a two-dimensional array is formulated by stacking nTPI data in an overlapping manner, which is termed as 2DonTPI. The classification results using the proposed nTPI and 2DonTPI over a standard LDV dataset are promising: with encoding duration significantly smaller than that required by the original TPI, consistently high performance is attained for all four neural computing methods. The new vibration data representation combined with neural computing approaches gives rise to a powerful expert and intelligent system for vehicle engine classification, which can find a great array of applications for civil, law enforcement, and military agencies for Intelligence, Surveillance and Reconnaissance purposes that are of crucial importance to national and international security.}
}
@article{GINTERS2019167,
title = {Augmented reality use for cycling quality improvement},
journal = {Procedia Computer Science},
volume = {149},
pages = {167-176},
year = {2019},
note = {ICTE in Transportation and Logistics 2018 (ICTE 2018)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.120},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919301279},
author = {Egils Ginters},
keywords = {Augmented reality, Green transport, Bicycle routes simulation},
abstract = {The impact of air pollution on the health of the population and the greenhouse effect determine the development of green transport modes. One is cycling. The developed a dual-use cycling route planning and management tool VeloRouter provides agent-based routing load simulation and allows each user to choose the most attractive travel time. However, during the trip, the cyclists want to get real-time information not only about the weather, but also about the terrain that would improve travel safety. One of the possible solutions is augmented reality (AR) smart glasses. It is important to develop an appropriate and sustainable AR solution, which is not possible without careful prior analysis.}
}
@article{BABALOLA2023106214,
title = {Applications of immersive technologies for occupational safety and health training and education: A systematic review},
journal = {Safety Science},
volume = {166},
pages = {106214},
year = {2023},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2023.106214},
url = {https://www.sciencedirect.com/science/article/pii/S092575352300156X},
author = {Akinloluwa Babalola and Patrick Manu and Clara Cheung and Akilu Yunusa-Kaltungo and Paulo Bartolo},
keywords = {Augmented reality, Mixed reality, Training and education, Immersive technologies, Virtual reality},
abstract = {Immersive technologies (ImTs) have emerged as a viable pathway to address poor occupational safety and health (OSH) performance through training and education of workers. This study aimed to gain a holistic view of the applications of ImTs for OSH training and education. A review of the application of ImTs for OSH training and education is conducted using the preferred reporting items for systematic reviews and meta-analysis (PRISMA) approach and bibliometric analysis. This resulted in the evaluation of 67 relevant journal articles collected from Scopus, Web of Science, and Engineering Village. The review revealed that ImTs have been applied for OSH training and education in various industries including transportation, construction, mining, and healthcare. It was also revealed that the OSH hazards addressed by ImT-based training and education include but are not limited to fire, fall, electrical and chemical hazards in order to prevent or reduce injuries, illnesses and fatalities. In addition, it was revealed that one of the benefits of ImTs for OSH training and education is better retention of concepts when compared to conventional training and education. Challenges associated with the use of ImTs for OSH training and education include insufficient display brightness for users to effectively see virtual objects in a brightly luminated environment. Among the recommendations for future work is research into how to develop effective communication methods between trainers and trainees immersed in a virtual environment for trainers to fully understand the difficulties trainees experience in operating the developed ImT-based platform and provide solutions to such difficulties.}
}
@article{BELLALOUNA2021554,
title = {Digitization of industrial engineering processes using the augmented reality technology: industrial case studies},
journal = {Procedia CIRP},
volume = {100},
pages = {554-559},
year = {2021},
note = {31st CIRP Design Conference 2021 (CIRP Design 2021)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.05.120},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121005941},
author = {Fahmi Bellalouna},
keywords = {Augmented Reality (AR), Product Configuration Management, Production Planning, Assembly Assistance, Industrial Digitization},
abstract = {Augmented Reality Technology is one of the key technologies of digital transformation in industrial and non-industrial areas. Due to the rapid development of display hardware and tracking systems, virtual and augmented reality applications are being developed today that would only have been possible in large research laboratories with huge efforts a few years ago. This paper presents the application of the augmented reality technology (AR) in the industrial field. Based on industrial case studies the implementation of two AR applications and their potential to digitize the product lifecycle processes will be discussed. The use cases were developed within cooperation projects between the University of Applied Sciences Karlsruhe and a fire truck manufacturer. The case studies focus on the following topics: digital product configuration management and production planning and assistance. The objective of the case studies is the investigation of the AR application in the industrial environment and its capability as digital transformation technology along the product lifecycle. The presented case studies deliver experiences and suggestions in terms of praxis-oriented development and deployment of the AR technology in the engineering field, which can be used for future AR research works.}
}
@article{CARLSON2016123,
title = {Augmented Reality Integrated Simulation Education in Health Care},
journal = {Clinical Simulation in Nursing},
volume = {12},
number = {4},
pages = {123-127},
year = {2016},
note = {Special Issue: Gaming},
issn = {1876-1399},
doi = {https://doi.org/10.1016/j.ecns.2015.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S1876139915001012},
author = {Kasey J. Carlson and David J. Gagnon},
keywords = {augmented reality, simulation, health care, ARIS, game theory, situated learning theory, TAACCCT},
abstract = {Background
Augmented Reality Integrated Simulation Education (ARISE) merges the concepts of simulation with augmented reality and game-based situated learning theory. As part of a Department of Labor grant, 150 ARISE scenarios for health care students are being constructed for open-source use.
Method
Four prototypes were trialed with representatives from the Wisconsin Technical College System.
Results
Results showed versatile use and favorable reviews.
Conclusions
Constructive feedback will be used in creating future scenarios. Further ARISE research, including student learning, student experience, and faculty evaluation, is recommended as well as expanding the concept to other disciplines.}
}
@article{CZARNECKI202041,
title = {Roll up the tape? Laser and optical technologies improve paediatric weight estimation},
journal = {Resuscitation},
volume = {157},
pages = {41-48},
year = {2020},
issn = {0300-9572},
doi = {https://doi.org/10.1016/j.resuscitation.2020.09.027},
url = {https://www.sciencedirect.com/science/article/pii/S0300957220304913},
author = {Ryan W. Czarnecki and Lamia G. Harik and Lauren Q. Malthaner and Junxin Shi and Julie C. Leonard},
keywords = {Emergency medical services, Broselow tape, Paediatric weight estimation, Paediatric resuscitation},
abstract = {Background
A robust estimation method is needed to prevent medication dosing and equipment sizing errors and improve time to administration during paediatric resuscitation. An electronic measurement with computer interface may improve accuracy and alleviate cognitive burden. This study evaluates the accuracy of two electronic height measurement methods, a laser and an optical device, and compares them to the Broselow™ Pediatric Emergency Tape (BT) for weight estimation.
Methods
We enrolled children ages 0–14 years from the emergency department of a free-standing, academic children’s hospital. We obtained sex, body habitus, true weight, true height, BT colour, and experimental heights. We converted experimental height measurements into weight estimates using standardised growth charts. We calculated Pearson correlations between experimental and actual measurements and the percentages of weight estimates within 10% and 20% of true weights. We repeated analyses on a restricted cohort of children 0–11 years, the intended BT age range.
Results
We enrolled 198 children. The laser, optical device and BT weight estimates had strong positive correlations with the actual weight measurements with Pearson’s correlation coefficients of 0.946, p < 0.0001, 0.965, p < 0.0001, and 0.825, p < 0.0001 respectively. 47.8% of optical weight estimates fell within 10% of actual weight and 80.6% within 20%, compared to 40.5% and 75.4% of laser estimates and 39.8% and 65.1% of BT estimates.
Conclusion
Electronic-based weight estimates were more accurate than the BT. The accuracy of medication dosing and equipment sizing during paediatric resuscitation may be improved by integrating optical height-based weight estimates with electronic clinical decision support.}
}
@article{BENSENY2023102595,
title = {Urban wireless traffic evolution: The role of new devices and the effect of policy},
journal = {Telecommunications Policy},
volume = {47},
number = {7},
pages = {102595},
year = {2023},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2023.102595},
url = {https://www.sciencedirect.com/science/article/pii/S0308596123001064},
author = {Jaume Benseny and Jarno Lahteenmaki and Juuso Toyli and Heikki Hammainen},
keywords = {Wireless traffic, Traffic evolution, Smart city, Internet of things, Telecom policy},
abstract = {The emergence of new wireless technologies, such as the Internet of Things, allows digitalizing new and diverse urban activities. Thus, wireless traffic grows in volume and complexity, making prediction, investment planning, and regulation increasingly difficult. This article characterizes urban wireless traffic evolution, supporting operators to drive mobile network evolution and policymakers to increase national and local competitiveness. We propose a holistic method that widens previous research scope, including new devices and the effect of policy from multiple government levels. We provide an analytical formulation that combines existing complementary methods on traffic evolution research and diverse data sources. Results for a centric area of Helsinki during 2020–2030 indicate that daily volumes increase, albeit a surprisingly large part of the traffic continues to be generated by smartphones. Machine traffic gains importance, driven by surveillance video cameras and connected cars. While camera traffic is sensitive to law enforcement policies and data regulation, car traffic is less affected by transport electrification policy. High-priority traffic remains small, even under encouraging autonomous vehicle policies. Based on peak hour results, we suggest that 5G small cells might be needed around 2025, albeit the utilization of novel radio technology and additional mid-band spectrum could delay this need until 2029. We argue that mobile network operators inevitably need to cooperate in constructing a single, shared small cell network to mitigate the high deployment costs of massively deploying small cells. We also provide guidance to local and national policymakers for IoT-enabled competitive gains via the mitigation of five bottlenecks. For example, local monopolies for mmWave connectivity should be facilitated on space-limited urban furniture or risk an eventual capacity crunch, slowing down digitalization.}
}
@article{LU2024107401,
title = {Agent-based modeling of high-rise building fires reveals self-rescue behaviors and better fire protection designs},
journal = {Engineering Applications of Artificial Intelligence},
volume = {127},
pages = {107401},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.107401},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623015853},
author = {Peng Lu and Zhuo Zhang and Chiamaka Henrietta Onyebuchi and Lifan Zheng},
keywords = {Crowd dynamics, High-rise building fires, Social force model, Self-rescue behaviors},
abstract = {It is always challenging to seek external rescue assistance in high-rise building fires. Therefore, it is critical for individuals to master survival skills. For crowd dynamics modeling, previous research focused on numerical simulations and building designs with little attention to the self-rescue mechanism. It is critical to understanding crowd evacuations and better response strategies. We modeled the Grenfell Tower (a high-rise building with a complicated structure) case in 2017. Based on the percolation and social force models, we build an agent-based model to simulate individual behaviors inside. We obtain the optimal solution and robust paralleled outcomes under all counterfactual situations based on precisely matching tangible case outcomes (fire duration, deaths, and injuries). For individuals, mastering self-rescue skills is better at reducing social losses (deaths & injuries). In terms of high-rise buildings design, the central alarm system is also useful to reduce them. Besides, the crowd evacuation guided by the social force model also reduces deaths & injuries. This work provides insight into better high-rise building design and practical response strategies for societies. The central alarm system and fire-proof materials should be used in high-rise buildings. The residents should have routine training in social force-based evacuations and survival (self-rescue) skills to better the evacuation process and outcome under natural disasters and social emergencies.}
}
@article{LI2022117459,
title = {Immersive technology-enabled digital transformation in transportation fields: A literature overview},
journal = {Expert Systems with Applications},
volume = {202},
pages = {117459},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117459},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422007928},
author = {Fan Li and Amy J.C. Trappey and Ching-Hung Lee and Li Li},
keywords = {Immersive technology, Digital transformation, Transportation, Virtual reality (VR), Augmented reality (AR)},
abstract = {Immersive technology is rapidly emerging as a powerful tool for enhancing the digital transformation in transportation to deal with the complexity, high cost, and uncertainty in the dynamic traffic environment. In this study, we investigate the innovations induced by immersive technologies for transportation fields. The literature (153 articles) published over the past five years were collected and analyzed systematically, pertaining to the lifecycle model of the product–service system, i.e., design, development, evaluation, and applications. The review indicated that immersive systems and scenarios have been developed and applied in various transportation areas, such as air traffic management, autonomous vehicles, railways, highways, and vehicle maintenance. Additionally, the review indicated that immersive technology brings significant benefits to the transportation field and induces significant changes in the interactions between humans and the physical world. These changes are expected to induce digital transformation in several aspects, e.g., digital testbeds for theoretical models and algorithms, immersive and safe digital environments for user studies, and digital training. Based on the review, a novel framework and conceptual lifecycle model of developing immersive technology-enabled digital transformation in transportation is proposed, a novel evaluation matrix for measuring the immersive experience level is established. The results provide essential references for practitioners to apply immersive technology and guidance regarding potential future research directions.}
}
@article{ALSABBAG2022101473,
title = {Interactive defect quantification through extended reality},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101473},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101473},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621002238},
author = {Zaid Abbas Al-Sabbag and Chul Min Yeum and Sriram Narasimhan},
keywords = {Visual inspection, Extended reality, Augmented reality, Damage detection},
abstract = {In this study, a new visual inspection method that can interactively detect and quantify structural defects using an Extended Reality (XR) device (headset) is proposed. The XR device, which is at the core of this method, supports an interactive environment using a holographic overlay of graphical information on the spatial environment and physical objects being inspected. By leveraging this capability, a novel XR-supported inspection pipeline, called eXtended Reality-based Inspection and Visualization (XRIV), is developed. Key tasks supported by this method include detecting visual damage from sensory data acquired by the XR device, estimating its size, and visualizing (overlaying) information on the spatial environment. The crucial step of real-time interactive segmentation—detection and pixel-wise damage boundary refinement—is achieved using a feature Back-propagating Refinement Scheme (f-BRS) algorithm. Then, a ray-casting algorithm is applied to back-project the 2D image pixel coordinates of the damage region to their 3D world coordinates for damage area quantification in real-world (physical) units. Finally, the area information is overlaid and anchored to the scene containing damage for visualization and documentation. The performance of XRIV is experimentally demonstrated by measuring surface structural damage of an in-service concrete bridge with less than 10% errors for two different test cases, and image processing latency of 2–3 s (or 0.5 s per seed point) from f-BRS. The proposed XRIV pipeline underscores the advantages of real-time interaction between expert users and the XR device through immersive visualization so that a human–machine collaborative workflow can be established to obtain better inspection outcomes in terms of accuracy and robustness.}
}
@article{DOSSANTOS2024104136,
title = {Identifying firefighters' situation awareness requirements for fire and non-fire emergencies using a goal-directed task analysis},
journal = {Applied Ergonomics},
volume = {114},
pages = {104136},
year = {2024},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2023.104136},
url = {https://www.sciencedirect.com/science/article/pii/S0003687023001746},
author = {Viviana {Dos Santos} and Changwon Son},
keywords = {Emergency, Firefighters, Situation awareness},
abstract = {A firefighter's situation awareness (SA) is considered crucial to making effective tactical decisions and actions at the scene. Despite the importance of the firefighter's SA, there have been limited research efforts to understand what cues and information firefighters use to assess ongoing situations and predict future conditions. In addition to fire events, contemporary firefighters respond to an increasing volume of non-fire incidents. Thus, this study aims to identify firefighters' SA during three fire incidents (single house fire, vehicle fire, and passenger aircraft fire) and three non-fire incidents (medical emergency, hazardous materials, and urban search and rescue). A goal-directed task analysis was conducted via focus group discussions with eight career firefighters. Findings indicate that firefighters build their SA by processing various cues from hazards (e.g., fire, ignition source), humans (e.g., occupants, bystanders, drivers, passengers), spatial elements (e.g., building structure, location of hazards), and surrounding conditions (e.g., traffic, weather). Our findings provide insights into SA measurement, SA-oriented work processes, training for SA, and designing technologies to support firefighters' SA during all-hazard responses.}
}
@article{YIN2020381,
title = {VR and AR in human performance research―An NUS experience},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {5},
pages = {381-393},
year = {2020},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2020.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S2096579620300723},
author = {Jun-Hao Yin and Chin-Boon Chng and Pooi-Mun Wong and Nicholas Ho and Matthew Chua and Chee-Kong Chui},
keywords = {Human computer interaction, Virtual environment, Human performance},
abstract = {With the mindset of constant improvement in efficiency and safety in the workspace and training in Singapore, there is a need to explore varying technologies and their capabilities to fulfil this need. The ability of Virtual Reality (VR) and Augmented Reality (AR) to create an immersive experience of tying the virtual and physical environments coupled with information filtering capabilities brings a possibility of introducing this technology into the training process and workspace. This paper surveys current research trends, findings and limitation of VR and AR in its effect on human performance, specifically in Singapore, and our experience in the National University of Singapore (NUS).}
}
@article{GRASSI2023103386,
title = {Emergency management through information crowdsourcing},
journal = {Information Processing & Management},
volume = {60},
number = {4},
pages = {103386},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2023.103386},
url = {https://www.sciencedirect.com/science/article/pii/S0306457323001231},
author = {Lucrezia Grassi and Mario Ciranni and Pierpaolo Baglietto and Carmine Tommaso Recchiuto and Massimo Maresca and Antonio Sgorbissa},
keywords = {Information crowdsourcing, Participatory sensing, Delay Tolerant Network, Search & rescue},
abstract = {This article proposes a new framework to model a scenario in which First Responders, citizens, smart devices, or robots explore the environment in an emergency situation, i.e., after an earthquake, assessing damages and searching for people needing assistance. While moving, the agents observe events and exchange the information collected with other agents encountered: to this end, they use messaging systems purposely adapted to use point-to-point network connections to allow local data exchange between agents even when global network connections are not available. As is common in Delay Tolerant Networks, exchanged messages are locally stored: when a global network is available, the agents can upload all the information collected by themselves and other agents they encountered to a Control Room or a database in the Cloud. Differently from traditional DTN algorithms such as Epidemic and Spray&Wait, we propose a solution that keeps track of agents that shared information along the path and assess the quality of the information collected by multiple agents through a reputation-based mechanism that is safer than majority voting. A simulator compatible with OpenStreetMap is presented, as well as simulated experiments in two Italian towns to validate the feasibility of the approach.}
}
@article{DALINS201840,
title = {Laying foundations for effective machine learning in law enforcement. Majura – A labelling schema for child exploitation materials},
journal = {Digital Investigation},
volume = {26},
pages = {40-54},
year = {2018},
issn = {1742-2876},
doi = {https://doi.org/10.1016/j.diin.2018.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1742287618301555},
author = {Janis Dalins and Yuriy Tyshetskiy and Campbell Wilson and Mark J. Carman and Douglas Boudry},
keywords = {Neural networks, Digital forensics, Child exploitation, Forensic triage, Annotation schema},
abstract = {The health impacts of repeated exposure to distressing concepts such as child exploitation materials (CEM, aka ‘child pornography’) have become a major concern to law enforcement agencies and associated entities. Existing methods for ‘flagging’ materials largely rely upon prior knowledge, whilst predictive methods are unreliable, particularly when compared with equivalent tools used for detecting ‘lawful’ pornography. In this paper we detail the design and implementation of a deep-learning based CEM classifier, leveraging existing pornography detection methods to overcome infrastructure and corpora limitations in this field. Specifically, we further existing research through direct access to numerous contemporary, real-world, annotated cases taken from Australian Federal Police holdings, demonstrating the dangers of overfitting due to the influence of individual users' proclivities. We quantify the performance of skin tone analysis in CEM cases, showing it to be of limited use. We assess the performance of our classifier and show it to be sufficient for use in forensic triage and ‘early warning’ of CEM, but of limited efficacy for categorising against existing scales for measuring child abuse severity. We identify limitations currently faced by researchers and practitioners in this field, whose restricted access to training material is exacerbated by inconsistent and unsuitable annotation schemas. Whilst adequate for their intended use, we show existing schemas to be unsuitable for training machine learning (ML) models, and introduce a new, flexible, objective, and tested annotation schema specifically designed for cross-jurisdictional collaborative use. This work, combined with a world-first ‘illicit data airlock’ project currently under construction, has the potential to bring a ‘ground truth’ dataset and processing facilities to researchers worldwide without compromising quality, safety, ethics and legality.}
}
@article{MOLINARO2022103637,
title = {From forest to finished products: The contribution of Industry 4.0 technologies to the wood sector},
journal = {Computers in Industry},
volume = {138},
pages = {103637},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103637},
url = {https://www.sciencedirect.com/science/article/pii/S016636152200032X},
author = {Margherita Molinaro and Guido Orzes},
keywords = {Industry 4.0, Digitalization, Wood, Forest, Paper manufacturing},
abstract = {This study offers a Systematic Literature Review of the main applications of Industry 4.0 technologies in the wood sector, from forest management and raw materials production to the manufacturing of finished wood and paper products. The review, based on a rigorous and structured process, includes 106 papers published between January 2011 and December 2020. The analysis and categorization of the selected papers brings to the creation of a summary framework, which identifies (1) the needs of the wood sector that can be addressed with Industry 4.0, (2) the actions to be implemented to satisfy each need and (3) the specific Industry 4.0 technologies to be adopted for the implementation of the identified actions. Overall, the analyses conducted show that Industry 4.0 is mainly applied in previous literature to collect, share and analyze different types of data through network and data processing technologies, thus supporting decision-making processes along the entire wood supply chain. The aforementioned summary framework, which provides a complete overview of the contribution of Industry 4.0 to the wood sector, is used for the development of promising future research opportunities, deriving mainly from the investigation of underexploited Industry 4.0 technologies (i.e., blockchain, augmented reality, autonomous and collaborative robots). The research provides contributions to both academics and practitioners interested in the application of the new technologies to the different wood supply chain processes.}
}
@article{BRUNS200433,
title = {Ubiquitous Computing and New Frontiers of Automation},
journal = {IFAC Proceedings Volumes},
volume = {37},
number = {5},
pages = {33-42},
year = {2004},
note = {7th IFAC Symposium on Cost-Oriented Automation (COA 2004), Gatineau, Québec, Canada, 6-9 June 2004},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)32340-6},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017323406},
author = {Wilhelm Bruns},
keywords = {ubiquitous computing, intelligent environments, invisible computing, pervasive computing},
abstract = {Abstract
Ubiquitous Computing, a vision of invisible computing integrated in our everyday surroundings, first introduced by M. Weiser and his group at Xerox PARC in 1988, is still in its early infancy and far from leaving the laboratory stage. Nevertheless there are foreseeable applications in specific areas like automotive automation, health care, home automation, advanced manufacturing. Some aspects of Ubiquitous Computing from an automatic control perspective and its relation to mixed reality, augmented reality and pervasive computing will be covered. Several problems like geometric representation, extensible computing, scalability, movable interactions, integration of various network-technologies, user interface design for multi-modality, design methodologies and evaluation techniques, security/privacy issues and enabling software concepts are touched and it will be speculated about how Ubiquitous Computing might influence Low Cost Advanced Manufacturing and how experiences from the automation and control field might influence the emerging community.}
}
@article{LI2023105069,
title = {Mobile augmented reality-based visualization framework for lifecycle O&M support of urban underground pipe networks},
journal = {Tunnelling and Underground Space Technology},
volume = {136},
pages = {105069},
year = {2023},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2023.105069},
url = {https://www.sciencedirect.com/science/article/pii/S0886779823000895},
author = {Minghao Li and Xin Feng and Yang Han and Xudu Liu},
keywords = {Urban underground pipe networks, Mobile augmented reality (MAR), Lifecycle O&M support, Multisource data, Distributed fiber optic sensors (DFOS), Hyperrealistic human–machine interaction},
abstract = {The structural deterioration of urban underground pipe networks is the accumulated combined effects of physical, environmental and operational factors during the whole lifecycle. Accurate on-site locating and mapping of pipes and their conditions has been a great challenge for construction, maintenance and emergency response in a dynamic manner. Existing approaches, mostly based on design-centric and static visualization, are unsuitable for implementation within the operation and maintenance (O&M) phase because they are performed in a completely virtual environment and are unable to interact with buried assets on site. An integrated hyperrealistic visualization framework based on mobile augmented reality (MAR) is developed to support various real-life project tasks for O&M support of urban underground pipe networks. The proposed approach consists of three parts: a cloud-based data management platform, a 3D visualization development platform, and a location-based registration method for MAR. A cloud–edge–mobile layer is proposed to manage, analyze, process and visualize multisource data, especially for dynamic data. Field experiments on a real-life urban gas pipe network were carried out to investigate the feasibility of the proposed framework to achieve informed O&M support. The results demonstrate that (1) a smartphone linked to a Lite RTK device allows for both accurate locating and easy operation; (2) both the 3D virtual digital model and the multisource data visually superimpose onto the realistic environment, enabling on-site human–machine interaction in a hyperrealistic manner; and (3) MAR-based immersive visualization contributes to avoiding excavation damage, improving O&M efficiency and assisting emergency responders to make timely and better–informed decisions based on multisource information fusion.}
}
@article{APPLIN2021100010,
title = {Facebook's Project Aria indicates problems for responsible innovation when broadly deploying AR and other pervasive technology in the Commons},
journal = {Journal of Responsible Technology},
volume = {5},
pages = {100010},
year = {2021},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2021.100010},
url = {https://www.sciencedirect.com/science/article/pii/S2666659621000032},
author = {Sally A. Applin and Catherine Flick},
keywords = {Ethics, Responsible innovation, Augmented reality, Pervasive technology, Social media, Surveillance, Anthropology},
abstract = {Nearly every week, a technology company is introducing a new surveillance technology, varying from applying facial recognition to observing and cataloguing behaviours of the public in the Commons and private spaces, to listening and recording what we say, or mapping what we do, where we go, and who we're with—or as much of these facets of our lives as can be accessed. As such, the general public writ-large has had to wrestle with the colonization of publicly funded space, and the outcomes to each of our personal lives as a result of the massive harvesting and storing of our data, and the potential machine learning and processing applied to that data. Facebook, once content to harvest our data through its website, cookies, and apps on mobile phones and computers, has now planned to follow us more deeply into the Commons by developing new mapping technology combined with smart camera equipped Augmented Reality (AR) eyeglasses, that will track, render and record the Commons—and us with it. The resulting data will privately benefit Facebook's continued goal to expand its worldwide reach and growth. In this paper, we examine the ethical implications of Facebook's Project Aria research pilot through the perspectives of Responsible Innovation, comparing both existing understandings of Responsible Research and Innovation and Facebook's own Responsible Innovation Principles; we contextualise Project Aria within the Commons through applying current social multi-dimensional communications theory to understand the extensive socio-technological implications of Project Aria within society and culture; and we address the potentially serious consequences of the Facebook Project Aria experiment, inspiring countless other companies to shift their focus to compete with Project Aria, or beat it to the consumer marketplace.}
}
@article{SABETI2021101429,
title = {Toward AI-enabled augmented reality to enhance the safety of highway work zones: Feasibility, requirements, and challenges},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101429},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101429},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001816},
author = {Sepehr Sabeti and Omidreza Shoghli and Mohammadreza Baharani and Hamed Tabkhi},
keywords = {Real-time Artificial Intelligence, Human-AI Interaction, Multi-modal Augmented Reality, Internet-of-Things (IoT), Highway Work Zone Safety},
abstract = {Highway work zones are considered among the most hazardous working environments. In 2018 alone, 124 workers lost their lives to fatal accidents. The lack of predictive safety systems that notify workers of upcoming dangers in advance is a major reason to blame in the highway maintenance and operation community. This article presents an integrative design framework for bringing recent advances in Augmented Reality (AR) and Artificial Intelligence (AI) to enhance the safety of highway workers through real-time multimodal notifications on-spot. To this end, this article conceptualizes and co-designs three major pillars: (1) AR user interface design for multimodal notification, (2) real-time AI at the edge for vehicle detection/classification from distance, and (3) real-time wireless communication in work zone setting to enable latency-aware operation between AI and AR components. Our early results demonstrate that we can achieve 24.83 FPS end-to-end execution latency on the Xavier AGX Jetson board with 48.7% mAP on BDD100K dataset, and a real-time communication covering 120 meters with an average latency of 5.1 ms at the farthest distance. Our mixed-method user research also reveals an acceptable level of excitement and engagement from the body of highway workers toward both the proposed technology and the designed user interface. Overall, this article provides a proof-of-concept toward AI-enabled AR safety systems in highway work zones.}
}
@incollection{2023291,
title = {Index},
editor = {Rajesh Kumar Dhanaraj and Ali Kashif Bashir and Vani Rajasekar and Balamurugan Balusamy and Pooja Malik},
booktitle = {Digital Twin for Smart Manufacturing},
publisher = {Academic Press},
pages = {291-301},
year = {2023},
isbn = {978-0-323-99205-3},
doi = {https://doi.org/10.1016/B978-0-323-99205-3.00019-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323992053000195}
}
@article{ZHOU2023106100,
title = {Cognition-driven navigation assistive system for emergency indoor wayfinding (CogDNA): Proof of concept and evidence},
journal = {Safety Science},
volume = {162},
pages = {106100},
year = {2023},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2023.106100},
url = {https://www.sciencedirect.com/science/article/pii/S0925753523000425},
author = {Tianyu Zhou and Pengxiang Xia and Qi Zhu and Jing Du},
keywords = {User Interface, Emergency Wayfinding, Cognition-Driven Adaptive System, Human factors},
abstract = {Attributed to the advances in sensing and visualization technologies, novel wayfinding assistive systems are becoming more accessible such as the augmented reality (AR)-based wayfinding guidance that provides immersive information about the space and environments. However, the excessive information provided by these new methods results in cognitive overload, leading to subpar performance. Especially for emergency wayfinding where responders need to process a large amount of information while performing search and hazard identification tasks, an overdesigned wayfinding user interface and excessive information can cause confusion or disorientation among responders. This study proposes and tests a real-time cognition-driven navigation assistive system for emergency wayfinding called CogDNA. CogDNA measures responders’ real-time cognitive load and mental status based on the high-frequent pupillometer and gazes tracking data. Then the elements and formats of the primary wayfinding information are adjusted based on the individualized cognitive load models and the mental fatigue status measured by the pupillometer data including gaze movement entropy and blink rate. To test the efficacy of CogDNA, a human-subject experiment (n = 31) was performed with local firefighters in a simulated emergency wayfinding task. Three conditions were presented including the control condition with a static wayfinding system, a self-adaption condition where responders could adjust information with gesture controls, and the auto-adaption condition that tailored information based on real-time cognitive load and mental status measures. The result shows that the proposed method reduces the workload and improves the performance of the responders during the wayfinding task. The self-reported assessments also indicate benefits of the proposed method in cognitive load. The findings prove the efficacy of adaptive wayfinding information systems based on real-time cognitive load measures for future emergency wayfinding tasks.}
}
@article{CROFTON2019102178,
title = {Potential applications for virtual and augmented reality technologies in sensory science},
journal = {Innovative Food Science & Emerging Technologies},
volume = {56},
pages = {102178},
year = {2019},
issn = {1466-8564},
doi = {https://doi.org/10.1016/j.ifset.2019.102178},
url = {https://www.sciencedirect.com/science/article/pii/S1466856418314449},
author = {E.C. Crofton and C. Botinestean and M. Fenelon and E. Gallagher},
keywords = {Virtual reality, Augmented reality, Emerging technologies, Sensory science},
abstract = {Sensory science has advanced significantly in the past decade and is quickly evolving to become a key tool for predicting food product success in the marketplace. Increasingly, sensory data techniques are moving towards more dynamic aspects of sensory perception, taking account of the various stages of user-product interactions. Recent technological advancements in virtual reality and augmented reality have unlocked the potential for new immersive and interactive systems which could be applied as powerful tools for capturing and deciphering the complexities of human sensory perception. This paper reviews recent advancements in virtual and augmented reality technologies and identifies and explores their potential application within the field of sensory science. The paper also considers the possible benefits for the food industry as well as key challenges posed for widespread adoption. The findings indicate that these technologies have the potential to alter the research landscape in sensory science by facilitating promising innovations in five principal areas: consumption context, biometrics, food structure and texture, sensory marketing and augmenting sensory perception. Although the advent of augmented and virtual reality in sensory science offers new exciting developments, the exploitation of these technologies is in its infancy and future research will understand how they can be fully integrated with food and human responses.
Industrial relevance
The need for sensory evaluation within the food industry is becoming increasingly complex as companies continuously compete for consumer product acceptance in today's highly innovative and global food environment. Recent technological developments in virtual and augmented reality offer the food industry new opportunities for generating more reliable insights into consumer sensory perceptions of food and beverages, contributing to the design and development of new products with optimised consumer benefits. These technologies also hold significant potential for improving the predictive validity of newly launched products within the marketplace.}
}
@article{CHEN2023105629,
title = {The effects of an augmented reality application developed for paediatric first aid training on the knowledge and skill levels of nursing students: An experimental controlled study},
journal = {Nurse Education Today},
volume = {120},
pages = {105629},
year = {2023},
issn = {0260-6917},
doi = {https://doi.org/10.1016/j.nedt.2022.105629},
url = {https://www.sciencedirect.com/science/article/pii/S0260691722003653},
author = {Pao-Ju Chen and Wei-Kai Liou},
keywords = {Augmented reality, Paediatric, First aid, Nursing students, Nursing education},
abstract = {Background
The purpose of the present research was to develop an augmented reality paediatric first aid training system to investigate the effects of such a system on the knowledge, skill levels, and self-confidence of nursing students in first-aid practices.
Objective
This controlled experiment was conducted at a junior college. Ninety-five fourth-year nursing students participated in this study; 46 in the experimental group and 49 in the control group.
Design and methods
Data were collected using the Paediatric First Aid Knowledge Scale, Visual Analogue Scale of Self-Confidence and First Aid Practice Evaluation Checklist and pre- and post-test analyses were performed. The experimental group used the augmented reality application for paediatric first aid training, while the control group used a traditional method.
Results
There was no statistically significant difference between the students' pre-test scores in both groups, indicating that the students had similar levels of knowledge of paediatric first aid before the intervention. The post-test of the paediatric first aid knowledge results showed that students in the experimental group significantly outperformed those in the control group (p < .05). A significant difference was found in the first aid skills between the groups (p < .001). Additionally, the students in the experimental group stated that the augmented reality application significantly improved their self-confidence in performing paediatric emergency care (p < .001).
Conclusions
The augmented reality application had a positive effect on the knowledge and skill levels of nursing students regarding first aid practices. This study suggests that augmented reality is a valuable teaching tool in many areas of nursing education.}
}
@article{RICHARD2021101425,
title = {INTERVALES: INTERactive Virtual and Augmented framework for industriaL Environment and Scenarios},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101425},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101425},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001774},
author = {Killian Richard and Vincent Havard and Jordan His and David Baudry},
keywords = {Virtual reality, Augmented reality, Authoring, UML modeling, Virtual Reality Training System (VRTS), Industry 4.0},
abstract = {One of Industry 4.0’s greatest challenges for companies is the digitization of their processes and the integration of new related technologies such as virtual reality (VR) and augmented reality (AR), which can be used for training purposes, design, or assistance during industrial operations. Moreover, recent results and industrial proofs of concept show that these technologies demonstrate critical advantages in the industry. Nevertheless, the authoring and editing process of virtual and augmented content remains time-consuming, especially in complex industrial scenarios. While the use of interactive virtual environments through virtual and augmented reality presents new possibilities for many domains, a wider adoption of VR/AR is possible only if the authoring process is simplified, allowing for more rapid development and configuration without the need for advanced IT skills. To meet this goal, this study presents a new framework: INTERVALES. First, framework architecture is proposed, along with its different modules; this study then shows that the framework can be updated by not only IT workers, but also other job experts. The UML data model is presented to format and simplify the authoring processes for both VR and AR. This model takes into account virtual and augmented environments, the possible interactions, and ease operations orchestration. Finally, this paper presents the implementation of an industrial use case composed of collaborative robotic (cobotic) and manual assembly workstations in VR and AR based on INTERVALES data.}
}
@article{WARD2022103720,
title = {More or less? Improving monocular head mounted display assisted visual search by reducing guidance precision},
journal = {Applied Ergonomics},
volume = {102},
pages = {103720},
year = {2022},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2022.103720},
url = {https://www.sciencedirect.com/science/article/pii/S0003687022000436},
author = {Matthew Ward and William S. Helton},
keywords = {Attention, Head-mounted display, Mobile devices, Visual search, Wearable devices},
abstract = {Objective
To test six different methods of directing a user's attention in a peripheral head mounted display assisted visual search task.
Background
Each time a user needs to shift their attention between virtual information and their environment has a cost. The faster a user can process a guiding cue and the fewer times they need to return to it, the more efficient that cue will be at directing a user's attention. The most effective method, creating a visual effect at the location of the target, is not suitable for peripheral head mounted displays. This study tests alternative guiding cues better suited to these devices.
Method
Participants searched for a singleton target hidden among 299 distractors while directed with one of six device-delivered guiding cues. Search times were recorded.
Results
A static region map was the most efficient and most preferred cue. Static and dynamic directional cues were also effective in comparison to non-guided search. Cues designed to work solely within the participants’ peripheral vision were relatively ineffective
Conclusion
Guidance cues that direct a user's attention to targets within the real environment do not need to precisely lead to the target. It is instead more efficient to lead a user to the general vicinity of the target quickly and then have the user revert to their natural visual search behaviour.
Application
This finding is broadly useful when assisting visual search tasks with handheld or worn devices which do not cover the user's full field of view.
Précis
This study tested six different methods of guiding attention in a peripheral head-mounted display assisted visual search task. This study compared static, dynamic and peripheral-vision endogenous cues to targets and found a static simple map cue both fastest and most preferred by users.}
}
@article{CAVALIERIDORO2019100054,
title = {Modeling and evaluating a complex edge computing based systems: An emergency management support system case study},
journal = {Internet of Things},
volume = {6},
pages = {100054},
year = {2019},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2019.100054},
url = {https://www.sciencedirect.com/science/article/pii/S2542660519300319},
author = {Edoardo {Cavalieri d’Oro} and Simone Colombo and Marco Gribaudo and Mauro Iacono and Davide Manca and Pietro Piazzolla},
keywords = {Performance evaluation, Edge computing, Crisis management systems, IoT, Cloud computing, Critical systems, CBRN response, Emergency management},
abstract = {The edge computing paradigm enhances the exploitability of cloud computing by providing, in principle, the means to overcome the limitations in terms of responsiveness, bandwidth needs, privacy and availability in critical applications. By moving some of the components of the system towards the physical location where results are timely needed, it is possible to support mission critical applications and back them up with the flexibility and the resource availability and scalability offered by the cloud, while keeping mission costs lower (and response efficiency higher) than classical approaches. The main challenge in merging cloud and edge components lies in their correct balance to allow for the best results at the lowest costs. This means that performance-oriented evaluation models are crucial in the design, deployment and execution phases of the system lifetime. In this paper we present a modeling approach for complex, critical-edge computing-based systems relying on the use of queuing networks, applied to a novel architecture aiming at supporting operations in case of medium or large-scale accidents that involve interoperability among responders during the emergency phase. The proposed architecture allows the coordination of fire brigade teams, equipped with sensors and augmented reality devices, to minimize mission problems and timely exploit local and external information, as well as supporting interoperations with other first responders. The results show that, even with a standard modeling approach, these systems are extremely interesting and show non-trivial behaviors. Security and dependability issues of the proposed architecture are discussed.}
}
@article{OFITSEROV2020507,
title = {Conceptual aspects of improving legislation and law enforcement practice in use of robot systems to ensure traffic safety in large cities},
journal = {Transportation Research Procedia},
volume = {50},
pages = {507-517},
year = {2020},
note = {XIV International Conference on Organization and Traffic Safety Management in Large Cities (OTS-2020)},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2020.10.060},
url = {https://www.sciencedirect.com/science/article/pii/S2352146520308085},
author = {Aleksander Ofitserov and Sergey Blatin},
keywords = {unmanned aerial vehicle, traffic safety, road traffic accident, administrative procedure, legal regulation},
abstract = {The article provides a general description of the legislation for the use of unmanned aerial vehicles (UAVs) in the Russian Federation and some foreign countries. Special attention is paid to the promising directions of UAV use in the process of ensuring traffic safety and regulatory restrictions that prevent the use of UAVs over highways and in the airspace of large cities (megacities). We analyze UAV capabilities in the implementation of the main administrative procedures (actions) to study the circumstances of road traffic accidents (RTAs), as well as gaps in legislation related to the implementation of these procedures. Based on the research, including comparative legal studies, we propose a “road map” for improving legislation aimed at protecting the interests of a person, society, and the state against the negative consequences of RTAs through the use of UAVs.}
}
@article{MITSUHARA20212105,
title = {Expressing Disaster Situations for Evacuation Training Using Markerless Augmented Reality},
journal = {Procedia Computer Science},
volume = {192},
pages = {2105-2114},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.218},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921017142},
author = {Hiroyuki Mitsuhara and Chie Tanimura and Junko Nemoto and Masami Shishibori},
keywords = {Augmented reality, disaster education, evacuation training, smartphone application},
abstract = {Evacuation training is crucial for protecting human lives from natural and man-made disasters, but it should be more realistic to achieve training effects. In this study, we focused on expressing disaster situations using markerless augmented reality to achieve realistic evacuation training. We prototyped a scenario-based evacuation training system that superimposed three-dimensional computer graphics of disaster situations (e.g. fire and debris) onto real-time vision (captured by Android tablets or smartphones) using ARCore and Unity3D. Through preliminary experiments, we found that the prototype system can provide realistic expression and potentially be used for evacuation training, but we have not yet clarified the training results and how the expressions influenced participants’ emotions.}
}
@article{MENZEMER2023103742,
title = {A scoping review and bibliometric analysis of methods for fire evacuation training in buildings},
journal = {Fire Safety Journal},
volume = {136},
pages = {103742},
year = {2023},
issn = {0379-7112},
doi = {https://doi.org/10.1016/j.firesaf.2023.103742},
url = {https://www.sciencedirect.com/science/article/pii/S0379711223000103},
author = {Leo Willem Menzemer and Enrico Ronchi and Mette Marie Vad Karsten and Steve Gwynne and Janne Frederiksen},
keywords = {Augmented reality, Drill, Evacuation, Fire, Learning, Serious game, Training, Virtual reality, VR},
abstract = {This paper presents a scoping review of methods for fire evacuation training in buildings. It adopts the PRISMA methodology (Transparent Reporting of Systematic Reviews and Meta-Analyses) and systematically identifies 73 sources among scientific literature published between 1997 and 2022. It was found that the literature is dominated by research on modern technology (Augmented Reality, Virtual Reality, Serious Games) for evacuation training emphasizing that increased immersion, engagement, and realism positively affect learning effects. These technologies may be able to overcome main drawbacks of traditional evacuation drills bringing down cost and reducing the disruptiveness of evacuation training. However, great inconsistencies in measuring performance of trainees and lack of reasoning in the design of training programmes impede conclusions that go beyond qualitative trends. The field would profit from clear methodologies to assess evacuation performance and training effects and from transdisciplinary approaches aimed at ensuring that training can deliver on its educational goals.}
}
@article{BA202177,
title = {Multi-hazard disaster scenario method and emergency management for urban resilience by integrating experiment–simulation–field data},
journal = {Journal of Safety Science and Resilience},
volume = {2},
number = {2},
pages = {77-89},
year = {2021},
issn = {2666-4496},
doi = {https://doi.org/10.1016/j.jnlssr.2021.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666449621000141},
author = {Rui Ba and Qing Deng and Yi Liu and Rui Yang and Hui Zhang},
keywords = {Multi-hazard, Disaster, Scenario, Resilience, Emergency management},
abstract = {Due to the frequent occurrence of multi-hazard disasters worldwide in recent years, effective multi-hazard scenario analysis is imperative for disaster rescue and emergency management. The response procedure for different single hazards were investigated and formulated before. However, the investigations of disaster scenario rarely systematically address the entire development and response process of multi-hazards, including the coupling mechanisms, evolution dynamics, scenario assessment and emergency response. To this end, this paper presents our methodology of multi-hazard disaster scenario that integrates experiment–simulation–field data, focusing on three dimensions consisting of multi-hazard coupling, structures and systems, and emergency management. The newly proposed scenario method mainly comprises three aspects: experiments and simulations, multi-hazard field investigation, scenario analysis and response. Specifically, in order to study the large-scale, high-intensity and multi-hazard coupling effects, we carried out reduced-scale experiments and field measurement experiments to develop experimental similarity theory and prototype simulations of multi-hazards. In addition, a variety of field rescue and survey equipment, such as robots, Unmanned Aerial Vehicle (UAV), and Virtual Reality/Augmented Reality (VR/AR) technologies were utilized to acquire real-time data of multi-hazard field. Furthermore, we also examine the mechanism and framework of multi-hazard scenarios to formulate the detailed procedures of management and response. They are incorporated with the experiments, simulations, field data and models to construct a new scenario platform. The proposed scenario method was applied in a case study of the coupled wind and snow multi-hazard to verify its effectiveness. The new method contributes to the disaster relief, decision-making and emergency management for multi-hazard disaster to improve the urban resilience.}
}
@article{KULKOV2021101614,
title = {Navigating uncharted waters: Designing business models for virtual and augmented reality companies in the medical industry},
journal = {Journal of Engineering and Technology Management},
volume = {59},
pages = {101614},
year = {2021},
issn = {0923-4748},
doi = {https://doi.org/10.1016/j.jengtecman.2021.101614},
url = {https://www.sciencedirect.com/science/article/pii/S0923474821000035},
author = {Ignat Kulkov and Björn Berggren and Magnus Hellström and Kim Wikström},
keywords = {Business model, Virtual reality, Augmented reality, Design approach, Medical industry},
abstract = {New technologies are at the heart of industry transformation. Virtual and augmented reality companies provide fundamentally new ways of communication, treatment, education, and specialist training within the medical industry. However, business models for new ventures that target the medical industry have received scant attention within academic research. Using a multiple case study approach, we analyze how virtual and augmented reality firms create value for their customers in the medical industry. In all, we have studied eight companies that offer different types of solutions for their target segments. The results of the analysis are four design elements consisting of twelve positions and three design themes that define the similarities and differences between the business models for the companies. We contribute to existing research within the field by analyzing business models of the investigated companies using a design approach, classifying the virtual and augmented reality companies, and analyzing the role of new technology in the development of the medical industry.}
}
@article{RETZINGER2023,
title = {Holographic Hintways: A systems feasibility and usability study of augmented reality cueing for gait adaptation},
journal = {Gait & Posture},
year = {2023},
issn = {0966-6362},
doi = {https://doi.org/10.1016/j.gaitpost.2023.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0966636223014546},
author = {Gwendolyn R. Retzinger and Borna Golbarg and Wendy T. Pham and Isaiah J. Lachica and Thomas Chan and Jacob W. Hinkel-Lipsker},
keywords = {Gait, Augmented reality, Sensory cues, Usability, Variability},
abstract = {Background
Through providing on-demand visual and auditory cues while walking,augmented reality (AR) can theoretically cue spatiotemporal gait adaptations in, populations such as those with Parkinson’s disease. However, given the novelty of the, technology, the type and extent of gait adaptations in response to such a cueing, system are unknown. Before such systems can be incorporated into rehabilitation, approaches, it is important to understand how people interact with the technology. Research questions: What are the effects of visual and auditory cues delivered, through AR on spatiotemporal walking patterns and variability in a healthy, young, population? Is there a relationship between system usability and gait variability?,
Methods
Twenty healthy, young participants walked in four different cueing conditions using an AR headset: No Cues (NC) (i.e., natural gait), Auditory (A), Visual (V), and Auditory + Visual (AV). Inertial measurement units recorded spatiotemporal gait data at 200 Hz, a System Usability Survey was administered afterward, and linear regression models were developed to examine whether gait variability is predictive of system usability.
Results
All cueing conditions exhibited a significantly slower cadence compared to, NC trials. Cadence variability was significantly higher for A trials compared to V and, NC. V trials exhibited significantly decreased stride lengths compared to NC. Increased, reported system usability was significantly correlated with decreased stance phase, time variability across A trials.
Significance
Our findings support that holographic spatial-visual and auditory cues, are promising to evoke spatiotemporal gait adaptations. Results also support the, notion that the type of system and cue delivery design may impact gait outcomes.,Before an AR cueing system can be applied to a specific population in future, interventions, a more holistic approach towards finding the relationship between this, technology and its users is needed.}
}
@article{ZHU20211,
title = {Virtual and augmented reality technologies for emergency management in the built environments: A state-of-the-art review},
journal = {Journal of Safety Science and Resilience},
volume = {2},
number = {1},
pages = {1-10},
year = {2021},
issn = {2666-4496},
doi = {https://doi.org/10.1016/j.jnlssr.2020.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S266644962030030X},
author = {Yiqing Zhu and Nan Li},
keywords = {Virtual reality, Augmented reality, Emergency management, Built environment, Literature review},
abstract = {With the rapid technological advancements in recent decades, virtual reality (VR) and augmented reality (AR) technologies have been increasingly adopted to address various challenges in emergency management in the built environments. This paper presents a review of state-of-the-art applications in this rapidly evolving area. A total of 84 relevant articles are identified based on searching in the Web of Science Core Collection and snowballing. These papers are then organized based on a taxonomy developed in this study. Next, a range of VR/AR applications presented in these papers that are aimed to enhance various processes associated with pre-emergency preparedness, responses during emergency and post-emergency recovery are reviewed in detail. The existing VR/AR applications are also described from a human-computer interaction perspective. Finally, current research trends, knowledge gaps and directions for future research are discussed. The findings presented in this paper are expected to provide a synthetic and critical review of state-of-the-art VR/AR applications for emergency management in the built environment and facilitate further advancements in both research and practice in this area.}
}
@article{SHAHARUDDIN2023100803,
title = {The role of IoT sensor in smart building context for indoor fire hazard scenario: A systematic review of interdisciplinary articles},
journal = {Internet of Things},
volume = {22},
pages = {100803},
year = {2023},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100803},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523001269},
author = {Sarah Shaharuddin and Khairul Nizam {Abdul Maulud} and Syed Ahmad Fadhli {Syed Abdul Rahman} and Adi Irfan {Che Ani} and Biswajeet Pradhan},
keywords = {Internet of things (Iot), Sensor, Indoor fire hazard, Smart buildings, Systematic review, Thematic analysis},
abstract = {In recent years, there has been growing interest in using Internet of Things (IoT) sensors to address indoor fire hazards in smart buildings. This study conducted a systematic review of 54 articles from interdisciplinary databases using selected keywords over the past decade, with the aim of investigating the potential role of IoT sensors in indoor fire hazard contingency. Through thematic analysis, five main themes and 24 sub-themes were identified, including vision-based sensing, smart automation, evacuation and indoor navigation, early fire detection, intervention and prevention, and BIM-related. The results of this review indicate that there are numerous aspects of indoor fire hazards that could benefit from the use of IoT sensors, and that the recurrence of technical terminologies in the analysed articles underscores the importance of these technologies in establishing an IoT sensor network in smart building environments, particularly in addressing indoor fire incidents. The outcome and findings spurred a concept for potential future research ideas. As a result of the findings, a conceptual framework for IoT sensors in the context of smart buildings is proposed.}
}
@article{NURNOBY20231102,
title = {A Real-Time Deep Learning-based Smart Surveillance Using Fog Computing: A Complete Architecture},
journal = {Procedia Computer Science},
volume = {218},
pages = {1102-1111},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.089},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923000893},
author = {M Fasial Nurnoby and Tarek Helmy},
keywords = {Fog-Computing, Deep Learning, Computer Vision, Smart Video Surveillance, Internet of Things},
abstract = {Fog computing offers low-latency and real-time big-data processing capabilities closer to the network edge. This particular benefit addresses the main bottleneck in a centralized cloud framework, which is, it cannot process latency-sensitive large video frames generated from the Internet of Things-based video surveillance cameras in real-time. Besides, the recent advancements in the computer vision field offer many state-of-the-art image processing capabilities that can be utilized for real-time surveillance data processing. Deploying those processing powers at several fog computing layers can bring novel solutions for computer vision-based real-time security solutions. This paper proposes a deep learning-based framework for smart video surveillance that can process the real-time frames on two consecutive fog layers, one for action recognition and the other for criminal threat-based response generation. The proposed architecture consists of three major modules. The first module is responsible for capturing surveillance videos by deploying RaspberryPi cameras in a distributed network. The second module is responsible for action recognition using a deep learning-based model installed inside NVIDIA Jetson Nano-devices placed on two fog layers. Finally, the security response is generated and broadcast to the law-enforcement agency. To evaluate the proposed model, experiments on semantic segmentation-based scene object recognition were run. The experimental results came up with a suitable recognition model that can be deployed in the fog layers of our proposed framework.}
}
@article{ROOPA20213860,
title = {Revolutionizing education system with interactive augmented reality for quality education},
journal = {Materials Today: Proceedings},
volume = {46},
pages = {3860-3863},
year = {2021},
note = {International Conference on Materials, Manufacturing and Mechanical Engineering for Sustainable Developments-2020 (ICMSD 2020)},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.02.294},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321013936},
author = {D. Roopa and R. Prabha and G.A. Senthil},
keywords = {Augmented reality, Vuforia, Blender, 60-degree view, Transformative learning, Experimental learning},
abstract = {Augmented reality would soon impact the modern learning cycle in education. AR has the ability to alter research location and timing, adding new and additional forms and methods. Augmented Reality technology’s capabilities will make classes more interactive and knowledgeable. We all are 3D creatures. Our brain is the most powerful 3D computer technology in the world. All of us have evolved to think and store memory in 3-dimensions i.e when we look at the information in a flat piece of paper or computer screen, our brain takes its own time to translate it back to a 2D visual and store it. This is how our education system is, it started with textbook material and recently educators have started encouraging the use of smart classes which again shows videos. As a result, there is no practical exposure that lags us a step behind. This is where a promising technology of the future called Augmented Reality holds for us. Learners will have a 360-degree view of the real-world entities, interacting with those entities by touching them which will eventually make them have better insight about the concepts. Hence our learning curve decreases increasing the brain's productivity. With the development of smartphones, Augmented Reality is applied to a broader range. Augmented reality can turn an ordinary classroom into an engaging experience. The aim is to implement an interactive Augmented Reality Experience using Vuforia, Unity 3D and Blender. The purpose is to promote situational learning, experimental learning, transformative learning as the learning theory basis of mobile Augmented Reality.}
}
@article{DIAZDIAZ2017198,
title = {Business model analysis of public services operating in the smart city ecosystem: The case of SmartSantander},
journal = {Future Generation Computer Systems},
volume = {76},
pages = {198-214},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.01.032},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17301553},
author = {Raimundo Díaz-Díaz and Luis Muñoz and Daniel Pérez-González},
keywords = {Smart cities, Business model, Canvas, Sustainability, Santander, IoT},
abstract = {As the deployment of Internet of Things and other enabling technologies is still in an initial phase worldwide, few research studies have addressed the associated business models. This paper aims to fill this gap. The main objective of this research is to gain a deeper knowledge about practical business models matching into a real-life smart city ecosystem. Hence, a benchmarking of eight urban services provided in the city of Santander has been carried out: waste management; water supply; traffic management; street lighting; augmented reality and tourism; incidences management, parks and gardens and citizen participation. Among the main results of our study, we highlight that those public services properly managed embedding IoT technology convey cost reductions in the long term. There is also a reduction in energy consumption and environmental impact with the consequent social impact. It should also be highlighted that most data are managed with the same platform. Last but not least, an emerging ecosystem of incentivized citizens has been proved to be arising.}
}
@article{BRAUN2022406,
title = {Virtual Reality for Immersive Multi-User Firefighter Training Scenarios},
journal = {Virtual Reality & Intelligent Hardware},
volume = {4},
number = {5},
pages = {406-417},
year = {2022},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2022.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S209657962200081X},
author = {Philipp Braun and Michaela Grafelmann and Felix Gill and Hauke Stolz and Johannes Hinckeldeyn and Ann-Kathrin Lange},
keywords = {Virtual reality, Multi-user, Motion capture, Training, Firefighting},
abstract = {Virtual Reality (VR) applications are tools used to provide comprehensive training scenarios that are difficult or impossible to represent in physical configurations. This includes team training for emergency services, e.g. in firefighting. To achieve effective virtual training, creating a high level of immersion is essential. In that respect, motion capture systems offer the possibility to create highly immersive multi-user training experiences including full-body avatars. This work presents a preliminary prototype that enables the extinguishing of a fire on a container ship as a VR training scenario. The prototype provides a full-body and multi-user VR experience, based on the synthesis of position data provided by the motion capture system and orientation data from the VR headsets. As a consequence, the prototype allows an initial evaluation of results. The results confirm the value of using VR to train procedures that are difficult to train in the real world. Furthermore, the results show that motion capture based VR technologies are particularly useful for firefighting training, in which participants can collaborate in otherwise difficult-to-access environments. However, the work also illustrates that increasing the immersion of such training remains a challenge.}
}
@article{HINCAPIE2021107289,
title = {Educational applications of augmented reality: A bibliometric study},
journal = {Computers & Electrical Engineering},
volume = {93},
pages = {107289},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107289},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621002706},
author = {Mauricio Hincapie and Christian Diaz and Alejandro Valencia and Manuel Contero and David Güemes-Castorena},
keywords = {Augmented reality, Educational innovation, Educational technology, Human-computer interaction, Simulation, higher education},
abstract = {Augmented Reality (AR) has been used successfully in several industries; one of these is education. A systematic understanding of how AR contributes to education still lacks studies about the content type and its effects on learning outcomes. This article systematically analyzes the AR state-of-the-art in education, determines productivity and publication indicators in this field, and identifies research works that have studied how content type affects the learning outcomes. The methodology was performed through a bibliometric analysis using the Scopus database, focusing on AR's educational uses. Engineering education is the primary research trend, followed by simulation, tracking, and virtual reality. Education and e-learning also have leading roles within this analysis, along with gamification and human-computer interaction, whose impacts are further explored. There is no preferred design methodology for creating AR content. In its absence, most of the works suggest a design based on the developers' and researchers' experience.}
}
@article{ZHANG2021101432,
title = {Enhancing human indoor cognitive map development and wayfinding performance with immersive augmented reality-based navigation systems},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101432},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101432},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001841},
author = {Jinyue Zhang and Xiaolu Xia and Ruiqi Liu and Nan Li},
keywords = {Immersive augmented reality (IAR), Navigation, Wayfinding, Cognitive map, Workload},
abstract = {Augmented reality (AR) is an interactive experience where computer-generated perceptual information is superimposed into the real-world environment. Most existing research in AR-based wayfinding has focused on the technological aspects of developing AR-based software or devices to realize navigation. No previous investigations have focused on understanding the impact of immersive augmented reality (IAR)–based systems on human wayfinding performance from the cognitive perspective. Aimed at investigating the influence of IAR-based systems on people’s cognitive map development and their subsequent wayfinding performance as well as the effect of using three-dimensional (3D) layout models in IAR environments in addition to superimposed guideposts, an experiment was carried out in a building with a complex floor plan. A total of 54 university students were evenly divided into three groups: a control group with no IAR assistance, a second group using an IAR-based navigation system that includes only superimposed guideposts, and a third group using an IAR-based navigation system that includes both guideposts and a 3D layout model. Each participant was asked to conduct a spatial exploration task in the environment, sketch a floor map based on their spatial cognition, and perform a wayfinding task to find eight specific locations in the building. An analysis of the participants’ performance and responses to a number of self-evaluation questionnaires collected in the experiment indicates that IAR technology can help people develop their cognitive maps more effectively and can substantially improve their wayfinding performance with a much lower workload. A second finding is that adding a 3D layout model can enhance the effect of an IAR-based navigation system in terms of cognitive map development. The findings from this research extend the existing knowledge about IAR-based navigation and further verify that AR technology has the potential to reduce human workload for cognitive tasks. The results also could support its more effective application in various scenarios that require assisted wayfinding and cognitive map training, such as emergency evacuation drills.}
}
@article{TOSHPULATOV2021104119,
title = {Generative adversarial networks and their application to 3D face generation: A survey},
journal = {Image and Vision Computing},
volume = {108},
pages = {104119},
year = {2021},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2021.104119},
url = {https://www.sciencedirect.com/science/article/pii/S026288562100024X},
author = {Mukhiddin Toshpulatov and Wookey Lee and Suan Lee},
keywords = {Generative adversarial networks, 3 face generation, Generator, Discriminator, Deep neural network, Deep learning},
abstract = {Generative adversarial networks (GANs) have been extensively studied in recent years and have been used to address several problems in the fields of image generation and computer vision. Despite significant advancements in computer vision, applying GANs to real-world problems such as 3D face generation remains a challenge. Owing to the proliferation of fake images generated by GANs, it is important to analyze and build a taxonomy for providing an overall view of GANs. This, in turn, would facilitate many interesting applications, including virtual reality, augmented reality, computer games, teleconferencing, virtual try-on, special effects in movies, and 3D avatars. This paper reviews and discusses GANs and their application to 3D face generation. We aim to compare existing GANs methods in terms of their application to 3D face generation, investigate the related theoretical issues, and highlight the open research problems. Authors provided both qualitative and quantitative evaluations of the proposed approach. They claimed their results show the higher quality of the synthesized data compared to state-of-the-art ones.}
}
@article{SCQUIZZATO202016,
title = {Enhancing citizens response to out-of-hospital cardiac arrest: A systematic review of mobile-phone systems to alert citizens as first responders},
journal = {Resuscitation},
volume = {152},
pages = {16-25},
year = {2020},
issn = {0300-9572},
doi = {https://doi.org/10.1016/j.resuscitation.2020.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0300957220301854},
author = {Tommaso Scquizzato and Ottavia Pallanch and Alessandro Belletti and Antonio Frontera and Luca Cabrini and Alberto Zangrillo and Giovanni Landoni},
keywords = {Out-of-hospital cardiac arrest, First responders, Cardiopulmonary resuscitation, Mobile-phone technology},
abstract = {Introduction
Involving laypersons in response to out-of-hospital cardiac arrest through mobile-phone technology is becoming widespread in numerous countries, and different solutions were developed. We performed a systematic review on the impact of alerting citizens as first responders and to provide an overview of different strategies and technologies used.
Methods
We searched electronic databases up to October 2019. Eligible studies described systems to alert citizens first responders to out-of-hospital cardiac arrest through text messages or apps. We analyzed the implementation and performance of these systems and their impact on patients’ outcomes.
Results
We included 28 manuscripts describing 12 different systems. The first text message system was implemented in 2006 and the first app in 2010. First responders accepted to intervene in median (interquartile) 28.7% (27–29%) of alerts and reached the scene after 4.6 (4.4–5.5) minutes for performing CPR. First responders arrived before ambulance, started CPR and attached a defibrillator in 47% (34–58%), 24% (23–27%) and 9% (6–14%) of cases, respectively. Pooled analysis showed that first responders activation increased layperson-CPR rates (1463/2292 [63.8%] in the intervention group vs. 1094/1989 [55.0%] in the control group; OR = 1.70; 95% CI, 1.11–2.60; p = 0.01) and survival to hospital discharge or at 30 days (327/2273 [14.4%] vs. 184/1955 [9.4%]; OR = 1.51; 95% CI, 1.24–1.84; p < 0.001).
Conclusions
Alerting citizens as first responders in case of out-of-hospital cardiac arrest may reduce the intervention-free time and improve patients’ outcomes.}
}
@article{KIM2023301608,
title = {Digital forensic approaches for metaverse ecosystems},
journal = {Forensic Science International: Digital Investigation},
volume = {46},
pages = {301608},
year = {2023},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2023.301608},
url = {https://www.sciencedirect.com/science/article/pii/S2666281723001208},
author = {Donghyun Kim and Subin Oh and Taeshik Shon},
keywords = {Digital Forensics, Metaverse, Augmented Reality (AR), Virtual Reality (VR), Ecosystem, Meta, Meta Quest},
abstract = {The accelerating pace of digital transformation has given rise to metaverses that can participate freely in contactless environments. More than just game content, metaverses are driving everyday innovation across industries. However, threats are also prevalent, with crimes such as child sexual exploitation and privacy violations occurring in metaverses that mimic reality, making digital forensics for metaverse threats essential. Nevertheless, technical standards for different types of metaverses have yet to be defined, making investigation difficult. Furthermore, even though metaverses are complex forms that combine multiple hardware devices and software applications, existing studies have either focused on a single component or not analyzed the real-world environment. In this study, we derived a metaverse ecosystem with common components that comprise a metaverse and analyzed the hardware and software used throughout the user's metaverse lifecycle from a digital forensics perspective. In particular, we applied real-case-based scenario to the metaverse environment of the most popular Meta's currently in use to identify various artifacts that can be used across the ecosystem and validate the effectiveness of the process. We also developed a metaverse digital forensics tool for the first time in the current situation where open-source and commercial tools do not support metaverse investigations.}
}
@article{JOHNSON2006121,
title = {Simulation Education in Emergency Medical Services for Children},
journal = {Clinical Pediatric Emergency Medicine},
volume = {7},
number = {2},
pages = {121-127},
year = {2006},
issn = {1522-8401},
doi = {https://doi.org/10.1016/j.cpem.2006.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S1522840106000267},
author = {Laurie Johnson and Mary D. Patterson},
keywords = {patient simulator, patient simulation},
abstract = {Simulation has been utilized as a training method for nearly 80 years by the aviation industry to improve pilots' skills and enhance the teamwork of the flight crew. Within the medical profession, simulation techniques have been used extensively by anesthesiologists for individual and teamwork training tasks. Following the example in anesthesiology, simulation curricula are being developed for teaching and evaluating core competencies for surgical and emergency medicine residents, prehospital personnel, and medical students. Simulation models can be as simple as static mannequins for procedural skills or as complex as high-fidelity human patient simulators that react physiologically to administered medications and provider actions. As this technology continues to mature, benefits of incorporating such safe and valuable education to trainees across all medical disciplines and training levels must be considered in light of the expense of such endeavors. Benefits of simulation training within the medical field include learning and perfecting procedural skills before patient interaction, participating in rare or complicated case scenarios which might otherwise not be encountered in traditional training, and nonprocedural training such as teamwork training and disaster management skills. As efforts continue to emerge to increase patient safety and reduce medical errors, simulation has become more popular in promoting patient autonomy while allowing trainees to experience positive and negative outcomes with the opportunity for constructive feedback, or debriefing, afterward.}
}
@article{BRUNZINI2023100505,
title = {Human-centred data-driven redesign of simulation-based training: a qualitative study applied on two use cases of the healthcare and industrial domains},
journal = {Journal of Industrial Information Integration},
volume = {35},
pages = {100505},
year = {2023},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2023.100505},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X2300078X},
author = {Agnese BRUNZINI and Margherita PERUZZINI and Pamela BARBADORO},
keywords = { Healthcare 4.0, Industry 4.0, Simulation-based training, Design optimization, Human Factors, Ergonomics},
abstract = {Among the main features of Industry 4.0, digitization and the evolution of the human-machine interaction occupy a central role. These concepts are transferring even in the health domain, moving toward Healthcare 4.0. The new concept of Industry 5.0 further promotes the human-centric perspective focusing on the consideration of human factors. In this context, training for workers, both in the industry and in the healthcare sectors, needs to be strongly human-centred to be efficient and effective. This paper refers to simulation-based training and aims to provide a transdisciplinary framework for the simulation assessment from the learners’ perspective. The final scope is to outline a set of data-driven guidelines for the simulation optimization and redesign, throughout a human-centred approach, aiming to improve the workers’ performance and the overall learning process, considering the physical, cognitive, and emotional conditions. The proposed method is suitable for each kind of training (both traditional and with the use of virtual reality/augmented reality systems) and relevant for every sector. Two different use cases are presented, respectively referring to the healthcare and industry fields, proposing a unique assessment protocol. The healthcare use case considered the low-fidelity simulation of lumbar puncture, while the industrial use case referred to the replacement of the engine oil filter on tractors. Although the great differences between the content of the use cases, the results obtained about performance as well as cognitive and emotional states are close enough to define a common set of guidelines to redesign and optimize the simulation-based training.}
}
@article{ROMAN2018680,
title = {Mobile edge computing, Fog et al.: A survey and analysis of security threats and challenges},
journal = {Future Generation Computer Systems},
volume = {78},
pages = {680-698},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16305635},
author = {Rodrigo Roman and Javier Lopez and Masahiro Mambo},
keywords = {Security, Privacy, Cloud computing, Fog computing, Mobile edge computing, Mobile cloud computing},
abstract = {For various reasons, the cloud computing paradigm is unable to meet certain requirements (e.g. low latency and jitter, context awareness, mobility support) that are crucial for several applications (e.g. vehicular networks, augmented reality). To fulfill these requirements, various paradigms, such as fog computing, mobile edge computing, and mobile cloud computing, have emerged in recent years. While these edge paradigms share several features, most of the existing research is compartmentalized; no synergies have been explored. This is especially true in the field of security, where most analyses focus only on one edge paradigm, while ignoring the others. The main goal of this study is to holistically analyze the security threats, challenges, and mechanisms inherent in all edge paradigms, while highlighting potential synergies and venues of collaboration. In our results, we will show that all edge paradigms should consider the advances in other paradigms.}
}
@article{CHEN2012833,
title = {Supporting Urban Search and Rescue with digital assessments of structures and requests of response resources},
journal = {Advanced Engineering Informatics},
volume = {26},
number = {4},
pages = {833-845},
year = {2012},
note = {EG-ICE 2011 + SI: Modern Concurrent Engineering},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2012.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1474034612000572},
author = {Albert Y. Chen and Feniosky Peña-Mora and Albert P. Plans and Saumil J. Mehta and Zeeshan Aziz},
keywords = {Urban Search and Rescue, Building assessment, MANET, RFID, GIS},
abstract = {First responders, including structural engineers and firefighters, inspect buildings and identify the structural integrity of buildings within a disaster affected area. The performance of their inspection and dissemination of the assessment information are critical to Urban Search and Rescue (US&R) operations. This paper presents an innovative approach for structural assessment and resource requests through an application – Supporting Urban Preparedness and Emergency Response using Mobile Ad hoc Network (SUPER-MAN). The goal of this research is to address challenges encountered in the current practice for structural engineers and first responders to inspect and disseminate building damage assessments and resource requests more efficiently to support US&R. The SUPER-MAN system is equipped with Radio Frequency Identification (RFID) tags, as the storage device of assessment information on the disaster site, and a Mobile Ad hoc Network (MANET) with a Dynamic Source Routing (DSR) implementation for communication. SUPER-MAN strengthens responders’ situational awareness, reduces confusion of inconsistent assessment formats, and automates information dissemination and editing. As a result, lifesaving operations are adequately prioritized, risk of first responders are minimized, and requests of response resources are facilitated. Results obtained from field trials carried out at the Illinois Fire Service Institute with a simulated disaster scenario and computer simulations of the MANET are presented to highlight the benefits provided by SUPER-MAN.}
}
@article{SYAMIMI2020409,
title = {VR industrial applications―A singapore perspective},
journal = {Virtual Reality & Intelligent Hardware},
volume = {2},
number = {5},
pages = {409-420},
year = {2020},
issn = {2096-5796},
doi = {https://doi.org/10.1016/j.vrih.2020.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S209657962030070X},
author = {Athirah Syamimi and Yiwei Gong and Ryan Liew},
keywords = {Virtual reality, VR applications, Building information modeling},
abstract = {Virtual Reality (VR) has been around for a long time but has come into the spotlight only recently. From an industrial perspective, this article serves as a proverbial scalpel to dissect the different use cases and commercial applications of VR in Singapore. Before researching the Singapore market, we examine how VR has evolved. At the moment, the global annual budget for VR (and augmented reality) is at an upward trend with a leading growth in market value for the training sector. VR in Singapore has also seen a rapid development in recent years. We discuss some of the Singapore government's initiatives to promote the commercial adoption of VR for the digital economy of the nation. To address the mass adoption of VR, we present VRcollab's business solutions for the construction and building industry. 2020 is one of the most important years for VR in history.}
}
@article{ZHANG2020114,
title = {A parallel vision approach to scene-specific pedestrian detection},
journal = {Neurocomputing},
volume = {394},
pages = {114-126},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.03.095},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219308859},
author = {Wenwen Zhang and Kunfeng Wang and Yating Liu and Yue Lu and Fei-Yue Wang},
keywords = {Pedestrian detection, Specific scene, Synthetic data, Video surveillance, Parallel vision},
abstract = {In recent years, with the development of computing power and deep learning algorithms, pedestrian detection has made great progress. Nevertheless, once a detection model trained on generic datasets (such as PASCAL VOC and MS COCO) is applied to a specific scene, its precision is limited by the distribution gap between the generic data and the specific scene data. It is difficult to train the model for a specific scene, due to the lack of labeled data from that scene. Even though we manage to get some labeled data from a specific scene, the changing environmental conditions make the pre-trained model perform bad. In light of these issues, we propose a parallel vision approach to scene-specific pedestrian detection. Given an object detection model, it is trained via two sequential stages: (1) the model is pre-trained on augmented-reality data, to address the lack of scene-specific training data; (2) the pre-trained model is incrementally optimized with newly synthesized data as the specific scene evolves over time. On publicly available datasets, our approach leads to higher precision than the models trained on generic data. To tackle the dynamically changing scene, we further evaluate our approach on the webcam data collected from Church Street Market Place, and the results are also encouraging.}
}
@article{BELLALOUNA2021400,
title = {The Augmented Reality Technology as Enabler for the Digitization of Industrial Business Processes: Case Studies},
journal = {Procedia CIRP},
volume = {98},
pages = {400-405},
year = {2021},
note = {The 28th CIRP Conference on Life Cycle Engineering, March 10 – 12, 2021, Jaipur, India},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.01.124},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121001542},
author = {Fahmi Bellalouna},
keywords = {Augmented Reality (AR), Product Configuration Management, Production Planning, Assembly Assistance, Idustrial Digitization},
abstract = {Augmented Reality Technology Augmented Reality Technology (AR) is one of the key technologies of digital transformation in industrial and non-industrial areas. Due to the rapid development of display hardware and tracking systems, virtual and augmented reality applications are being developed today that would only have been possible in large research laboratories with huge efforts a few years ago. This paper presents the application of the augmented reality technology in the industrial field. Based on industrial case studies the implementation of two AR applications and their potential to digitize the product lifecycle processes will be discussed. The case studies were developed within cooperation projects between the University of Applied Sciences Karlsruhe and a fire truck manufacturer with the focus on the following topics: product configuration management and assembly assistance. The objective of this work is the investigation of the AR application in the industrial environment and its capability as digital transformation technology along the product lifecycle. The presented paper delivers experiences and suggestions in terms of praxis-oriented development and deployment of the AR technology in the engineering field, which can be used for future AR research works.}
}
@article{HSIAO2023105913,
title = {Real-time fire protection system architecture for building safety},
journal = {Journal of Building Engineering},
volume = {67},
pages = {105913},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.105913},
url = {https://www.sciencedirect.com/science/article/pii/S235271022300092X},
author = {Chung-Jung Hsiao and Shang-Hsien Hsieh},
keywords = {Prevention level, Building information modeling, Quality management system, Simulated reality},
abstract = {Regardless of the type of disaster a building undergoes, personal safety, property preservation, and attribution of responsibility are the three assessment indicators that summarize the effectiveness of disaster prevention, evacuation, and rescue operations when the area of a building affected by a disaster endangers personal safety. The only possible response under these conditions is for people to immediately leave the dangerous area and escape to a safe and secure area. Therefore, both the timeliness of evacuation and rescue advice directly determine the outcome of the disaster. This study reflects on current fire protection systems and puts forward the approaches of Prevention Level for Deployment in Advance and Spatial Transformation by Human–Machine Collaboration to deal with disasters and promote building safety. A real-time fire protection system architecture for disaster prevention, evacuation, and rescue operations in buildings is proposed and prototyped for a proof of concept. The architecture promotes simulated verification of the effectiveness of disaster prevention, evacuation, and rescue operations at the building planning and design stage.}
}
@incollection{2018181,
title = {Index},
editor = {Ida Arlene Joiner},
booktitle = {Emerging Library Technologies},
publisher = {Chandos Publishing},
pages = {181-188},
year = {2018},
series = {Chandos Information Professional Series},
isbn = {978-0-08-102253-5},
doi = {https://doi.org/10.1016/B978-0-08-102253-5.00022-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022535000228}
}
@article{CHEN2021103631,
title = {Development of BIM, IoT and AR/VR technologies for fire safety and upskilling},
journal = {Automation in Construction},
volume = {125},
pages = {103631},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103631},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521000820},
author = {Haosen Chen and Lei Hou and Guomin (Kevin) Zhang and Sungkon Moon},
keywords = {Building fires, Situational awareness, Fire safety, IoT, BIM, VR, AR},
abstract = {High-rise building fires can pose a significant threat to occupants and firefighters. The state-of-the-art technologies such as sensor-based Internet of Things (IoT), Building Information Modelling (BIM), Virtual Reality (VR) and Augmented Reality (AR) may offer great potential to improve building fire safety and rescue efficiency, primarily through ameliorating the level of situational awareness. This study proposes an innovative technology integrated framework for prototyping a proof-of-concept BIM, IoT and AR/VR system based upon the rationale of situational awareness. A pilot test based on a simulated fire scenario is conducted to evaluate the functionality of the framework. The outcomes reveal that the data generated by the system can be leveraged by the firefighting department to quickly locate the whereabouts of the indoor fires, and the VR gamification scenarios can expedite the development of situational awareness for the trainees. The limitations and future works are also discussed at the end of this paper.}
}
@article{IQBAL2023100961,
title = {Cognitive D2D communication: A comprehensive survey, research challenges, and future directions},
journal = {Internet of Things},
volume = {24},
pages = {100961},
year = {2023},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100961},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523002846},
author = {Adeel Iqbal and Ali Nauman and Riaz Hussain and Muhammad Bilal},
keywords = {Cognitive D2D, Energy efficiency, Spectrum efficiency, Security, QoS, 5G, 6G},
abstract = {The integration of cognitive radio and device-to-device (D2D) communication gives rise to Cognitive D2D (cD2D) communication, which offers numerous advantages, such as improved spectrum and energy efficiency, increased network throughput, and enhanced coverage. Although there are existing survey papers on cognitive networks and D2D communications, the topic of integrated cD2D communication has received limited attention. To bridge this gap in the literature, this paper presents a comprehensive survey of cD2D communication. We commence by introducing cD2D communication and highlighting the key research challenges in this field. Subsequently, we categorize the recent advancements in cD2D communications into eight major types of techniques and provide an in-depth review of the existing literature. Furthermore, we address several significant challenges associated with cD2D communications and discuss future applications that can benefit from this technology. Through our analysis, we aim to contribute to a deeper understanding of cD2D communication and provide insights into its potential for various domains.}
}
@incollection{2023311,
title = {Index},
editor = {Surbhi {Bhatia Khan} and Suyel Namasudra and Swati Chandna and Arwa Mashat and Fatos Xhafa},
booktitle = {Innovations in Artificial Intelligence and Human-Computer Interaction in the Digital Era},
publisher = {Academic Press},
pages = {311-321},
year = {2023},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-99891-8},
doi = {https://doi.org/10.1016/B978-0-323-99891-8.20001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323998918200016}
}
@article{ATTARAN2023100165,
title = {Digital Twin: Benefits, use cases, challenges, and opportunities},
journal = {Decision Analytics Journal},
volume = {6},
pages = {100165},
year = {2023},
issn = {2772-6622},
doi = {https://doi.org/10.1016/j.dajour.2023.100165},
url = {https://www.sciencedirect.com/science/article/pii/S277266222300005X},
author = {Mohsen Attaran and Bilge Gokhan Celik},
keywords = {Digital Twins, Digital Twin Technologies, Digital Twin Drivers, Healthcare and Life Sciences, Automotive and Aerospace Industry, Construction and Real Estate},
abstract = {Applications of Digital Twin technology have been growing at an exponential rate, and it is transforming the way businesses operate. In the past few years, Digital Twins leveraged vital business applications, and it is predicted that the technology will expand to more applications, use cases, and industries. The purpose of this paper is to do a literature review and explore how Digital Twins streamline intelligent automation in different industries. This paper defines the concept, highlights the evolution and development of Digital Twins, reviews its key enabling technologies, examines its trends and challenges, and explores its applications in different industries.}
}
@article{GOLOMINGI2023451,
title = {Augmented reality in forensics and forensic medicine – Current status and future prospects},
journal = {Science & Justice},
volume = {63},
number = {4},
pages = {451-455},
year = {2023},
issn = {1355-0306},
doi = {https://doi.org/10.1016/j.scijus.2023.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S1355030623000321},
author = {Raffael Golomingi and Akos Dobay and Sabine Franckenberg and Lars Ebert and Till Sieberth},
keywords = {Augmented Reality, Forensics, Crime Scene Investigation, Forensic Medicine, Education},
abstract = {Forensic investigations require a vast variety of knowledge and expertise of each specialist involved. With the increase in digitization and advanced technical possibilities, the traditional use of a computer with a screen for visualization and a mouse and keyboard for interactions has limitations, especially when visualizing the content in relation to the real world. Augmented reality (AR) can be used in such instances to support investigators in various tasks at the scene as well as later in the investigation process. In this article, we present current applications of AR in forensics and forensic medicine, the technological basics of AR, and the advantages that AR brings for forensic investigations. Furthermore, we will have a brief look at other fields of application and at future developments of AR in forensics.}
}
@article{WEIDINGER2022103115,
title = {What is known and what remains unexplored: A review of the firefighter information technologies literature},
journal = {International Journal of Disaster Risk Reduction},
volume = {78},
pages = {103115},
year = {2022},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2022.103115},
url = {https://www.sciencedirect.com/science/article/pii/S221242092200334X},
author = {Julian Weidinger},
keywords = {Firefighter, Information technologies, Emergency management, Literature review},
abstract = {To increase the situation awareness of firefighters, literature frequently proposes the use of novel firefighter information technologies. Due to the versatility of the field and the lack of review papers, the existing body of literature in the domain appears largely fragmented. To bring structure to it and identify remaining literature gaps, this paper presents the results of an extensive literature review. A set of 108 recent papers was analyzed based on key concepts like the phases and types of firefighter operations. In doing so, we identified ten frequently researched types of technologies as well as their respective characteristics. The in-depth analysis of the results revealed several literature gaps regarding non-firefighting missions, support beyond the reconnaissance phase, integration of information, and domain specifics. All in all, the study constitutes a comprehensive and unique overview of the domain as well as a solid foundation for future research.}
}
@article{ALSBERG201224,
title = {Is sensing spatially distributed chemical information using sensory substitution with hyperspectral imaging possible?},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {114},
pages = {24-29},
year = {2012},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2012.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0169743912000238},
author = {Bjørn K. Alsberg},
keywords = {Augmented reality, Sensory substitution, Hyperspectral imaging, Hyperspectral camera, Chemometrics},
abstract = {Chemical images which are often computed from hyperspectral images contain the spatial distribution of chemical information of a scene. For many applications visualizing such images on computer screens is sufficient, however there are cases where there is a need to combine the chemical images more naturally with human vision. This is especially true for interactive work where chemical images are being rapidly updated to the user. Effective integration of spatial information in general from external sources with vision is a challenge. One approach is to overlay the view of the real physical world with computer-generated graphics as in augmented reality. However such cluttering of the visual field with computer-generated graphics may confuse the user and reduce functionality. Another is projecting the chemical images back onto the scene under study in order to render the chemical information in situ to the user. This approach, however has challenges in connection with very small and very large scenes under investigation. An alternative approach is here investigated based on the possibility of enhancing the human vision system using a sensory substitution device. Such devices enables a person to sense spatial information conveyed through sensory channels other than the eye, such as hearing and sense of touch. Results presented support the claim that spatial chemical information from a hyperspectral camera can be conveyed to the brain through a sensory channel different from the eyes. As this is tested on a sighted subject it effectively provides an extension of the human vision system to incorporate chemical information which otherwise is invisible to the naked eye.}
}
@article{BAKHTIARI2023104958,
title = {A critical review for the application of cutting-edge digital visualisation technologies for effective urban flood risk management},
journal = {Sustainable Cities and Society},
volume = {99},
pages = {104958},
year = {2023},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2023.104958},
url = {https://www.sciencedirect.com/science/article/pii/S2210670723005693},
author = {Vahid Bakhtiari and Farzad Piadeh and Kourosh Behzadian and Zoran Kapelan},
keywords = {Augmented reality, Digital twin, Digital visualisation, Flood risk management, Systematic review, Virtual reality},
abstract = {Cutting-edge digital visualisation tools (CDVT) are playing an increasingly important role in improving urban flood risk management. However, there is a paucity of comprehensive research examining their role across all stages of urban flood risk management. To address, this study conducts an integrated critical review to identify the application of CDVT and assess their contribution to the prevention, mitigation, preparation, response, and recovery stages of flood risk management. The results show that virtual reality, augmented reality, and digital twin technologies are the primary CDVT used in urban flood visualisation, with virtual reality being the most frequently used. The focus of urban flood visualisation studies has been primarily on preparation and mitigation stages. However, there is a need to investigate the application of these technologies in the entire urban water cycle. Furthermore, there is potential for greater adoption of digital twin, especially in simulating urban flood inundation and flood evacuation routes. Integrating real-time data, data-driven modeling, and CDVT can significantly improve real-time flood forecasting. This benefits stakeholders and the public by enhancing early warning systems, preparedness, and flood resilience, leading to more effective flood risk management and reduced impacts on communities.}
}
@article{HOLLERER2001799,
title = {User interface management techniques for collaborative mobile augmented reality},
journal = {Computers & Graphics},
volume = {25},
number = {5},
pages = {799-810},
year = {2001},
note = {Mixed realities - beyond conventions},
issn = {0097-8493},
doi = {https://doi.org/10.1016/S0097-8493(01)00122-4},
url = {https://www.sciencedirect.com/science/article/pii/S0097849301001224},
author = {Tobias Höllerer and Steven Feiner and Drexel Hallaway and Blaine Bell and Marco Lanzagorta and Dennis Brown and Simon Julier and Yohan Baillot and Lawrence Rosenblum},
keywords = {Augmented reality, Wearable computing, Information filtering, User interface design, View management},
abstract = {Mobile augmented reality systems (MARS) have the potential to revolutionize the way in which information is provided to users. Virtual information can be directly integrated with the real world surrounding the mobile user, who can interact with it to display related information, to pose and resolve queries, and to collaborate with other users. However, we believe that the benefits of MARS will only be achieved if the user interface (UI) is actively managed so as to maximize the relevance and minimize the confusion of the virtual material relative to the real world. This article addresses some of the steps involved in this process, focusing on the design and layout of the mobile user's overlaid virtual environment. The augmented view of the user's surroundings presents an interface to context-dependent operations, many of which are related to the objects in view—the augmented world is the user interface. We present three UI design techniques that are intended to make this interface as obvious and clear to the user as possible: information filtering, UI component design, and view management. Information filtering helps select the most relevant information to present to the user. UI component design determines the format in which this information should be conveyed, based on the available display resources and tracking accuracy. For example, the absence of high accuracy position tracking would favor body- or screen-stabilized components over world-stabilized ones that would need to be exactly registered with the physical objects to which they refer. View management attempts to ensure that the virtual objects that are displayed visually are arranged appropriately with regard to their projections on the view plane. For example, the relationships among objects should be as unambiguous as possible, and physical or virtual objects should not obstruct the user's view of more important physical or virtual objects in the scene. We illustrate these interface design techniques using our prototype collaborative, cross-site MARS environment, which is composed of mobile and non-mobile augmented reality and virtual reality systems.}
}
@article{MONARES20111255,
title = {Mobile computing in urban emergency situations: Improving the support to firefighters in the field},
journal = {Expert Systems with Applications},
volume = {38},
number = {2},
pages = {1255-1267},
year = {2011},
note = {Intelligent Collaboration and Design},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2010.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S095741741000429X},
author = {Álvaro Monares and Sergio F. Ochoa and José A. Pino and Valeria Herskovic and Juan Rodriguez-Covili and Andrés Neyem},
keywords = {Mobile collaborative application, Communication support, Emergency support, Decision support system, Mobile computing},
abstract = {Communication support is a serious limitation for Latin American firefighters when they deal with emergency situations. The insufficient number of radio channels and the impossibility to deliver digital information force firemen to improvise during response processes, e.g., to make decisions using their experience and poor or null supporting information. These improvised actions affect the time required to take control of an emergency, and also affect the evolution of the crisis situation. Provided most of Latin American fire companies are volunteer organizations, communication solutions that could help to overcome these problems are usually expensive for them. This article presents a low-cost mobile collaborative application, which may be used in emergency situations to overcome most of the firefighters’ communication problems. The application, named MobileMap, is the result of the research and development work conducted by the authors, supported by a Chilean fire company, during the last three years. MobileMap allows ad hoc communication, decisions support and collaboration among firefighters in the field using mobile devices. This solution complements the radio communication systems. Since the interactions supported by MobileMap are recorded, it is possible to analyze such information after the crisis and learn for future emergencies. The tool was evaluated in simulated and real scenarios, and the obtained results are highly encouraging.}
}
@article{KANCHANA2022961,
title = {A study of internet of things oriented smart medical systems},
journal = {Materials Today: Proceedings},
volume = {51},
pages = {961-964},
year = {2022},
note = {CMAE'21},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.06.363},
url = {https://www.sciencedirect.com/science/article/pii/S221478532104757X},
author = {V. Kanchana and Surendra Nath and Mahesh K. Singh},
keywords = {e-Health, Health-care, IoT health monitoring system, Emergency medical services, Sensor networks},
abstract = {Aging populace ratios are increasing drastically. Health monitoring systems (HMS) based on IoT in smart environments have developed quickly to developed into a feasible substitute to conventional healthcare solution by using IoT. The major objective of HMS is not merely decreasing the expenditure but to also provided e-health service timely to persons. It is feasible when using IoT aspiration to preserve their self-determination. By this way, aged populace be able to avoid, for as extensive as achievable. In any communication by healthcare institution are connecting through internet as example hospitals and nursing homes. It is in turn to reduce the pressure on the health system. To completely realize this revelation of unspoiled IoT based e-health service sustaining the people in requirement of them. Quantities of challenge that require additional examination unmoving survive. At the conclusion, it is provided a summary of the existing condition for smart health monitor system by IoT. Here presented a combined image of the most significant function as well as services obtainable by HMS for the detecting and monitoring human behavior. It is counting its processing techniques, approaches and concepts etc. Furthermore, it is provided a general, in detail study and assessment of the obtainable research conclusion in the field of e-health systems through IoT.}
}
@article{BEHZADAN2008737,
title = {Ubiquitous location tracking for context-specific information delivery on construction sites},
journal = {Automation in Construction},
volume = {17},
number = {6},
pages = {737-748},
year = {2008},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2008.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0926580508000186},
author = {Amir H. Behzadan and Zeeshan Aziz and Chimay J. Anumba and Vineet R. Kamat},
keywords = {Augmented reality, Construction, Context awareness, Information delivery, Location tracking},
abstract = {Construction projects are information-intensive in nature and require site personnel to have continuous on-demand access to information such as project plans, drawings, schedules, and budgets. Awareness of a user's context (such as user profile, role, preferences, task, and existing project conditions) can enhance the construction project delivery process by providing a mechanism to determine information relevant to a particular context. Context awareness can also be used to improve security, logistics and health and safety practices on construction sites. Location is an important aspect of context awareness. A location aware application can utilize the knowledge of the user/object location to provide relevant information and services. This paper argues that a successful and reliable location tracking system must be able to track a user's spatial context and deliver contextual data continuously in both outdoor and indoor environments to effectively support construction projects. Research describing the use of Wireless Local Area Network (WLAN) for indoor tracking and Global Positioning System (GPS) for outdoor spatial context tracking is presented, and an integrated tracking technique using WLAN and GPS for ubiquitous location sensing is introduced. The key benefits and technical challenges of such an integrated approach are also highlighted. The presented tracking techniques have been validated in both indoor and outdoor environments to ensure their practical implementation on real construction jobsites.}
}
@article{MCCARTHY2016166,
title = {How transport users perceive personal safety apps},
journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
volume = {43},
pages = {166-182},
year = {2016},
issn = {1369-8478},
doi = {https://doi.org/10.1016/j.trf.2016.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1369847816304107},
author = {Orla Thérèse McCarthy and Brian Caulfield and Margaret O’Mahony},
keywords = {Personal safety, Phone apps, Public transport},
abstract = {Fear of crime and a perceived sense of being unsafe have the potential to discourage individuals from using public transport. This paper presents analysis of the results of a survey on aspects of personal safety apps and how individuals perceive them in relation to their personal safety, privacy and their preference to purchase them. It explores their willingness to download for free or purchase such an app, their impression of features that an app might include such as revealing their location, how they would rate police force monitoring if included as a feature of the app and finally how they would rate a personal safety app against other technologies used to improve perceived transport user safety. The results show that the majority of respondents would consider downloading a personal safety app. Lower levels of engagement with technology, a higher level of education, being resident in the city of Dublin (as opposed to surrounding regions) and privacy concerns tended to make females less likely to consider downloading the app. These findings were not repeated for males. The results suggest that younger respondents were more likely to be negatively impacted by the inclusion of a facility to report location in an emergency on the app, while the older age groups were more likely to be unaffected. For the location tracking feature, cluster membership, gender and resident outside Dublin were significant. Less than half of the respondents would be negatively influenced in their decision to buy the app if a cost of €1.79 were introduced. When respondents were asked about the inclusion of police monitoring as a feature of the app, the results suggested that police monitoring had a more definite effect on perceived personal safety than on privacy.}
}
@article{CARLAW20208,
title = {Impact on biometrics of Covid-19},
journal = {Biometric Technology Today},
volume = {2020},
number = {4},
pages = {8-9},
year = {2020},
issn = {0969-4765},
doi = {https://doi.org/10.1016/S0969-4765(20)30050-3},
url = {https://www.sciencedirect.com/science/article/pii/S0969476520300503},
author = {Stuart Carlaw},
abstract = {At this time of global concern for the health of our loved ones, communities and workplaces, ABI Research analysts have assessed the likely short and long-term impacts that the global Covid-19 pandemic will have on the biometrics and related technology markets. Beyond the significant cost to human life due to the pandemic, it can be argued that Covid-19 will also have a significant and long-term impact on biometrics companies, developers, investors and customers.}
}
@article{SERMET2022105010,
title = {GeospatialVR: A web-based virtual reality framework for collaborative environmental simulations},
journal = {Computers & Geosciences},
volume = {159},
pages = {105010},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2021.105010},
url = {https://www.sciencedirect.com/science/article/pii/S0098300421002934},
author = {Yusuf Sermet and Ibrahim Demir},
keywords = {Virtual reality, Geospatial visualization, Disaster management, Decision-support systems, Web-based interaction},
abstract = {This research introduces GeospatialVR, an open-source collaborative virtual reality framework to dynamically create 3D real-world environments that can be served on any web platform and accessed via desktop and mobile devices and virtual reality headsets. The framework can generate realistic simulations of desired locations entailing the terrain, elevation model, infrastructures, dynamic visualizations (e.g. water and fire simulation), and information layers (e.g. disaster damages and extent, sensor readings, occupancy, traffic, weather). These layers enable in-situ visualization of useful data to aid public, scientists, officials, and decision-makers in acquiring a bird's eye view of the current, historical, or forecasted condition of a community. The framework incorporates multiuser support to allow different stakeholders to remotely work on the same VR environment and observe other users' actions and 3D positions via avatars in real-time, and thus, presenting the potential to be utilized as a virtual incident command center or a meeting room. GeospatialVR's purpose is to enhance existing web-based cyberinfrastructure systems with the integration of immersive geospatial capabilities to assist the development of next-generation information and decision support systems powered by virtual reality. Finally, several case studies have been developed for flooding, wildfire, transportation, and public safety.}
}
@article{TOSHPULATOV2023119678,
title = {Talking human face generation: A survey},
journal = {Expert Systems with Applications},
volume = {219},
pages = {119678},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119678},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423001793},
author = {Mukhiddin Toshpulatov and Wookey Lee and Suan Lee},
keywords = {Talking human face animation, 3 face generation, Deep generative model, Autoencoder, Neural radiance field, Datasets, Evaluation metrics, Neural networks, Unsupervised learning, Mel spectogram},
abstract = {Talking human face generation aims at synthesizing a natural human face that talks in correspondence to the given text or audio series. Implementing the recently developed Deep Learning (DL) methods such as Convolutional Neural Networks (CNN), Generative Adversarial Networks (GAN)s, Neural Rendering Fields (NeRF) for data generation, and talking human face generation has attracted significant research interest from academia and industry. They have been explored and exploited recently and have been used to address several problems in image processing and computer vision. Notwithstanding notable advancements, implementing them to real-world problems such as talking human face generation remains challenging. The generation of deepfakes created by the abovementioned methods would greatly promote many fascinating applications, including augmented reality, virtual reality, computer games, teleconferencing, virtual try-on, special movie effects, and avatars. This research reviews and discusses DL related methods, including CNN, GANs, NeRF, and their implementation in talking human face generation. We aim to analyze existing approaches regarding their implementation to talking face generation, investigate the related general problems, and highlight the open study issues. We also provide quantitative and qualitative evaluations of the existing research approaches in the related field.}
}
@article{DAVE2013133,
title = {Augmenting Situational Awareness for First responders using Social media as a sensor},
journal = {IFAC Proceedings Volumes},
volume = {46},
number = {15},
pages = {133-140},
year = {2013},
note = {12th IFAC Symposium on Analysis, Design, and Evaluation of Human-Machine Systems},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20130811-5-US-2037.00088},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016330567},
author = {Rakesh Dave and Sanjay K. Boddhu and Matt McCartney and James West},
abstract = {First responders to an emergency situation rely on ground truths measured by various sensing mechanisms for effective decision making. The sensors are typically airborne or ground based. Seamless sharing of information among users using Social networking provides for a unique type of sensor. This human-as-a sensor is already deployed in the field and only requires harvesting of the information to glean ground truth. Further, the proliferation of the smartphones and their connectivity networks has powered the ordinary individuals to share and acquire information regarding the events happening in his/her immediate vicinity in a real-time fashion. The information provided by these sensors is already annotated with descriptions such as “urgency” “critically wounded” which normally would not be found in traditional machine based sensors. Further, when appropriately employed this real-time data can support in detecting localized events like fire, accidents, shooting, etc…, as they unfold and pin-point individuals being affected by those events. The spatio-temporal information can be indexed, grouped and deployed on Smartphones and other devices that first responders can use in the field to augment decision making. In this vein, under SATE and YATE programs, the research team at AFRL TecˆEdge Discovery labs had demonstrated the feasibility of developing Smartphone applications, that can provide a augmented reality view of the appropriate detected events in a given geographical location (localized) and also provide an event search capability over a large geographic extent. In its current state, the application thru its backend connectivity utilizes a data (Text & Image) processing framework, which deals with data challenges like; identifying and aggregating important events, analyzing and correlating the events temporally and spatially and building a search enabled event database. Further, the smartphone application with its backend data processing workflow has been successfully field tested with live user generated feeds.}
}
@article{QAMAR2023103127,
title = {A systematic threat analysis and defense strategies for the metaverse and extended reality systems},
journal = {Computers & Security},
volume = {128},
pages = {103127},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103127},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823000378},
author = {Sara Qamar and Zahid Anwar and Mehreen Afzal},
keywords = {Extended reality (XR), Metaverse, Cyber defense, Privacy, Cyber threats, Cyberstalking, Physical safety, XR commerce, Virtual reality, Augmented reality, Mixed reality, Blockchain, Cybersickness, Currency scams},
abstract = {With the rapid development and evolution of immersive technologies there are growing concerns of security and privacy threats to the metaverse and extended reality (XR) systems. Immersive reality solutions are a combination of multiple vulnerable technologies allowing attackers to easily undermine security. Furthermore the deployment of appropriate security controls and defensive mechanisms for resource constrained proprietary XR products has been limited. In this paper, we provide a comprehensive overview of extended reality systems and the metaverse with emphasis on technology weaknesses, cyber security challenges and users’ safety concerns. Five major taxonomies have been presented in this research with an aim of identifying privacy inference vectors and potential cyber threats; determining the impact on human health and the extent to which cyberstalking, and digital currency scam activities proliferate when using XR. This research also proposes strategies for primary lines of defense and provides recommendations on the adoption of safety measures.}
}
@article{WU2022238,
title = {Signalling security: An observational and game theory approach to inter-pedestrian psychology},
journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
volume = {86},
pages = {238-251},
year = {2022},
issn = {1369-8478},
doi = {https://doi.org/10.1016/j.trf.2022.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S1369847822000419},
author = {Yifei Wu and Hansong Li},
keywords = {Pedestrians, Public safety, Anxiety, Signalling, Communication, Game theory},
abstract = {Whereas the mental health and personal safety of pedestrians have been the subject of both debates in the public sphere and discourses in social sciences, the interpersonal dynamic of citizens on foot remains so far unexplored. This paper takes a game-theoretical approach to the psychology, behaviour, and welfare of pedestrians, who experience different levels of mutualised anxiety and confidence in spontaneous encounters with strangers on the road. Through a combined method of survey, modelling, and theory, it proposes an equilibrium-analysis of inter-pedestrian signalling, as well as a set of public policy recommendations aimed to reduce unnecessary frictions, improve information transparency, and therefore promote public safety.}
}
@article{HALEEM202242,
title = {Holography and its applications for industry 4.0: An overview},
journal = {Internet of Things and Cyber-Physical Systems},
volume = {2},
pages = {42-48},
year = {2022},
issn = {2667-3452},
doi = {https://doi.org/10.1016/j.iotcps.2022.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S2667345222000141},
author = {Abid Haleem and Mohd Javaid and Ravi Pratap Singh and Rajiv Suman and Shanay Rab},
keywords = {Holography, Industry 4.0, Technology, 3D modelling, Virtual reality, Augmented reality},
abstract = {Industry 4.0 is a new stage in the organisation and control of the industrial value chain, interchangeably with the fourth industrial revolution. It has a broad vision with well-defined frameworks and reference designs, focusing on bridging physical infrastructure and digital technology in so-called cyber-physical systems. Apart from the other essential technologies, Holography is considered a new innovative technology that can completely transform the vision of Industry 4.0. In industrial applications, holographic technology is used for quality control in manufacturing and fracture testing, such as holographic nondestructive testing. Holography has a wide range of applications in medicine, the military, weather forecasting, virtual reality, digital art, and security. The fourth industrial revolution aims to provide automated asset monitoring, decision-making for corporate operations, and real-time network connectivity. This paper explores holography and its significant benefits through various development processes, features, and applications, where the focus is on ‘holography for Industry 4.0'. Hologram technology is a new industry trend and can impact multiple domains of Industry 4.0. Furthermore, the adoption of holographic technologies may improve the efficiency of existing products and services in other technology sectors such as architecture, 3D modelling, mechatronics, robotics, and healthcare and medical engineering.}
}
@article{MEINER2019445,
title = {Combining virtual reality and mobile eye tracking to provide a naturalistic experimental environment for shopper research},
journal = {Journal of Business Research},
volume = {100},
pages = {445-458},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2017.09.028},
url = {https://www.sciencedirect.com/science/article/pii/S0148296317303478},
author = {Martin Meißner and Jella Pfeiffer and Thies Pfeiffer and Harmen Oppewal},
keywords = {Eye tracking, Visual attention, Virtual reality, Augmented reality, Assistance system, Shopper behavior},
abstract = {Technological advances in eye tracking methodology have made it possible to unobtrusively measure consumer visual attention during the shopping process. Mobile eye tracking in field settings however has several limitations, including a highly cumbersome data coding process. In addition, field settings allow only limited control of important interfering variables. The present paper argues that virtual reality can provide an alternative setting that combines the benefits of mobile eye tracking with the flexibility and control provided by lab experiments. The paper first reviews key advantages of different eye tracking technologies as available for desktop, natural and virtual environments. It then explains how combining virtual reality settings with eye tracking provides a unique opportunity for shopper research in particular regarding the use of augmented reality to provide shopper assistance.}
}
@article{TABONE2021100293,
title = {Vulnerable road users and the coming wave of automated vehicles: Expert perspectives},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {9},
pages = {100293},
year = {2021},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2020.100293},
url = {https://www.sciencedirect.com/science/article/pii/S2590198220302049},
author = {Wilbert Tabone and Joost {de Winter} and Claudia Ackermann and Jonas Bärgman and Martin Baumann and Shuchisnigdha Deb and Colleen Emmenegger and Azra Habibovic and Marjan Hagenzieker and P.A. Hancock and Riender Happee and Josef Krems and John D. Lee and Marieke Martens and Natasha Merat and Don Norman and Thomas B. Sheridan and Neville A. Stanton},
keywords = {Automated vehicles, External human-machine interfaces, Smart infrastructure, Augmented reality, Virtual reality, Position paper},
abstract = {Automated driving research over the past decades has mostly focused on highway environments. Recent technological developments have drawn researchers and manufacturers to look ahead at introducing automated driving in cities. The current position paper examines this challenge from the viewpoint of scientific experts. Sixteen Human Factors researchers were interviewed about their personal perspectives on automated vehicles (AVs) and the interaction with VRUs in the future urban environment. Aspects such as smart infrastructure, external human-machine interfaces (eHMIs), and the potential of augmented reality (AR) were addressed during the interviews. The interviews showed that the researchers believed that fully autonomous vehicles will not be introduced in the coming decades and that intermediate levels of automation, specific AV services, or shared control will be used instead. The researchers foresaw a large role of smart infrastructure and expressed a need for AV-VRU segregation, but were concerned about corresponding costs and maintenance requirements. The majority indicated that eHMIs will enhance future AV-VRU interaction, but they noted that implicit communication will remain dominant and advised against text-based and instructive eHMIs. AR was commended for its potential in assisting VRUs, but given the technological challenges, its use, for the time being, was believed to be limited to scientific experiments. The present expert perspectives may be instrumental to various stakeholders and researchers concerned with the relationship between VRUs and AVs in future urban traffic.}
}
@article{KERIN2019117805,
title = {A review of emerging industry 4.0 technologies in remanufacturing},
journal = {Journal of Cleaner Production},
volume = {237},
pages = {117805},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.117805},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619326654},
author = {Mairi Kerin and Duc Truong Pham},
keywords = {Industry 4.0, Virtual reality, Augmented reality, Internet of things, Cyber-physical systems, Smart remanufacturing},
abstract = {This paper reviews the literature on the emerging digital technologies of Industry 4.0 (I4.0) focussed on the applicability of the Internet of Things (IoT), Virtual Reality (VR) and Augmented Reality (AR) in remanufacturing. Inspired by the frameworks developed to support exploration and realisation of I4.0 technologies for disassembly, the paper discusses the same emerging technologies in the wider context of remanufacturing. Trends and gaps have been identified from a value-creation perspective that encompasses the product to be remanufactured, the remanufacturing equipment and processes adopted and related organisation issues. Findings suggest there is a need to explore the connection of cyber-physical systems to the IoT to support smart remanufacturing, whilst aligning with evolving information and communication infrastructures and circular economy business models. The review highlights twenty-nine research topics that require attention to support this field.}
}
@article{SCHROEDER2023109772,
title = {KIDS SAVE LIVES: Basic Life Support Education for Schoolchildren: A Narrative Review and Scientific Statement From the International Liaison Committee on Resuscitation},
journal = {Resuscitation},
volume = {188},
pages = {109772},
year = {2023},
issn = {0300-9572},
doi = {https://doi.org/10.1016/j.resuscitation.2023.109772},
url = {https://www.sciencedirect.com/science/article/pii/S0300957223000850},
author = {Daniel C. Schroeder and Federico Semeraro and Robert Greif and Janet Bray and Peter Morley and Michael Parr and Naomi {Kondo Nakagawa} and Taku Iwami and Simon-Richard Finke and Carolina {Malta Hansen} and Andrew Lockey and Marina {Del Rios} and Farhan Bhanji and Comilla Sasson and Stephen M. Schexnayder and Tommaso Scquizzato and Wolfgang A. Wetsch and Bernd W. Böttiger},
keywords = {cardiopulmonary resuscitation, ILCOR Scientific Statements, out-of-hospital cardiac arrest, retrospective studies, students, sudden cardiac death},
abstract = {BACKGROUND
Basic life support education for schoolchildren has become a key initiative to increase bystander cardiopulmonary resuscitation rates. Our objective was to review the existing literature on teaching schoolchildren basic life support to identify the best practices to provide basic life support training in schoolchildren.
METHODS
After topics and subgroups were defined, a comprehensive literature search was conducted. Systematic reviews and controlled and uncontrolled prospective and retrospective studies containing data on students <20 years of age were included.
RESULTS
Schoolchildren are highly motivated to learn basic life support. The CHECK-CALL-COMPRESS algorithm is recommended for all schoolchildren. Regular training in basic life support regardless of age consolidates long-term skills. Young children from 4 years of age are able to assess the first links in the chain of survival. By 10 to 12 years of age, effective chest compression depths and ventilation volumes can be achieved on training manikins. A combination of theoretical and practical training is recommended. Schoolteachers serve as effective basic life support instructors. Schoolchildren also serve as multipliers by passing on basic life support skills to others. The use of age-appropriate social media tools for teaching is a promising approach for schoolchildren of all ages.
CONCLUSIONS
Schoolchildren basic life support training has the potential to educate whole generations to respond to cardiac arrest and to increase survival after out-of-hospital cardiac arrest. Comprehensive legislation, curricula, and scientific assessment are crucial to further develop the education of schoolchildren in basic life support.}
}
@article{KLEIN2020119883,
title = {Understanding controversies in digital platform innovation processes: The Google Glass case},
journal = {Technological Forecasting and Social Change},
volume = {152},
pages = {119883},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2019.119883},
url = {https://www.sciencedirect.com/science/article/pii/S0040162519310285},
author = {Amarolinda Klein and Carsten Sørensen and Angilberto Sabino de Freitas and Cristiane Drebes Pedron and Silvia Elaluf-Calderwood},
keywords = {Controversy, Digital platform innovation processes, Innovation management, ARSG (Augmented Reality Smart Glasses)},
abstract = {Due to their scaling potential and complexity, digital platforms tend to generate public interest, and in some cases significant controversies and paradoxes. Previous research has generated knowledge about controversies in digital platform innovations. However, this work mainly focuses on the types of controversies and their effects rather than on the process of controversy emergence. In this article, we analyze how controversies related to digital platform innovation emerge and how they unfold over the innovation process. We analyze the case of the Google Glass failure to establish this ARSG (Augmented Reality Smart Glasses) extension to Google's digital platform. The paper contributes to the study of controversies by analyzing the digital platform innovation process as a process of translation, in which there are possible controversy emergence points originated in types of disagreements among the different human actors involved and their interactions with non-human elements. These disagreements are related to specific features of digital platforms: the digital platform generativity, the multisided market arrangements in the platform; the loosely coupled layers of technologies and applications involved, and the opaqueness that results from these arrangements. The framework proposed can support digital platform scholars and practitioners to in better understand and manage controversies.}
}
@article{GREGORIADES20153033,
title = {Human Requirements Validation for Complex Systems Design},
journal = {Procedia Manufacturing},
volume = {3},
pages = {3033-3040},
year = {2015},
note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2015.07.848},
url = {https://www.sciencedirect.com/science/article/pii/S2351978915008495},
author = {Andreas Gregoriades and Jack Hadjicosti and Christos Florides and Maria Pamapaka},
keywords = {Driving simulator, Requirements discovery, Complex systems, Human factors analysis},
abstract = {One of the most critical phases in complex systems design is the requirements engineering process. During this phase, system designers need to accurately elicit, model and validate the desired system based on user requirements. Smart driver assistive technologies (SDAT) belong to a class of complex systems that are used to alleviate accident risk by improving situation awareness, reducing driver workload or enhancing driver attentiveness. Such systems aim to draw drivers’ attention on critical information cues that improve decision making. Discovering the requirements for such systems necessitates a holistic approach that addresses not only functional and non-functional aspects but also the human requirements such as drivers’ situation awareness and workload. This work describes a simulation-based user requirements discovery method. It utilizes the benefits of a modular virtual reality simulator to model driving conditions to discover user needs that subsequently inform the design of prototype SDATs that exploit the augmented reality method. Herein, we illustrate the development of the simulator, the elicitation of user needs through an experiment and the prototype SDAT designs using UNITY game engine.}
}
@article{CRIDER2020102579,
title = {Digital Daimons: Algorithmic Rhetorics of Augmented Reality},
journal = {Computers and Composition},
volume = {57},
pages = {102579},
year = {2020},
note = {Composing Algorithms: Writing (with) Rhetorical Machines},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2020.102579},
url = {https://www.sciencedirect.com/science/article/pii/S8755461520300402},
author = {Jason Crider and Jacob Greene and Sean Morey},
keywords = {Algorithms, augmented reality, Daimon, GPS, writing studies, digital rhetoric, Mixed-Reality, Location-Based Writing, Electracy},
abstract = {This article develops a theoretical framework for studying the algorithmic underpinnings of contemporary augmented reality technologies. We delineate this framework through the rhetorical figure of the “daimon,” a greek mythological entity as well as a technical concept within computer science, to articulate an approach to AR algorithms as emergent, material processes that can create unpredicted, unintended effects. Ultimately, we argue that the conceptual framework of the “daimon” provides an interface through which writing and rhetoric scholars can better discern the algorithmic effects of emerging AR composing platforms.}
}
@article{ZAHABI2020103041,
title = {Human factors in police mobile computer terminals: A systematic review and survey of recent literature, guideline formulation, and future research directions},
journal = {Applied Ergonomics},
volume = {84},
pages = {103041},
year = {2020},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2019.103041},
url = {https://www.sciencedirect.com/science/article/pii/S0003687019302479},
author = {Maryam Zahabi and Carl Pankok and Junho Park},
keywords = {Productivity, Physical effects, Usability, Driving distraction},
abstract = {The objectives of this research were to: (1) identify Mobile Computer Terminal (MCT) human factors issues, (2) formulate guidelines and an enhanced MCT for improving interface design and implementation in police patrols, and (3) identify areas of future research to fill gaps in the literature. A systematic literature search was conducted leading to results categorized in four groups including: productivity, physical discomfort, interface usability, and driving distraction. Although MCT use has increased officer productivity, several usability issues need to be resolved. The MCT has also increased officer physical discomfort and distraction. MCT design and implementation guidelines that resolve human factors issues in police patrols were identified along with an enhanced design concept. Guidelines for MCT design were validated with an online survey completed by 81 police officers. Future research directions were proposed to recognize police officer needs and work context.}
}
@article{BELLALOUNA2020262,
title = {New Approach for Industrial Training Using Virtual Reality Technology},
journal = {Procedia CIRP},
volume = {93},
pages = {262-267},
year = {2020},
note = {53rd CIRP Conference on Manufacturing Systems 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120305539},
author = {Fahmi Bellalouna},
keywords = {Virtual Reality, VR Data, CAD Data, Cognitive Approach, Intuitive Approach},
abstract = {This paper presents two case studies achieved within industrial cooperation projects between the University of Applied Sciences Karlsruhe and German manufacturers for special appliances. The aim of the case studies is development and implementation of training applications for the use and the handling of special vehicle using the virtual reality technology. Based on the experiences gathered during these cooperation projects the challenges that face the VR introduction in the industrial area is outlined in this paper. Furthermore, a best practice approach on how to transfer CAD to VR data to implement industrial VR application is presented in this contribution.}
}
@article{LUO2021173,
title = {Psychiatric Clinics: Computer-Based Teaching},
journal = {Psychiatric Clinics of North America},
volume = {44},
number = {2},
pages = {173-181},
year = {2021},
note = {Medical Education in Psychiatry},
issn = {0193-953X},
doi = {https://doi.org/10.1016/j.psc.2021.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0193953X21000083},
author = {John Luo},
keywords = {Distance learning, Computerized teaching, Computer-based teaching, Technology for teaching}
}
@article{LOZOYASANTOS2019175,
title = {Survey on biometry for cognitive automotive systems},
journal = {Cognitive Systems Research},
volume = {55},
pages = {175-191},
year = {2019},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2019.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S1389041717301948},
author = {Jorge de J. Lozoya-Santos and Victorino Sepúlveda-Arróniz and Juan C. Tudon-Martinez and Ricardo A. Ramirez-Mendoza},
keywords = {Biometry, Biometrics, Cognitive, Autonomous, Intelligent vehicles, Automotive systems},
abstract = {A survey on biometry for cognitive automotive systems is presented in this paper, specially those biometric systems used for high tech security access, law enforcement and/or commercial transactions. In general, biometric systems can be expensive due to the amount of sensors and processing resources involved. Efforts have been made to integrate these systems to vehicles mostly for security purposes and user authentication. Until now, most of the systems do not go beyond using facial and fingerprint data to start the engine or access the car; however, new generations demand more personalization plus the vehicle making decisions based on their physiological characteristics. Although this kind of technology is considered a luxury feature in general, actually it could help users and save their lives. Indeed, biometry is the way to make the human-vehicle relationship happen, whether the biometric devices are embedded inside the vehicle, used as an accessory or wearables. By using as input not only the sensors inside the vehicle but also data from outside, the vehicle could adapt and/or learn new information to make the best possible decision.}
}
@article{LI201417,
title = {Situational awareness for supporting building fire emergency response: Information needs, information sources, and implementation requirements},
journal = {Fire Safety Journal},
volume = {63},
pages = {17-28},
year = {2014},
issn = {0379-7112},
doi = {https://doi.org/10.1016/j.firesaf.2013.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S0379711213002269},
author = {Nan Li and Zheng Yang and Ali Ghahramani and Burcin Becerik-Gerber and Lucio Soibelman},
keywords = {Building fire emergency, Card game, First responder, Situational awareness, Survey},
abstract = {Building fire emergencies are big threats to safety of building occupants and first responders, and they can result in significant property losses. During building fire emergency response operations, establishment and utilization of situational awareness are of essential importance, enabling first responders, especially incident commanders, to better assess changing on-scene situations and make informed decisions. The paper uses an interactive card game to guide first responders to establish situational awareness about an imaginary building fire emergency, and examines the information items needed by first responders in this process. An evaluation of the importance of each information item is performed. The results show that location and status of occupants and first responders, and status and development of fire and smoke are the foremost important information items. Information items in the “at emergency scene” category were updated on average five times more frequently than information items in other categories. In order to investigate sources for obtaining the above information, and examine requirements for better implementing situational awareness at emergency scenes, a nationwide survey was conducted with first responders in the United States. The results point out a discrepancy between current and desired information sources for establishing situational awareness at building fire emergency scenes. The paper also reports detailed implementation requirements, including type of assistance, level of details and format of representation for establishing situational awareness, and illustrates statistical impacts of location, years of experience and job title on the requirements.}
}
@incollection{2013155,
editor = {Gregory Kipper},
booktitle = {Augmented Reality},
publisher = {Syngress},
address = {Boston},
pages = {155-158},
year = {2013},
isbn = {978-1-59749-733-6},
doi = {https://doi.org/10.1016/B978-1-59-749733-6.00017-6},
url = {https://www.sciencedirect.com/science/article/pii/B9781597497336000176}
}
@article{BECHAR201530,
title = {A review and framework of laser-based collaboration support},
journal = {Annual Reviews in Control},
volume = {39},
pages = {30-45},
year = {2015},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2015.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1367578815000048},
author = {A. Bechar and S.Y. Nof and J.P. Wachs},
abstract = {New technologies are emerging to enable and support physical, implicit and explicit collaborations. They are essential for dealing with increasingly complex systems in unstructured, dynamic environments. The purpose of this article is to review the role of laser technology in enabling better, more precise interactions and their control, and to identify opportunities and challenges in this area. While the most common applications of laser technology are found in medical and health care, manufacturing, and communication, other domains such as safety, quality assurance, agriculture, construction, entertainment, defense, transportation, and law enforcement also benefit from it. In spite of the rapid dissemination of this technology, its role in support of collaboration and discovery is still in its infancy. Research activities concerning new ways of using lasers as a collaboration supporting technology that may strengthen new areas have been relatively limited. Nevertheless, the translation to this domain of collaboration support has been recognized as vital for activities that demand increasingly more coordinated effort among interacting agents (e.g., humans, machines, particles) and digital, possibly also photonic agents. Recent advances in laser technology in a number of application domains are reviewed in this article, focusing primarily on lasers’ role for supporting different forms of precision interactions and collaboration. In addition, a framework with five collaboration support functions and five collaboration dimensions is defined for this review. The taxonomy framework is useful for enabling better understanding of the existing and emerging opportunities that laser-based technology offers for collaboration support, its advantages and several research gaps.}
}